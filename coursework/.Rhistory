# same fores with importance = true
rf_imp <-randomForest(as.factor(label)~.,data = train_scaled_label, importance = TRUE)
# plot variable importance
varImpPlot(rf_imp,type=1,scale=FALSE,
n.var=ncol(train_scaled),cex=.5,
main="Variable Importance")
# update plot
varImpPlot(rf_imp,type=1,scale=FALSE,
n.var=20,cex=.75,
main="Variable Importance")
par(mar=c(4,2,2,1)+.1)
# plot top 15
varImpPlot(rf_imp,type=1,scale=FALSE,
n.var=12,cex=1,
main="Top 12 Variable Importance")
# predict using rf_imp
RFprob<-predict(object=rf_imp, newdata = valid_scaled, type = "prob")
# calculate brier score for random forest
BrierScore(z=RFprob, class=valid_scaled_label$label)
#train_scaled_label
train_1 <- train_scaled_label[, c("X7", "X8", "X9", "X10")]
train_2 <- cbind(train_1, train_scaled_label[, c("X3", "X1")])
train_3 <- cbind(train_1, train_scaled_label[, c("X3", "X11")])
train_4 <- cbind(train_2, train_scaled_label[, c("X2", "X13")])
train_5 <- cbind(train_3, train_scaled_label[["X16"]])
colnames(train_5)[ncol(train_5)] <- "X16"
colnames(train_scaled_label)
# valid_scaled
valid_1 <- valid_scaled_label[, c("X7", "X8", "X9", "X10")]
valid_2 <- cbind(valid_1, valid_scaled_label[, c("X3", "X1")])
valid_3 <- cbind(valid_1, valid_scaled_label[, c("X3", "X11")])
valid_4 <- cbind(valid_2, valid_scaled_label[, c("X2", "X13")])
valid_5 <- cbind(valid_3, valid_scaled_label[["X16"]])
colnames(valid_5)[ncol(valid_5)] <- "X16"
# loop through k, print confusion matrix and store accuracies
# square root N of dataset to get upper K
K = sqrt(length(train_1[,1]))
k_values <- 1:K
# empty vectors to store results
accuracies <- rep(0, length(k_values))
# loop over different values of K, storing accuracies
for (i in seq_along(k_values)) {
k <- k_values[i]
# pull each each knn
pull <- knn(train=train_1, test=valid_1, cl=train_label_vec, k=k, prob=TRUE)[1:length(valid_label_vec)]
# create kable table
# tab <- kbl(table(truth=valid_label_vec,predicted=pull)) |>
#   kable_classic(full_width = F, html_font = "Cambria")
# # print each
# print(tab)
# store each accuracy = predict == actual/total
acc <- sum(pull == valid_label_vec) / length(valid_label_vec)
accuracies[i] <- acc
}
# create a df of results
knn_accuracies <- data.frame(K=k_values, Accuracy=accuracies)
# loop through k, print confusion matrix and store accuracies
# square root N of dataset to get upper K
K = sqrt(length(train_1[,1]))
k_values <- 1:K
# empty vectors to store results
accuracies <- rep(0, length(k_values))
# loop over different values of K, storing accuracies
for (i in seq_along(k_values)) {
k <- k_values[i]
# pull each each knn
pull <- knn(train=train_1, test=valid_1, cl=train_label_vec, k=k, prob=TRUE)[1:length(valid_label_vec)]
# create kable table
# tab <- kbl(table(truth=valid_label_vec,predicted=pull)) |>
#   kable_classic(full_width = F, html_font = "Cambria")
# # print each
# print(tab)
# store each accuracy = predict == actual/total
acc <- sum(pull == valid_label_vec) / length(valid_label_vec)
accuracies[i] <- acc
}
# create a df of results
knn_accuracies <- data.frame(K=k_values, Accuracy=accuracies)
# max accuracy
max_accuracy <- max(knn_accuracies$Accuracy)
# associated k
best_k <- knn_accuracies$K[which.max(knn_accuracies$Accuracy)]
# print
cat("Best accuracy:\n")
max_accuracy
cat("K for best accuracy:\n")
best_k
# loop through k, print confusion matrix and store accuracies
# square root N of dataset to get upper K
K = sqrt(length(train_1[,1]))
k_values <- 1:K
# empty vectors to store results
accuracies <- rep(0, length(k_values))
# loop over different values of K, storing accuracies
for (i in seq_along(k_values)) {
k <- k_values[i]
# pull each each knn
pull <- knn(train=train_1, test=valid_1, cl=train_label_vec, k=k, prob=TRUE)[1:length(valid_label_vec)]
# create kable table
# tab <- kbl(table(truth=valid_label_vec,predicted=pull)) |>
#   kable_classic(full_width = F, html_font = "Cambria")
# # print each
# print(tab)
# store each accuracy = predict == actual/total
acc <- sum(pull == valid_label_vec) / length(valid_label_vec)
accuracies[i] <- acc
}
# create a df of results
knn_accuracies <- data.frame(K=k_values, Accuracy=accuracies)
# max accuracy
max_accuracy <- max(knn_accuracies$Accuracy)
# associated k
best_k <- knn_accuracies$K[which.max(knn_accuracies$Accuracy)]
# print
cat("Best accuracy:\n")
max_accuracy
cat("K for best accuracy:\n")
best_k
# create a plot
ggplot(knn_accuracies, aes(x=K, y=Accuracy)) +
geom_line() +
geom_point() +
labs(x="K", y="Accuracy", title="KNN Classifier Accuracy for Different K Values, Scaled, Feature Reduced Set 1")
# define a list of data frames
train_dfs <- list(train_1, train_2, train_3, train_4, train_5)
valid_dfs <- list(valid_1, valid_2, valid_3, valid_4, valid_5)
# loop over each data frame
for (j in seq_along(train_dfs)) {
train_df <- train_dfs[[j]]
valid_df <- valid_dfs[[j]]
# loop through k, print confusion matrix and store accuracies
# square root N of dataset to get upper K
K = sqrt(length(train_df[,1]))
k_values <- 1:K
# empty vectors to store results
accuracies <- rep(0, length(k_values))
# loop over different values of K, storing accuracies
for (i in seq_along(k_values)) {
k <- k_values[i]
# pull each each knn
pull <- knn(train=train_df, test=valid_df, cl=train_label_vec, k=k, prob=TRUE)[1:length(valid_label_vec)]
# create kable table
# tab <- kbl(table(truth=valid_label_vec,predicted=pull)) |>
#   kable_classic(full_width = F, html_font = "Cambria")
# # print each
# print(tab)
# store each accuracy = predict == actual/total
acc <- sum(pull == valid_label_vec) / length(valid_label_vec)
accuracies[i] <- acc
}
# create a df of results
knn_accuracies <- data.frame(K=k_values, Accuracy=accuracies)
# max accuracy
max_accuracy <- max(knn_accuracies$Accuracy)
# associated k
best_k <- knn_accuracies$K[which.max(knn_accuracies$Accuracy)]
# print
cat(paste0("\nBest accuracy for valid_", j, ":\n"))
print(max_accuracy)
cat(paste0("K for best accuracy for valid_", j, ":\n"))
print(best_k)
}
train_df[,1]
K
# define a list of data frames
train_dfs <- list(train_1, train_2, train_3, train_4, train_5)
valid_dfs <- list(valid_1, valid_2, valid_3, valid_4, valid_5)
# loop over each data frame
for (j in seq_along(train_dfs)) {
train_df <- train_dfs[[j]]
valid_df <- valid_dfs[[j]]
# loop through k, print confusion matrix and store accuracies
# square root N of dataset to get upper K
K = sqrt(length(train_df[,1]))
k_values <- 1:K
# empty vectors to store results
accuracies <- rep(0, length(k_values))
# loop over different values of K, storing accuracies
for (i in seq_along(k_values)) {
k <- k_values[i]
# pull each each knn
pull <- knn(train=train_df, test=valid_df, cl=train_label_vec, k=k, prob=TRUE)[1:length(valid_label_vec)]
# create kable table
# tab <- kbl(table(truth=valid_label_vec,predicted=pull)) |>
#   kable_classic(full_width = F, html_font = "Cambria")
# # print each
# print(tab)
# store each accuracy = predict == actual/total
acc <- sum(pull == valid_label_vec) / length(valid_label_vec)
accuracies[i] <- acc
}
# create a df of results
knn_accuracies <- data.frame(K=k_values, Accuracy=accuracies)
# max accuracy
max_accuracy <- max(knn_accuracies$Accuracy)
# associated k
best_k <- knn_accuracies$K[which.max(knn_accuracies$Accuracy)]
# print
cat(paste0("\nBest accuracy for valid_", j, ":\n"))
print(max_accuracy)
cat(paste0("K for best accuracy for valid_", j, ":\n"))
print(best_k)
ggplot(knn_accuracies, aes(x=K, y=Accuracy)) +
geom_line() +
geom_point() +
labs(x="K", y="Accuracy", title="KNN Classifier Accuracy for Different K Values: ", j)
}
knn_accuracies
# define a list of data frames
train_dfs <- list(train_1, train_2, train_3, train_4, train_5)
valid_dfs <- list(valid_1, valid_2, valid_3, valid_4, valid_5)
# loop over each data frame
for (j in seq_along(train_dfs)) {
train_df <- train_dfs[[j]]
valid_df <- valid_dfs[[j]]
# loop through k, print confusion matrix and store accuracies
# square root N of dataset to get upper K
K = sqrt(length(train_df[,1]))
k_values <- 1:K
# empty vectors to store results
accuracies <- rep(0, length(k_values))
# loop over different values of K, storing accuracies
for (i in seq_along(k_values)) {
k <- k_values[i]
# pull each each knn
pull <- knn(train=train_df, test=valid_df, cl=train_label_vec, k=k, prob=TRUE)[1:length(valid_label_vec)]
# create kable table
# tab <- kbl(table(truth=valid_label_vec,predicted=pull)) |>
#   kable_classic(full_width = F, html_font = "Cambria")
# # print each
# print(tab)
# store each accuracy = predict == actual/total
acc <- sum(pull == valid_label_vec) / length(valid_label_vec)
accuracies[i] <- acc
}
# create a df of results
knn_accuracies <- data.frame(K=k_values, Accuracy=accuracies)
# max accuracy
max_accuracy <- max(knn_accuracies$Accuracy)
# associated k
best_k <- knn_accuracies$K[which.max(knn_accuracies$Accuracy)]
# print
cat(paste0("\nBest accuracy for valid_", j, ":\n"))
print(max_accuracy)
cat(paste0("K for best accuracy for valid_", j, ":\n"))
print(best_k)
print(ggplot(knn_accuracies, aes(x=K, y=Accuracy)) +
geom_line() +
geom_point() +
labs(x="K", y="Accuracy", title="KNN Classifier Accuracy for Different K Values: ", j))
}
valid_3
names(valid_3)
# hyperparameters to search
n.components <- c(2, 3, 4, 5, 6)
#n.components <- c(3, 4)
diagonal <- c(TRUE, FALSE)
modelName <- c("VEV", "EII", "VII", "EEE", "VVV", "EEI", "VEI", "EVI", "VVI", "VEE", "EVE", "VVE", "EEV", "VEV", "EVV"  )
# grid to search
hyperparameters <- expand.grid(n.components = n.components,
diagonal = diagonal,
modelName = modelName
)
# initialise the best
best.model <- NULL
best.bic <- Inf
# loop over grid
for (i in 1:nrow(hyperparameters)) {
# train_scaled to a numeric matrix
data <- as.matrix(train_1)
# fit mclust model
model <- MclustDA(data = data,
class = train_label_vec,
G = hyperparameters$n.components[i],
diagonal = hyperparameters$diagonal[i],
modelName = hyperparameters$modelName[i],
verbose = FALSE)
# bic
sum <- summary(model)
bic <- sum$bic
# better bic, saved
if (bic < best.bic) {
best.model <- model
best.bic <- bic
}
}
# best model
sum_best_model2 <- summary(best.model, newdata = valid_1, newclass = valid_label_vec)
# results
cat("Best BIC:\n")
print(sum_best_model2$bic)
cat("Model with best BIC:\n")
print(sum_best_model2)
# make predictions on the validation set
valid_preds2 <- predict(best.model, newdata = valid_1)
valid_preds2 <- valid_preds2$classification
# calculate accuracy of predictions
accuracy2 <- sum(valid_preds2 == valid_label_vec)/ length(valid_label_vec)
cat("Best accuracy (DA):\n")
accuracy2
# hyperparameters to search
n.components <- c(2, 3, 4, 5, 6)
diagonal <- c(TRUE, FALSE)
modelName <- c("VEV", "EII", "VII", "EEE", "VVV", "EEI", "VEI", "EVI", "VVI", "VEE", "EVE", "VVE", "EEV", "VEV", "EVV")
# grid to search
hyperparameters <- expand.grid(n.components = n.components,
diagonal = diagonal,
modelName = modelName)
# initialise the best
best.model <- NULL
best.accuracy <- 0
# loop over grid
for (i in 1:nrow(hyperparameters)) {
# train_scaled to a numeric matrix
data <- as.matrix(train_1)
# fit mclust model
model <- MclustDA(data = data,
class = train_label_vec,
G = hyperparameters$n.components[i],
diagonal = hyperparameters$diagonal[i],
modelName = hyperparameters$modelName[i],
verbose = FALSE)
# make predictions on the validation set
valid_preds <- predict(model, newdata = valid_1)
valid_preds <- valid_preds$classification
# calculate accuracy of predictions
accuracy <- sum(valid_preds == valid_label_vec)/length(valid_label_vec)
# better accuracy, save model
if (accuracy > best.accuracy) {
best.model <- model
best.accuracy <- accuracy
}
}
# best model
sum_best_model <- summary(best.model, newdata = valid_1, newclass = valid_label_vec)
# print results
cat("Best accuracy (DA):\n")
print(best.accuracy)
cat("Model with best accuracy:\n")
print(sum_best_model)
# make predictions on the validation set
valid_preds <- predict(best.model, newdata = valid_1)
valid_preds <- valid_preds$classification
# calculate accuracy of predictions
accuracy <- sum(valid_preds == valid_label_vec)/ length(valid_label_vec)
cat("Accuracy of predictions on validation set (DA):\n")
accuracy
# hyperparameters to search
n.components <- c(2, 3, 4, 5, 6)
diagonal <- c(TRUE, FALSE)
modelName <- c("VEV", "EII", "VII", "EEE", "VVV", "EEI", "VEI", "EVI", "VVI", "VEE", "EVE", "VVE", "EEV", "VEV", "EVV")
# grid to search
hyperparameters <- expand.grid(n.components = n.components,
diagonal = diagonal,
modelName = modelName)
# initialise the best
best.model <- NULL
best.accuracy <- 0
best.df <- 0
# create a list of dataframes
train_list <- list(train_1, train_2, train_3, train_4, train_5)
valid_list <- list(valid_1, valid_2, valid_3, valid_4, valid_5)
# loop over dataframes
for (j in 1:length(train_list)) {
# loop over grid
for (i in 1:nrow(hyperparameters)) {
# train_scaled to a numeric matrix
data <- as.matrix(train_list[[j]])
# fit mclust model
model <- MclustDA(data = data,
class = train_label_vec,
G = hyperparameters$n.components[i],
diagonal = hyperparameters$diagonal[i],
modelName = hyperparameters$modelName[i],
verbose = FALSE)
# make predictions on the validation set
valid_preds <- predict(model, newdata = valid_list[[j]])
valid_preds <- valid_preds$classification
# calculate accuracy of predictions
accuracy <- sum(valid_preds == valid_label_vec)/length(valid_label_vec)
# better accuracy, save model and dataframe index
if (accuracy > best.accuracy) {
best.model <- model
best.accuracy <- accuracy
best.df <- j
}
}
}
# best model
sum_best_model <- summary(best.model, newdata = valid_list[[best.df]], newclass = valid_label_vec)
# print results
cat("Best accuracy (DA) for dataframe ", best.df, ":\n")
print(best.accuracy)
cat("Model with best accuracy for dataframe ", best.df, ":\n")
print(sum_best_model)
# make predictions on the validation set
valid_preds <- predict(best.model, newdata = valid_list[[best.df]])
valid_preds <- valid_preds$classification
# calculate accuracy of predictions
accuracy <- sum(valid_preds == valid_label_vec)/ length(valid_label_vec)
cat("Accuracy of predictions on validation set (DA) for dataframe ", best.df, ":\n")
accuracy
# loop through datasets
for (i in 1:length(train_list)) {
train <- train_list[[i]]
valid <- valid_list[[i]]
# rain model, with labels
rf_default <- randomForest(as.factor(label) ~ ., data = train)
# predictions on validation set
pred_valid <- predict(rf_default, valid)
# accuracy
accuracy_rf_default <- sum(pred_valid == valid$label) / nrow(valid)
# results
cat(paste0("Accuracy of random forest (default) on variable set ", i, ": ", round(accuracy_rf_default,3), "\n"))
}
train_1_rf <- train_scaled_label[, c("X7", "X8", "X9", "X10", "label")]
train_2_rf <- cbind(train_1, train_scaled_label[, c("X3", "X1")])
train_3_rf <- cbind(train_1, train_scaled_label[, c("X3", "X11")])
train_4_rf <- cbind(train_2, train_scaled_label[, c("X2", "X13")])
train_5_rf <- cbind(train_3, train_scaled_label[["X16"]])
#colnames(train_5)[ncol(train_5)] <- "X16"
#colnames(train_scaled_label)
# valid_scaled
valid_1_rf <- valid_scaled_label[, c("X7", "X8", "X9", "X10", "label")]
valid_2_rf <- cbind(valid_1, valid_scaled_label[, c("X3", "X1")])
valid_3_rf <- cbind(valid_1, valid_scaled_label[, c("X3", "X11")])
valid_4_rf <- cbind(valid_2, valid_scaled_label[, c("X2", "X13")])
valid_5_rf <- cbind(valid_3, valid_scaled_label[["X16"]])
colnames(valid_5)[ncol(valid_5)] <- "X16"
train_1_rf
train_2_rf
train_1_rf <- train_scaled_label[, c("X7", "X8", "X9", "X10", "label")]
train_2_rf <- cbind(train_1_rf, train_scaled_label[, c("X3", "X1")])
train_3_rf <- cbind(train_1_rf, train_scaled_label[, c("X3", "X11")])
train_4_rf <- cbind(train_2_rf, train_scaled_label[, c("X2", "X13")])
train_5_rf <- cbind(train_3_rf, train_scaled_label[["X16"]])
#colnames(train_5)[ncol(train_5)] <- "X16"
#colnames(train_scaled_label)
# valid_scaled
valid_1_rf <- valid_scaled_label[, c("X7", "X8", "X9", "X10", "label")]
valid_2_rf <- cbind(valid_1_rf, valid_scaled_label[, c("X3", "X1")])
valid_3_rf <- cbind(valid_1_rf, valid_scaled_label[, c("X3", "X11")])
valid_4_rf <- cbind(valid_2_rf, valid_scaled_label[, c("X2", "X13")])
valid_5_rf <- cbind(valid_3_rf, valid_scaled_label[["X16"]])
train_1_rf
train_2_rf
train_5_rf
train_5_rf
valid_5_rf
colnames(train_5)[ncol(train_5)] <- "X16"
colnames(valid_5)[ncol(valid_5)] <- "X16"
valid_5_rf
train_1_rf <- train_scaled_label[, c("X7", "X8", "X9", "X10", "label")]
train_2_rf <- cbind(train_1_rf, train_scaled_label[, c("X3", "X1")])
train_3_rf <- cbind(train_1_rf, train_scaled_label[, c("X3", "X11")])
train_4_rf <- cbind(train_2_rf, train_scaled_label[, c("X2", "X13")])
train_5_rf <- cbind(train_3_rf, train_scaled_label[["X16"]])
colnames(train_5_rf)[ncol(train_5_rf)] <- "X16"
#colnames(train_scaled_label)
# valid_scaled
valid_1_rf <- valid_scaled_label[, c("X7", "X8", "X9", "X10", "label")]
valid_2_rf <- cbind(valid_1_rf, valid_scaled_label[, c("X3", "X1")])
valid_3_rf <- cbind(valid_1_rf, valid_scaled_label[, c("X3", "X11")])
valid_4_rf <- cbind(valid_2_rf, valid_scaled_label[, c("X2", "X13")])
valid_5_rf <- cbind(valid_3_rf, valid_scaled_label[["X16"]])
colnames(valid_5_rf)[ncol(valid_5_rf)] <- "X16"
valid_5_rf
train_5_rf
# loop through datasets
for (i in 1:length(train_list)) {
train <- train_list[[i]]
valid <- valid_list[[i]]
# rain model, with labels
rf_default <- randomForest(as.factor(label) ~ ., data = train)
# predictions on validation set
pred_valid <- predict(rf_default, valid)
# accuracy
accuracy_rf_default <- sum(pred_valid == valid$label) / nrow(valid)
# results
cat(paste0("Accuracy of random forest (default) on variable set ", i, ": ", round(accuracy_rf_default,3), "\n"))
}
# add labels to reduced variable dataframes
train_1_rf <- train_scaled_label[, c("X7", "X8", "X9", "X10", "label")]
train_2_rf <- cbind(train_1_rf, train_scaled_label[, c("X3", "X1")])
train_3_rf <- cbind(train_1_rf, train_scaled_label[, c("X3", "X11")])
train_4_rf <- cbind(train_2_rf, train_scaled_label[, c("X2", "X13")])
train_5_rf <- cbind(train_3_rf, train_scaled_label[["X16"]])
colnames(train_5_rf)[ncol(train_5_rf)] <- "X16"
#colnames(train_scaled_label)
# valid_scaled
valid_1_rf <- valid_scaled_label[, c("X7", "X8", "X9", "X10", "label")]
valid_2_rf <- cbind(valid_1_rf, valid_scaled_label[, c("X3", "X1")])
valid_3_rf <- cbind(valid_1_rf, valid_scaled_label[, c("X3", "X11")])
valid_4_rf <- cbind(valid_2_rf, valid_scaled_label[, c("X2", "X13")])
valid_5_rf <- cbind(valid_3_rf, valid_scaled_label[["X16"]])
colnames(valid_5_rf)[ncol(valid_5_rf)] <- "X16"
train_list_rf <- list(train_1_rf, train_2_rf, train_3_rf, train_4_rf, train_5_rf)
valid_list_rf <- list(valid_1_rf, valid_2_rf, valid_3_rf, valid_4_rf, valid_5_rf)
# loop through datasets
for (i in 1:length(train_list_rf)) {
train <- train_list_rf[[i]]
valid <- valid_list_rf[[i]]
# rain model, with labels
rf_default <- randomForest(as.factor(label) ~ ., data = train)
# predictions on validation set
pred_valid <- predict(rf_default, valid)
# accuracy
accuracy_rf_default <- sum(pred_valid == valid$label) / nrow(valid)
# results
cat(paste0("Accuracy of random forest (default) on variable set ", i, ": ", round(accuracy_rf_default,3), "\n"))
}
setwd("C:/Users/zoona/OneDrive - UWE Bristol/Modules/ML_CW/Machine_Learning_Analytics/coursework")

@article{abuzohairPredictionStudentPerformance2019,
  title = {Prediction of {{Student}}'s Performance by Modelling Small Dataset Size},
  author = {Abu Zohair, Lubna Mahmoud},
  year = {2019},
  month = aug,
  journal = {International Journal of Educational Technology in Higher Education},
  volume = {16},
  number = {1},
  pages = {27},
  issn = {2365-9440},
  doi = {10.1186/s41239-019-0160-3},
  urldate = {2023-05-14},
  abstract = {Prediction of student's performance became an urgent desire in most of educational entities and institutes. That is essential in order to help at-risk students and assure their retention, providing the excellent learning resources and experience, and improving the university's ranking and reputation. However, that might be difficult to be achieved for startup to mid-sized universities, especially those which are specialized in graduate and post graduate programs, and have small students' records for analysis. So, the main aim of this project is to prove the possibility of training and modeling a small dataset size and the feasibility of creating a prediction model with credible accuracy rate. This research explores as well the possibility of identifying the key indicators in the small dataset, which will be utilized in creating the prediction model, using visualization and clustering algorithms. Best indicators were fed into multiple machine learning algorithms to evaluate them for the most accurate model. Among the selected algorithms, the results proved the ability of clustering algorithm in identifying key indicators in small datasets. The main outcomes of this study have proved the efficiency of support vector machine and learning discriminant analysis algorithms in training small dataset size and in producing an acceptable classification's accuracy and reliability test rates.},
  keywords = {Classification algorithms,Learning analytics,Machine learning,Small dataset,Visualization},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\YTZFT52W\\Abu Zohair - 2019 - Prediction of Student’s performance by modelling s.pdf;C\:\\Users\\zoona\\Zotero\\storage\\RRNHXBF8\\s41239-019-0160-3.html}
}

@inproceedings{almayahiMachineLearningBased2020,
  title = {Machine {{Learning Based Predicting Student Academic Success}}},
  booktitle = {2020 12th {{International Congress}} on {{Ultra Modern Telecommunications}} and {{Control Systems}} and {{Workshops}} ({{ICUMT}})},
  author = {Al Mayahi, Khalfan and {Al-Bahri}, Mahmood},
  year = {2020},
  month = oct,
  pages = {264--268},
  issn = {2157-023X},
  doi = {10.1109/ICUMT51630.2020.9222435},
  abstract = {Today, all institutions and companies are accelerating the use of AI technologies in their businesses to achieve a clear vision and quality results. The education sector is one of the sectors where AI can be used because of big data. In this work we created a machine-based learning model to predict a student's educational performance. The developed model relied on the student's previous data and performance in the last stage of the school. The model showed a very accurate accuracy rate that can be adopted.},
  keywords = {accuracy,Algorithm,Education,Machine learning,Machine Learning,Machine learning algorithms,Physics,Predicting,Prediction algorithms,Task analysis,Training},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\CGMYCR7J\\9222435.html}
}

@book{alpaydinMachineLearning2016,
  title = {Machine {{Learning}}},
  author = {Alpaydin, Ethem},
  year = {2016},
  publisher = {{MIT Press Ltd}},
  address = {{Cambridge, Mass.}},
  isbn = {978-0-262-52951-8}
}

@book{alpaydinMachineLearning2016,
  title = {Machine {{Learning}}},
  author = {Alpaydin, Ethem},
  year = {2016},
  publisher = {{MIT Press Ltd}},
  address = {{Cambridge, Mass.}},
  isbn = {978-0-262-52951-8}
}

@misc{BBCVisualData,
  title = {{{BBC Visual}} and {{Data Journalism}} Cookbook for {{R}} Graphics},
  urldate = {2023-02-11},
  howpublished = {https://bbc.github.io/rcookbook/},
  keywords = {ggplot,R,Visualisation},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\VHB5VLUY\\rcookbook.html}
}

@misc{BBCVisualData,
  title = {{{BBC Visual}} and {{Data Journalism}} Cookbook for {{R}} Graphics},
  urldate = {2023-02-11},
  keywords = {ggplot,R,Visualisation},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\LKQYVLVS\\rcookbook.html}
}

@article{blankersMissingDataApproaches2010,
  title = {Missing {{Data Approaches}} in {{eHealth Research}}: {{Simulation Study}} and a {{Tutorial}} for {{Nonmathematically Inclined Researchers}}},
  shorttitle = {Missing {{Data Approaches}} in {{eHealth Research}}},
  author = {Blankers, Matthijs and Koeter, Maarten W J and Schippers, Gerard M},
  year = {2010},
  month = dec,
  journal = {Journal of Medical Internet Research},
  volume = {12},
  number = {5},
  pages = {e54},
  issn = {1438-8871},
  doi = {10.2196/jmir.1448},
  urldate = {2023-03-22},
  abstract = {Background: Missing data is a common nuisance in eHealth research: it is hard to prevent and may invalidate research findings. Objective: In this paper several statistical approaches to data ``missingness'' are discussed and tested in a simulation study. Basic approaches (complete case analysis, mean imputation, and last observation carried forward) and advanced methods (expectation maximization, regression imputation, and multiple imputation) are included in this analysis, and strengths and weaknesses are discussed. Methods: The dataset used for the simulation was obtained from a prospective cohort study following participants in an online self-help program for problem drinkers. It contained 124 nonnormally distributed endpoints, that is, daily alcohol consumption counts of the study respondents. Missingness at random (MAR) was induced in a selected variable for 50\% of the cases. Validity, reliability, and coverage of the estimates obtained using the different imputation methods were calculated by performing a bootstrapping simulation study. Results: In the performed simulation study, the use of multiple imputation techniques led to accurate results. Differences were found between the 4 tested multiple imputation programs: NORM, MICE, Amelia II, and SPSS MI. Among the tested approaches, Amelia II outperformed the others, led to the smallest deviation from the reference value (Cohen's d = 0.06), and had the largest coverage percentage of the reference confidence interval (96\%). Conclusions: The use of multiple imputation improves the validity of the results when analyzing datasets with missing observations. Some of the often-used approaches (LOCF, complete cases analysis) did not perform well, and, hence, we recommend not using these. Accumulating support for the analysis of multiple imputed datasets is seen in more recent versions of some of the widely used statistical software programs making the use of multiple imputation more readily available to less mathematically inclined researchers.},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\66DXXXAV\\Blankers et al. - 2010 - Missing Data Approaches in eHealth Research Simul.pdf}
}

@article{blankersMissingDataApproaches2010a,
  title = {Missing {{Data Approaches}} in {{eHealth Research}}: {{Simulation Study}} and a {{Tutorial}} for {{Nonmathematically Inclined Researchers}}},
  shorttitle = {Missing {{Data Approaches}} in {{eHealth Research}}},
  author = {Blankers, Matthijs and Koeter, Maarten W J and Schippers, Gerard M},
  year = {2010},
  month = dec,
  journal = {Journal of Medical Internet Research},
  volume = {12},
  number = {5},
  pages = {e54},
  issn = {1438-8871},
  doi = {10.2196/jmir.1448},
  urldate = {2023-03-22},
  abstract = {Background: Missing data is a common nuisance in eHealth research: it is hard to prevent and may invalidate research findings. Objective: In this paper several statistical approaches to data ``missingness'' are discussed and tested in a simulation study. Basic approaches (complete case analysis, mean imputation, and last observation carried forward) and advanced methods (expectation maximization, regression imputation, and multiple imputation) are included in this analysis, and strengths and weaknesses are discussed. Methods: The dataset used for the simulation was obtained from a prospective cohort study following participants in an online self-help program for problem drinkers. It contained 124 nonnormally distributed endpoints, that is, daily alcohol consumption counts of the study respondents. Missingness at random (MAR) was induced in a selected variable for 50\% of the cases. Validity, reliability, and coverage of the estimates obtained using the different imputation methods were calculated by performing a bootstrapping simulation study. Results: In the performed simulation study, the use of multiple imputation techniques led to accurate results. Differences were found between the 4 tested multiple imputation programs: NORM, MICE, Amelia II, and SPSS MI. Among the tested approaches, Amelia II outperformed the others, led to the smallest deviation from the reference value (Cohen's d = 0.06), and had the largest coverage percentage of the reference confidence interval (96\%). Conclusions: The use of multiple imputation improves the validity of the results when analyzing datasets with missing observations. Some of the often-used approaches (LOCF, complete cases analysis) did not perform well, and, hence, we recommend not using these. Accumulating support for the analysis of multiple imputed datasets is seen in more recent versions of some of the widely used statistical software programs making the use of multiple imputation more readily available to less mathematically inclined researchers.},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\WLQ725YN\\Blankers et al. - 2010 - Missing Data Approaches in eHealth Research Simul.pdf}
}

@article{bougeardSupervisedMultiblockAnalysis2018,
  title = {Supervised Multiblock Analysis in {{R}} with the {{ade4}} Package},
  author = {Bougeard, St{\'e}phanie and Dray, St{\'e}phane},
  year = {2018},
  journal = {Journal of Statistical Software},
  volume = {86},
  number = {1},
  pages = {1--17},
  doi = {10.18637/jss.v086.i01}
}

@article{brownChoosingRightType,
  title = {Choosing the {{Right Type}} of {{Rotation}} in {{PCA}} and {{EFA}}},
  author = {Brown, James Dean},
  langid = {english},
  keywords = {EFA,Factor Analysis,PCA},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\AM5PAAFR\\Brown - Choosing the Right Type of Rotation in PCA and EFA.pdf}
}

@article{brownChoosingRightTypea,
  title = {Choosing the {{Right Type}} of {{Rotation}} in {{PCA}} and {{EFA}}},
  author = {Brown, James Dean},
  langid = {english},
  keywords = {EFA,Factor Analysis,PCA},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\LMEGTBYD\\Brown - Choosing the Right Type of Rotation in PCA and EFA.pdf}
}

@book{bruntonDataDrivenScience,
  title = {Data {{Driven Science}} \& {{Engineering Machine Learning}}, {{Dynamical Systems}}, and {{Control}}},
  author = {Brunton, Steven L. and Kutz, J. Nathan}
}

@book{bruntonDataDrivenScience,
  title = {Data {{Driven Science}} \& {{Engineering Machine Learning}}, {{Dynamical Systems}}, and {{Control}}},
  author = {Brunton, Steven L. and Kutz, J. Nathan}
}

@book{bruntonDataDrivenScienceEngineering2019,
  title = {Data-{{Driven Science}} and {{Engineering}}: {{Machine Learning}}, {{Dynamical Systems}}, and {{Control}}},
  shorttitle = {Data-{{Driven Science}} and {{Engineering}}},
  author = {Brunton, Steven L. and Kutz, J. Nathan},
  year = {2019},
  publisher = {{Cambridge University Press}},
  address = {{New York, NY}},
  doi = {10.1017/9781108380690},
  abstract = {Data-driven discovery is revolutionizing the modeling, prediction, and control of complex systems. This textbook brings together machine learning, engineering mathematics, and mathematical physics to integrate modeling and control of dynamical systems with modern methods in data science. It highlights many of the recent advances in scientific computing that enable data-driven methods to be applied to a diverse range of complex systems, such as turbulence, the brain, climate, epidemiology, finance, robotics, and autonomy. Aimed at advanced undergraduate and beginning graduate students in the engineering and physical sciences, the text presents a range of topics and methods from introductory to state of the art.},
  isbn = {978-1-108-42209-3},
  langid = {english},
  keywords = {Engineering,Mathematical analysis,Science},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\LVPLWK9A\\Brunton and Kutz - 2019 - Data-Driven Science and Engineering Machine Learn.pdf}
}

@book{bruntonDataDrivenScienceEngineering2019,
  title = {Data-{{Driven Science}} and {{Engineering}}: {{Machine Learning}}, {{Dynamical Systems}}, and {{Control}}},
  shorttitle = {Data-{{Driven Science}} and {{Engineering}}},
  author = {Brunton, Steven L. and Kutz, J. Nathan},
  year = {2019},
  publisher = {{Cambridge University Press}},
  address = {{New York, NY}},
  doi = {10.1017/9781108380690},
  abstract = {Data-driven discovery is revolutionizing the modeling, prediction, and control of complex systems. This textbook brings together machine learning, engineering mathematics, and mathematical physics to integrate modeling and control of dynamical systems with modern methods in data science. It highlights many of the recent advances in scientific computing that enable data-driven methods to be applied to a diverse range of complex systems, such as turbulence, the brain, climate, epidemiology, finance, robotics, and autonomy. Aimed at advanced undergraduate and beginning graduate students in the engineering and physical sciences, the text presents a range of topics and methods from introductory to state of the art.},
  isbn = {978-1-108-42209-3},
  langid = {english},
  keywords = {Engineering,Mathematical analysis,Science},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\8TF5IMGC\\Brunton and Kutz - 2019 - Data-Driven Science and Engineering Machine Learn.pdf}
}

@misc{bruntonDataDrivenScienceEngineering2019a,
  title = {Data-{{Driven Science}} and {{Engineering}}: {{Machine Learning}}, {{Dynamical Systems}}, and {{Control}}},
  shorttitle = {Data-{{Driven Science}} and {{Engineering}}},
  author = {Brunton, Steven L. and Kutz, J. Nathan},
  year = {2019},
  month = feb,
  journal = {Cambridge Core},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781108380690},
  urldate = {2023-02-17},
  abstract = {Data-driven discovery is revolutionizing the modeling, prediction, and control of complex systems. This textbook brings together machine learning, engineering mathematics, and mathematical physics to integrate modeling and control of dynamical systems with modern methods in data science. It highlights many of the recent advances in scientific computing that enable data-driven methods to be applied to a diverse range of complex systems, such as turbulence, the brain, climate, epidemiology, finance, robotics, and autonomy. Aimed at advanced undergraduate and beginning graduate students in the engineering and physical sciences, the text presents a range of topics and methods from introductory to state of the art.},
  howpublished = {https://www.cambridge.org/core/books/datadriven-science-and-engineering/77D52B171B60A496EAFE4DB662ADC36E},
  isbn = {9781108380690 9781108422093},
  langid = {english},
  keywords = {DataScience,ML,stats},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\GKEBJP4Z\\Brunton and Kutz - 2019 - Data-Driven Science and Engineering Machine Learn.pdf;C\:\\Users\\zoona\\Zotero\\storage\\EGQFBHNP\\77D52B171B60A496EAFE4DB662ADC36E.html}
}

@misc{bruntonDataDrivenScienceEngineering2019a,
  title = {Data-{{Driven Science}} and {{Engineering}}: {{Machine Learning}}, {{Dynamical Systems}}, and {{Control}}},
  shorttitle = {Data-{{Driven Science}} and {{Engineering}}},
  author = {Brunton, Steven L. and Kutz, J. Nathan},
  year = {2019},
  month = feb,
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781108380690},
  urldate = {2023-02-17},
  abstract = {Data-driven discovery is revolutionizing the modeling, prediction, and control of complex systems. This textbook brings together machine learning, engineering mathematics, and mathematical physics to integrate modeling and control of dynamical systems with modern methods in data science. It highlights many of the recent advances in scientific computing that enable data-driven methods to be applied to a diverse range of complex systems, such as turbulence, the brain, climate, epidemiology, finance, robotics, and autonomy. Aimed at advanced undergraduate and beginning graduate students in the engineering and physical sciences, the text presents a range of topics and methods from introductory to state of the art.},
  isbn = {9781108380690 9781108422093},
  langid = {english},
  keywords = {DataScience,ML,stats},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\FEWHCXRL\\Brunton and Kutz - 2019 - Data-Driven Science and Engineering Machine Learn.pdf;C\:\\Users\\zoona\\Zotero\\storage\\PRTSEI9M\\77D52B171B60A496EAFE4DB662ADC36E.html}
}

@article{bujangMulticlassPredictionModel2021,
  title = {Multiclass {{Prediction Model}} for {{Student Grade Prediction Using Machine Learning}}},
  author = {Bujang, Siti Dianah Abdul and Selamat, Ali and Ibrahim, Roliana and Krejcar, Ondrej and {Herrera-Viedma}, Enrique and Fujita, Hamido and Ghani, Nor Azura Md.},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {95608--95621},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3093563},
  abstract = {Today, predictive analytics applications became an urgent desire in higher educational institutions. Predictive analytics used advanced analytics that encompasses machine learning implementation to derive high-quality performance and meaningful information for all education levels. Mostly know that student grade is one of the key performance indicators that can help educators monitor their academic performance. During the past decade, researchers have proposed many variants of machine learning techniques in education domains. However, there are severe challenges in handling imbalanced datasets for enhancing the performance of predicting student grades. Therefore, this paper presents a comprehensive analysis of machine learning techniques to predict the final student grades in the first semester courses by improving the performance of predictive accuracy. Two modules will be highlighted in this paper. First, we compare the accuracy performance of six well-known machine learning techniques namely Decision Tree (J48), Support Vector Machine (SVM), Na\"ive Bayes (NB), K-Nearest Neighbor (kNN), Logistic Regression (LR) and Random Forest (RF) using 1282 real student's course grade dataset. Second, we proposed a multiclass prediction model to reduce the overfitting and misclassification results caused by imbalanced multi-classification based on oversampling Synthetic Minority Oversampling Technique (SMOTE) with two features selection methods. The obtained results show that the proposed model integrates with RF give significant improvement with the highest f-measure of 99.5\%. This proposed model indicates the comparable and promising results that can enhance the prediction performance model for imbalanced multi-classification for student grade prediction.},
  keywords = {Classification algorithms,Data models,imbalanced problem,Machine learning,Machine learning algorithms,multi-class classification,Prediction algorithms,predictive model,Predictive models,student grade prediction,Support vector machines},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\JKSU596C\\Bujang et al. - 2021 - Multiclass Prediction Model for Student Grade Pred.pdf}
}

@book{changGraphicsCookbook2nd,
  title = {R {{Graphics Cookbook}}, 2nd Edition},
  author = {Chang, Winston},
  urldate = {2023-02-21},
  abstract = {This cookbook contains more than 150 recipes to help scientists, engineers, programmers, and data analysts generate high-quality graphs quickly\textemdash without having to comb through all the details of R's graphing systems. Each recipe tackles a specific problem with a solution you can apply to your own project and includes a discussion of how and why the recipe works.},
  keywords = {Graphics,R,Visualisations},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\Y4WZKWS9\\r-graphics.org.html}
}

@book{changGraphicsCookbook2nda,
  title = {R {{Graphics Cookbook}}, 2nd Edition},
  author = {Chang, Winston},
  urldate = {2023-02-21},
  abstract = {This cookbook contains more than 150 recipes to help scientists, engineers, programmers, and data analysts generate high-quality graphs quickly\textemdash without having to comb through all the details of R's graphing systems. Each recipe tackles a specific problem with a solution you can apply to your own project and includes a discussion of how and why the recipe works.},
  keywords = {Graphics,R,Visualisations},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\WE5FMINX\\r-graphics.org.html}
}

@book{Chapter4MultipleImputation,
  title = {Chapter4 {{Multiple Imputation}} | {{Book}}\_{{MI}}.{{Knit}}},
  urldate = {2023-04-18},
  keywords = {Multiple Imputation},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\N6288THU\\multiple-imputation.html}
}

@book{Chapter4MultipleImputationa,
  title = {Chapter4 {{Multiple Imputation}} | {{Book}}\_{{MI}}.Knit},
  urldate = {2023-04-18},
  keywords = {Multiple Imputation},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\4PUKB3BI\\multiple-imputation.html}
}

@article{chesselAde4PackageOnetable2004,
  title = {The {{ade4}} Package \textendash{} {{I}}: {{One-table}} Methods},
  author = {Chessel, Daniel and Dufour, Anne-B{\'e}atrice and Thioulouse, Jean},
  year = {2004},
  journal = {R News},
  volume = {4},
  number = {1},
  pages = {5--10}
}

@misc{CiteSeerX,
  title = {{{CiteSeerX}}},
  urldate = {2023-04-30},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\PVRB7SVA\\10.1.1.650.html}
}

@misc{ComposerPlots,
  title = {The {{Composer}} of {{Plots}}},
  author = {Pedersen, Thomas Lin},
  urldate = {2023-05-03},
  abstract = {The ggplot2 package provides a strong API for sequentially building up a plot, but does not concern itself with composition of multiple plots. patchwork is a package that expands the API to allow for arbitrarily complex composition of plots by, among others, providing mathematical operators for combining multiple plots. Other packages that try to address this need (but with a different approach) are gridExtra and cowplot.},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\3GQCB9M7\\index.html}
}

@misc{ComposerPlotsa,
  title = {The {{Composer}} of {{Plots}}},
  urldate = {2023-05-03},
  abstract = {The ggplot2 package provides a strong API for sequentially      building up a plot, but does not concern itself with composition of multiple     plots. patchwork is a package that expands the API to allow for      arbitrarily complex composition of plots by, among others, providing      mathematical operators for combining multiple plots. Other packages that try      to address this need (but with a different approach) are gridExtra and      cowplot.},
  howpublished = {https://patchwork.data-imaginist.com/index.html},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\4RA6SRTI\\index.html}
}

@misc{Content,
  title = {Content},
  urldate = {2023-02-11},
  howpublished = {https://blackboard.uwe.ac.uk/uwenav/ultra/courses/\_350158\_1/cl/outline},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\AGL4ENYK\\outline.html}
}

@misc{Content,
  title = {Content},
  urldate = {2023-02-11},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\6272UEYH\\outline.html}
}

@manual{corrplot2021,
  type = {Manual},
  title = {R Package 'Corrplot': {{Visualization}} of a Correlation Matrix},
  author = {Wei, Taiyun and Simko, Viliam},
  year = {2021}
}

@manual{corrplot2021,
  type = {Manual},
  title = {R Package 'Corrplot': {{Visualization}} of a Correlation Matrix},
  author = {Wei, Taiyun and Simko, Viliam},
  year = {2021}
}

@misc{CS231nConvolutionalNeural,
  title = {{{CS231n Convolutional Neural Networks}} for {{Visual Recognition}}},
  urldate = {2023-02-18},
  howpublished = {https://cs231n.github.io/},
  keywords = {{Neural network, visual recognition, convolutional nn, knn, stochachastic gradient descent}},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\EXHVBZ7E\\cs231n.github.io.html}
}

@misc{CS231nConvolutionalNeural,
  title = {{{CS231n Convolutional Neural Networks}} for {{Visual Recognition}}},
  urldate = {2023-02-18},
  keywords = {{Neural network, visual recognition, convolutional nn, knn, stochachastic gradient descent}},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\EEHF3SEJ\\cs231n.github.io.html}
}

@misc{DataManagementData,
  title = {4. {{Data Management}}, {{Data Engineering}}, and {{Data Science Overview}} - {{Implementing}} a {{Smart Data Platform}} [{{Book}}]},
  urldate = {2023-02-11},
  abstract = {Chapter 4. Data Management, Data Engineering, and Data Science Overview Data Management Data management refers to the process by which data is effectively acquired, stored, processed, and applied, aiming to \ldots{} - Selection from Implementing a Smart Data Platform [Book]},
  howpublished = {https://www.oreilly.com/library/view/implementing-a-smart/9781491983492/ch04.html},
  isbn = {9781491983485},
  langid = {english},
  keywords = {Data Engineering,Data Management,Data Science},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\4USFVL5S\\ch04.html}
}

@misc{DataManagementData,
  title = {4. {{Data Management}}, {{Data Engineering}}, and {{Data Science Overview}} - {{Implementing}} a {{Smart Data Platform}} [{{Book}}]},
  urldate = {2023-02-11},
  abstract = {Chapter 4. Data Management, Data Engineering, and Data Science Overview Data Management Data management refers to the process by which data is effectively acquired, stored, processed, and applied, aiming to \ldots{} - Selection from Implementing a Smart Data Platform [Book]},
  isbn = {9781491983485},
  langid = {english},
  keywords = {Data Engineering,Data Management,Data Science},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\RMXNPTYJ\\ch04.html}
}

@misc{DimensionalityReductionPrincipal,
  title = {Dimensionality {{Reduction}}; {{Principal Component Analysis}} ({{PCA}})},
  urldate = {2023-02-17},
  abstract = {Data that includes many features or many different vectors can be thought of as having many dimensions. Often, it's useful to reduce those dimensions down to something more easily visualized, for compression, or to just distill the most important information from a data set (that is, information that contributes the most to the data's variance). Principal Component Analysis and Singular Value Decomposition do that.},
  langid = {english},
  keywords = {Dimension Reduction,PCA},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\UA5HM4X4\\9781787127081-video6_3.html}
}

@misc{DimensionalityReductionPrincipal,
  title = {Dimensionality {{Reduction}}; {{Principal Component Analysis}} ({{PCA}})},
  urldate = {2023-02-17},
  abstract = {Data that includes many features or many different vectors can be thought of as having many dimensions. Often, it's useful to reduce those dimensions down to something more easily visualized, for compression, or to just distill the most important information from a data set (that is, information that contributes the most to the data's variance). Principal Component Analysis and Singular Value Decomposition do that.},
  langid = {english},
  keywords = {Dimension Reduction,PCA},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\RVV39MUG\\9781787127081-video6_3.html}
}

@manual{dinnoParanHornTest2018,
  type = {Manual},
  title = {Paran: {{Horn}}'s Test of Principal {{Components}}/{{Factors}}},
  author = {Dinno, Alexis},
  year = {2018}
}

@manual{dinnoParanHornTest2018a,
  type = {Manual},
  title = {Paran: {{Horn}}'s Test of Principal {{Components}}/{{Factors}}},
  author = {Dinno, Alexis},
  year = {2018}
}

@misc{doiSDS375,
  title = {{{SDS}} 375},
  author = {{published yet DOI}, Authors Affiliations Published Not},
  urldate = {2023-02-23},
  abstract = {Data Visualization in R},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\Q5BHUYD5\\SDS375.html}
}

@book{downeyThinkPythonHow2015,
  title = {Think Python: How to Think like a Computer Scientist},
  author = {Downey, Allen},
  year = {2015},
  edition = {Second edition},
  publisher = {{O'Reilly}},
  address = {{Sebastopol, CA}},
  isbn = {1-4919-3941-9}
}

@book{downeyThinkPythonHow2015,
  title = {Think Python: {{How}} to Think like a Computer Scientist},
  author = {Downey, Allen},
  year = {2015},
  edition = {Second edition},
  publisher = {{O'Reilly}},
  address = {{Sebastopol, CA}},
  isbn = {1-4919-3941-9}
}

@article{drayAde4PackageII2007,
  title = {The {{ade4}} Package \textendash{} {{II}}: {{Two-table}} and {{K-Table}} Methods},
  author = {Dray, St{\'e}phane and Dufour, Anne-B{\'e}atrice and Chessel, Daniel},
  year = {2007},
  journal = {R News},
  volume = {7},
  number = {2},
  pages = {47--52}
}

@article{drayAde4PackageImplementing2007,
  title = {The {{ade4}} Package: {{Implementing}} the Duality Diagram for Ecologists},
  author = {Dray, St{\'e}phane and Dufour, Anne{\textendash}B{\'e}atrice},
  year = {2007},
  journal = {Journal of Statistical Software},
  volume = {22},
  number = {4},
  pages = {1--20},
  doi = {10.18637/jss.v022.i04}
}

@misc{elgabryDatabaseDatabaseDesign2018,
  title = {Database \textemdash{} {{Database Design}}: {{Conceptual Design}} ({{Part}} 4)},
  shorttitle = {Database \textemdash{} {{Database Design}}},
  author = {Elgabry, Omar},
  year = {2018},
  month = jul,
  journal = {Medium},
  urldate = {2023-02-11},
  abstract = {The conceptual design provides a high-level description that's close to the way many users perceive data.},
  howpublished = {https://medium.com/omarelgabrys-blog/database-database-modeling-conceptual-design-part-4-645545a74a4b},
  langid = {english},
  keywords = {Conceptual,Database,Database Design},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\Z2UIL4TD\\database-database-modeling-conceptual-design-part-4-645545a74a4b.html}
}

@misc{elgabryDatabaseDatabaseDesign2018,
  title = {Database \textemdash{} {{Database Design}}: {{Conceptual Design}} ({{Part}} 4)},
  shorttitle = {Database \textemdash{} {{Database Design}}},
  author = {Elgabry, Omar},
  year = {2018},
  month = jul,
  urldate = {2023-02-11},
  abstract = {The conceptual design provides a high-level description that's close to the way many users perceive data.},
  langid = {english},
  keywords = {Conceptual,Database,Database Design},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\4ZGW7MN9\\database-database-modeling-conceptual-design-part-4-645545a74a4b.html}
}

@misc{elgabryDatabaseDatabaseOptions2021,
  title = {Database \textemdash{} {{Database Options}} ({{Part}} 10)},
  author = {Elgabry, Omar},
  year = {2021},
  month = jul,
  journal = {Medium},
  urldate = {2023-02-11},
  abstract = {What are the options, and why we need to choose one over another?},
  howpublished = {https://medium.com/omarelgabrys-blog/database-database-options-part-10-380c6e4467d0},
  langid = {english},
  keywords = {Database,Database Options},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\TQC9NJWE\\database-database-options-part-10-380c6e4467d0.html}
}

@misc{elgabryDatabaseDatabaseOptions2021,
  title = {Database \textemdash{} {{Database Options}} ({{Part}} 10)},
  author = {Elgabry, Omar},
  year = {2021},
  month = jul,
  urldate = {2023-02-11},
  abstract = {What are the options, and why we need to choose one over another?},
  langid = {english},
  keywords = {Database,Database Options},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\BNVVP2HY\\database-database-options-part-10-380c6e4467d0.html}
}

@misc{elgabryDatabaseDesignLogical2021,
  title = {Database \textemdash{} {{Design}}: {{Logical Design}} ({{Part}} 6)},
  shorttitle = {Database \textemdash{} {{Design}}},
  author = {Elgabry, Omar},
  year = {2021},
  month = dec,
  journal = {Medium},
  urldate = {2023-02-11},
  abstract = {The logical design is about mapping of entities, relationships, and multi-valued attributes into a logical schema.},
  howpublished = {https://medium.com/omarelgabrys-blog/database-modeling-logical-design-part-6-af029e93cc1f},
  langid = {english},
  keywords = {Database,Database Design,Entity Model},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\5GY9DJYL\\database-modeling-logical-design-part-6-af029e93cc1f.html}
}

@misc{elgabryDatabaseDesignLogical2021,
  title = {Database \textemdash{} {{Design}}: {{Logical Design}} ({{Part}} 6)},
  shorttitle = {Database \textemdash{} {{Design}}},
  author = {Elgabry, Omar},
  year = {2021},
  month = dec,
  urldate = {2023-02-11},
  abstract = {The logical design is about mapping of entities, relationships, and multi-valued attributes into a logical schema.},
  langid = {english},
  keywords = {Database,Database Design,Entity Model},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\7SHSXTYN\\database-modeling-logical-design-part-6-af029e93cc1f.html}
}

@misc{elgabryDatabaseDesignProcess2021,
  title = {Database \textemdash{} {{Design Process}} ({{Part}} 3)},
  author = {Elgabry, Omar},
  year = {2021},
  month = dec,
  journal = {Medium},
  urldate = {2023-02-11},
  abstract = {We'll walk through the steps to design and create a database.},
  howpublished = {https://medium.com/omarelgabrys-blog/database-design-process-part-3-7b5fafc78774},
  langid = {english},
  keywords = {Database,Database Design,Database Model},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\YSPEUAYC\\database-design-process-part-3-7b5fafc78774.html}
}

@misc{elgabryDatabaseDesignProcess2021,
  title = {Database \textemdash{} {{Design Process}} ({{Part}} 3)},
  author = {Elgabry, Omar},
  year = {2021},
  month = dec,
  urldate = {2023-02-11},
  abstract = {We'll walk through the steps to design and create a database.},
  langid = {english},
  keywords = {Database,Database Design,Database Model},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\CH99JLXC\\database-design-process-part-3-7b5fafc78774.html}
}

@misc{elgabryDatabaseFundamentalsPart2021,
  title = {Database \textemdash{} {{Fundamentals}} ({{Part}} 2)},
  author = {Elgabry, Omar},
  year = {2021},
  month = dec,
  journal = {Medium},
  urldate = {2023-02-11},
  abstract = {The first step towards understanding the databases (specifically relational databases) is understanding the basic features.},
  howpublished = {https://medium.com/omarelgabrys-blog/database-fundamentals-part-2-b841032243ac},
  langid = {english},
  keywords = {Database},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\M8NGIQ8X\\database-fundamentals-part-2-b841032243ac.html}
}

@misc{elgabryDatabaseFundamentalsPart2021,
  title = {Database \textemdash{} {{Fundamentals}} ({{Part}} 2)},
  author = {Elgabry, Omar},
  year = {2021},
  month = dec,
  urldate = {2023-02-11},
  abstract = {The first step towards understanding the databases (specifically relational databases) is understanding the basic features.},
  langid = {english},
  keywords = {Database},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\IH8JTLCM\\database-fundamentals-part-2-b841032243ac.html}
}

@misc{elgabryDatabaseIndexingTransactions2018,
  title = {Database \textemdash{} {{Indexing}}, {{Transactions}} \& {{Stored Procedures}} ({{Part}} 9)},
  author = {Elgabry, Omar},
  year = {2018},
  month = jul,
  journal = {Medium},
  urldate = {2023-02-11},
  abstract = {Optimization, Working with sensitive data, \& Re-usability.},
  howpublished = {https://medium.com/omarelgabrys-blog/database-indexing-and-transactions-part-9-a24781d429f8},
  langid = {english},
  keywords = {Database,Indexing,Transaction},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\WLJL5222\\database-indexing-and-transactions-part-9-a24781d429f8.html}
}

@misc{elgabryDatabaseIndexingTransactions2018,
  title = {Database \textemdash{} {{Indexing}}, {{Transactions}} \& {{Stored Procedures}} ({{Part}} 9)},
  author = {Elgabry, Omar},
  year = {2018},
  month = jul,
  urldate = {2023-02-11},
  abstract = {Optimization, Working with sensitive data, \& Re-usability.},
  langid = {english},
  keywords = {Database,Indexing,Transaction},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\7X3GBN2D\\database-indexing-and-transactions-part-9-a24781d429f8.html}
}

@misc{elgabryDatabaseIntroductionPart2021,
  title = {Database \textemdash{} {{Introduction}} ({{Part}} 1)},
  author = {Elgabry, Omar},
  year = {2021},
  month = dec,
  journal = {OmarElgabry's Blog},
  urldate = {2023-02-11},
  abstract = {Why a database, What's a database, and DBMS.},
  langid = {english},
  keywords = {Data Management,Database},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\GY3LDM9R\\database-introduction-part-1-4844fada1fb0.html}
}

@misc{elgabryDatabaseIntroductionPart2021,
  title = {Database \textemdash{} {{Introduction}} ({{Part}} 1)},
  author = {Elgabry, Omar},
  year = {2021},
  month = dec,
  urldate = {2023-02-11},
  abstract = {Why a database, What's a database, and DBMS.},
  langid = {english},
  keywords = {Data Management,Database},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\V6IRN6A2\\database-introduction-part-1-4844fada1fb0.html}
}

@misc{elgabryDatabaseModelingEntity2021,
  title = {Database \textemdash{} {{Modeling}} : {{Entity Relationship Diagram}} ({{ERD}}) ({{Part}} 5)},
  shorttitle = {Database \textemdash{} {{Modeling}}},
  author = {Elgabry, Omar},
  year = {2021},
  month = dec,
  journal = {Medium},
  urldate = {2023-02-11},
  abstract = {A common approach to sketch the entities and their relationships.},
  howpublished = {https://medium.com/omarelgabrys-blog/database-modeling-entity-relationship-diagram-part-5-352c5a8859e5},
  langid = {english},
  keywords = {Database,Entity Model,ERD},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\Y4K7K3HE\\database-modeling-entity-relationship-diagram-part-5-352c5a8859e5.html}
}

@misc{elgabryDatabaseModelingEntity2021,
  title = {Database \textemdash{} {{Modeling}} : {{Entity Relationship Diagram}} ({{ERD}}) ({{Part}} 5)},
  shorttitle = {Database \textemdash{} {{Modeling}}},
  author = {Elgabry, Omar},
  year = {2021},
  month = dec,
  urldate = {2023-02-11},
  abstract = {A common approach to sketch the entities and their relationships.},
  langid = {english},
  keywords = {Database,Entity Model,ERD},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\DK5ZF428\\database-modeling-entity-relationship-diagram-part-5-352c5a8859e5.html}
}

@misc{elgabryDatabaseNormalizationPart2018,
  title = {Database \textemdash{} {{Normalization}} ({{Part}} 7)},
  author = {Elgabry, Omar},
  year = {2018},
  month = jul,
  journal = {Medium},
  urldate = {2023-02-11},
  abstract = {The process of organizing your database through a set of defined steps.},
  howpublished = {https://medium.com/omarelgabrys-blog/database-normalization-part-7-ef7225150c7f},
  langid = {english},
  keywords = {Database,normalisation},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\MEZFG6JR\\database-normalization-part-7-ef7225150c7f.html}
}

@misc{elgabryDatabaseNormalizationPart2018,
  title = {Database \textemdash{} {{Normalization}} ({{Part}} 7)},
  author = {Elgabry, Omar},
  year = {2018},
  month = jul,
  urldate = {2023-02-11},
  abstract = {The process of organizing your database through a set of defined steps.},
  langid = {english},
  keywords = {Database,normalisation},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\R9Y6H3US\\database-normalization-part-7-ef7225150c7f.html}
}

@misc{elgabryDatabaseStructuredQuery2018,
  title = {Database \textemdash{} {{Structured Query Language}} ({{SQL}}) ({{Part}} 8)},
  author = {Elgabry, Omar},
  year = {2018},
  month = jul,
  journal = {Medium},
  urldate = {2023-02-11},
  abstract = {Leverage The Power of The Structured Query Language (SQL).},
  howpublished = {https://medium.com/omarelgabrys-blog/database-structured-query-language-part-8-230a1808ec96},
  langid = {english},
  keywords = {Database,SQL},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\3X3IZADF\\database-structured-query-language-part-8-230a1808ec96.html}
}

@misc{elgabryDatabaseStructuredQuery2018,
  title = {Database \textemdash{} {{Structured Query Language}} ({{SQL}}) ({{Part}} 8)},
  author = {Elgabry, Omar},
  year = {2018},
  month = jul,
  urldate = {2023-02-11},
  abstract = {Leverage The Power of The Structured Query Language (SQL).},
  langid = {english},
  keywords = {Database,SQL},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\VCLRP5JH\\database-structured-query-language-part-8-230a1808ec96.html}
}

@article{flanaganEarlywarningPredictionStudent2022,
  title = {Early-Warning Prediction of Student Performance and Engagement in Open Book Assessment by Reading Behavior Analysis},
  author = {Flanagan, Brendan and Majumdar, Rwitajit and Ogata, Hiroaki},
  year = {2022},
  month = aug,
  journal = {International Journal of Educational Technology in Higher Education},
  volume = {19},
  number = {1},
  pages = {41},
  issn = {2365-9440},
  doi = {10.1186/s41239-022-00348-4},
  urldate = {2023-03-27},
  abstract = {Digitized learning materials are a core part of modern education, and analysis of the use can offer insight into the learning behavior of high and low performing students. The topic of predicting student characteristics has gained a lot of attention in recent years, with applications ranging from affect to performance and at-risk student prediction. In this paper, we examine students reading behavior using a digital textbook system while taking an open-book test from the perspective of engagement and performance to identify the strategies that are used. We create models to predict the performance and engagement of learners before the start of the assessment and extract reading behavior characteristics employed before and after the start of the assessment in a higher education setting. It was found that strategies, such as: revising and previewing are indicators of how a learner will perform in an open ebook assessment. Low performing students take advantage of the open ebook policy of the assessment and employ a strategy of searching for information during the assessment. Also compared to performance, the prediction of overall engagement has a higher accuracy, and therefore could be more appropriate for identifying intervention candidates as an early-warning intervention system.},
  keywords = {Early warning prediction,HE,ML,Open-book assessment,Prediction,Reading behavior,Student modeling},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\XAWCJXZ2\\Flanagan et al. - 2022 - Early-warning prediction of student performance an.pdf;C\:\\Users\\zoona\\Zotero\\storage\\P76J2V3Z\\s41239-022-00348-4.html}
}

@article{flanaganEarlywarningPredictionStudent2022a,
  title = {Early-Warning Prediction of Student Performance and Engagement in Open Book Assessment by Reading Behavior Analysis},
  author = {Flanagan, Brendan and Majumdar, Rwitajit and Ogata, Hiroaki},
  year = {2022},
  month = aug,
  journal = {International Journal of Educational Technology in Higher Education},
  volume = {19},
  number = {1},
  pages = {41},
  issn = {2365-9440},
  doi = {10.1186/s41239-022-00348-4},
  urldate = {2023-03-27},
  abstract = {Digitized learning materials are a core part of modern education, and analysis of the use can offer insight into the learning behavior of high and low performing students. The topic of predicting student characteristics has gained a lot of attention in recent years, with applications ranging from affect to performance and at-risk student prediction. In this paper, we examine students reading behavior using a digital textbook system while taking an open-book test from the perspective of engagement and performance to identify the strategies that are used. We create models to predict the performance and engagement of learners before the start of the assessment and extract reading behavior characteristics employed before and after the start of the assessment in a higher education setting. It was found that strategies, such as: revising and previewing are indicators of how a learner will perform in an open ebook assessment. Low performing students take advantage of the open ebook policy of the assessment and employ a strategy of searching for information during the assessment. Also compared to performance, the prediction of overall engagement has a higher accuracy, and therefore could be more appropriate for identifying intervention candidates as an early-warning intervention system.},
  keywords = {Early warning prediction,HE,ML,Open-book assessment,Prediction,Reading behavior,Student modeling},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\T8GIK3GR\\Flanagan et al. - 2022 - Early-warning prediction of student performance an.pdf;C\:\\Users\\zoona\\Zotero\\storage\\R4B3CJY4\\s41239-022-00348-4.html}
}

@book{ForecastingPrinciplesPractice,
  title = {Forecasting: {{Principles}} and {{Practice}} (3rd Ed)},
  shorttitle = {Forecasting},
  urldate = {2023-03-28},
  abstract = {3rd edition},
  keywords = {Forecasting,Prediction,TimeSeries},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\N9BYUA4W\\fpp3.html}
}

@book{ForecastingPrinciplesPracticea,
  title = {Forecasting: {{Principles}} and {{Practice}} (3rd Ed)},
  shorttitle = {Forecasting},
  urldate = {2023-03-28},
  abstract = {3rd edition},
  keywords = {Forecasting,Prediction,TimeSeries},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\3ZKA3ZA5\\fpp3.html}
}

@article{fraleyModelBasedClusteringDiscriminant2002,
  title = {Model-{{Based Clustering}}, {{Discriminant Analysis}}, and {{Density Estimation}}},
  author = {Fraley, Chris and Raftery, Adrian E},
  year = {2002},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {97},
  number = {458},
  pages = {611--631},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1198/016214502760047131},
  urldate = {2023-03-08},
  abstract = {Cluster analysis is the automated search for groups of related observations in a dataset. Most clustering done in practice is based largely on heuristic but intuitively reasonable procedures, and most clustering methods available in commercial software are also of this type. However, there is little systematic guidance associated with these methods for solving important practical questions that arise in cluster analysis, such as how many clusters are there, which clustering method should be used, and how should outliers be handled. We review a general methodology for model-based clustering that provides a principled statistical approach to these issues. We also show that this can be useful for other problems in multivariate analysis, such as discriminant analysis and multivariate density estimation. We give examples from medical diagnosis, minefield detection, cluster recovery from noisy data, and spatial density estimation. Finally, we mention limitations of the methodology and discuss recent developments in model-based clustering for non-Gaussian data, high-dimensional datasets, large datasets, and Bayesian estimation.},
  keywords = {Bayes factor,Breast cancer diagnosis,Cluster analysis,Density Estimation,Discriminant Analysis,EM algorithm,Gene expression microarray data,Markov chain Monte Carlo,Mixture model,Model-Based Clustering,Outliers,Spatial point process},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\SIN27RYC\\Fraley and Raftery - 2002 - Model-Based Clustering, Discriminant Analysis, and.pdf}
}

@article{fraleyModelBasedClusteringDiscriminant2002a,
  title = {Model-{{Based Clustering}}, {{Discriminant Analysis}}, and {{Density Estimation}}},
  author = {Fraley, Chris and Raftery, Adrian E},
  year = {2002},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {97},
  number = {458},
  pages = {611--631},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1198/016214502760047131},
  urldate = {2023-03-08},
  abstract = {Cluster analysis is the automated search for groups of related observations in a dataset. Most clustering done in practice is based largely on heuristic but intuitively reasonable procedures, and most clustering methods available in commercial software are also of this type. However, there is little systematic guidance associated with these methods for solving important practical questions that arise in cluster analysis, such as how many clusters are there, which clustering method should be used, and how should outliers be handled. We review a general methodology for model-based clustering that provides a principled statistical approach to these issues. We also show that this can be useful for other problems in multivariate analysis, such as discriminant analysis and multivariate density estimation. We give examples from medical diagnosis, minefield detection, cluster recovery from noisy data, and spatial density estimation. Finally, we mention limitations of the methodology and discuss recent developments in model-based clustering for non-Gaussian data, high-dimensional datasets, large datasets, and Bayesian estimation.},
  keywords = {Bayes factor,Breast cancer diagnosis,Cluster analysis,Density Estimation,Discriminant Analysis,EM algorithm,Gene expression microarray data,Markov chain Monte Carlo,Mixture model,Model-Based Clustering,Outliers,Spatial point process},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\G3VN8PRC\\Fraley and Raftery - 2002 - Model-Based Clustering, Discriminant Analysis, and.pdf}
}

@article{FreedmanDiaconisRule2022,
  title = {Freedman\textendash{{Diaconis}} Rule},
  year = {2022},
  month = jul,
  journal = {Wikipedia},
  urldate = {2023-04-30},
  abstract = {In statistics, the Freedman\textendash Diaconis rule can be used to select the width of the bins to be used in a histogram. It is named after David A. Freedman and Persi Diaconis.  For a set of empirical measurements sampled from some probability distribution, the Freedman-Diaconis rule is designed roughly to minimize the integral of the squared difference between the histogram (i.e., relative frequency density) and the density of the theoretical probability distribution. The general equation for the rule is:                                   Bin width                  =         2                                                                          IQR                              (               x               )                                                          n                                    3                                                                                  \{\textbackslash displaystyle \{\textbackslash text\{Bin width\}\}=2\textbackslash,\{\{\textbackslash text\{IQR\}\}(x) \textbackslash over \{\textbackslash sqrt[\{3\}]\{n\}\}\}\}   where                         IQR         ⁡         (         x         )                 \{\textbackslash displaystyle \textbackslash operatorname \{IQR\} (x)\}    is the interquartile range of the data and                         n                 \{\textbackslash displaystyle n\}    is the number of observations in the sample                         x         .                 \{\textbackslash displaystyle x.\}},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1101307943},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\6UWT9NXM\\Freedman–Diaconis_rule.html}
}

@article{FreedmanDiaconisRule2022,
  title = {Freedman\textendash{{Diaconis}} Rule},
  year = {2022},
  month = jul,
  journal = {Wikipedia},
  urldate = {2023-04-30},
  abstract = {In statistics, the Freedman\textendash Diaconis rule can be used to select the width of the bins to be used in a histogram. It is named after David A. Freedman and Persi Diaconis. For a set of empirical measurements sampled from some probability distribution, the Freedman-Diaconis rule is designed roughly to minimize the integral of the squared difference between the histogram (i.e., relative frequency density) and the density of the theoretical probability distribution. The general equation for the rule is: Bin width = 2 IQR ( x ) n 3 \{\textbackslash displaystyle \{\textbackslash text\{Bin width\}\}=2\textbackslash,\{\{\textbackslash text\{IQR\}\}(x) \textbackslash over \{\textbackslash sqrt[\{3\}]\{n\}\}\}\} where IQR ⁡ ( x ) \{\textbackslash displaystyle \textbackslash operatorname \{IQR\} (x)\} is the interquartile range of the data and n \{\textbackslash displaystyle n\} is the number of observations in the sample x . \{\textbackslash displaystyle x.\}},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\B569AYYV\\Freedman–Diaconis_rule.html}
}

@misc{freedmanHistogrameDensityEstimator,
  title = {On the {{Histograme}} as a {{Density Estimator}}: {{L2 Theory}}},
  author = {Freedman, David and Diaconis, Persi},
  journal = {CiteSeerX},
  urldate = {2023-04-30},
  howpublished = {https://citeseerx.ist.psu.edu/doc/10.1.1.650.2473},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\WKN5JI45\\10.1.1.650.html}
}

@article{galiliDendextendPackageVisualizing2015,
  title = {Dendextend: An {{R}} Package for Visualizing, Adjusting, and Comparing Trees of Hierarchical Clustering},
  author = {Galili, Tal},
  year = {2015},
  journal = {Bioinformatics (Oxford, England)},
  eprint = {https://academic.oup.com/bioinformatics/article-pdf/31/22/3718/17122682/btv428.pdf},
  doi = {10.1093/bioinformatics/btv428}
}

@article{galiliDendextendPackageVisualizing2015a,
  title = {Dendextend: An {{R}} Package for Visualizing, Adjusting, and Comparing Trees of Hierarchical Clustering},
  author = {Galili, Tal},
  year = {2015},
  journal = {Bioinformatics (Oxford, England)},
  eprint = {https://academic.oup.com/bioinformatics/article-pdf/31/22/3718/17122682/btv428.pdf},
  doi = {10.1093/bioinformatics/btv428}
}

@book{geronHandsonMachineLearning2019,
  title = {Hands-on {{Machine Learning}} with {{Scikit-Learn}}, {{Keras}}, and {{TensorFlow}}: {{Concepts}}, {{Tools}}, and {{Techniques}} to {{Build Intelligent Systems}}},
  author = {Geron, Aurelien},
  year = {2019},
  edition = {2nd edition},
  publisher = {{O'Reilly}},
  address = {{Cambridge}},
  isbn = {978-1-4920-3261-8}
}

@book{geronHandsonMachineLearning2019,
  title = {Hands-on {{Machine Learning}} with {{Scikit-Learn}}, {{Keras}}, and {{TensorFlow}}: {{Concepts}}, {{Tools}}, and {{Techniques}} to {{Build Intelligent Systems}}},
  author = {Geron, Aurelien},
  year = {2019},
  edition = {2nd edition},
  publisher = {{O'Reilly}},
  address = {{Cambridge}},
  isbn = {978-1-4920-3261-8}
}

@book{geronHandsOnMachineLearning2022,
  title = {Hands-{{On Machine Learning}} with {{Scikit-Learn}}, {{Keras}}, and {{TensorFlow}}},
  author = {G{\'e}ron, Aur{\'e}lien},
  year = {2022},
  edition = {3rd ed.},
  publisher = {{O'Reilly Media, Inc.}},
  isbn = {978-1-09-812596-7},
  langid = {english},
  keywords = {keras,Machine Learning,scikit-learn,tensorflow}
}

@book{geronHandsOnMachineLearning2022a,
  title = {Hands-{{On Machine Learning}} with {{Scikit-Learn}}, {{Keras}}, and {{TensorFlow}}},
  author = {G{\'e}ron, Aur{\'e}lien},
  year = {2022},
  edition = {3rd ed.},
  publisher = {{O'Reilly Media, Inc.}},
  isbn = {978-1-09-812596-7},
  langid = {english},
  keywords = {keras,Machine Learning,scikit-learn,tensorflow}
}

@misc{GgplotFlipbook,
  title = {The Ggplot Flipbook},
  urldate = {2023-02-21},
  howpublished = {https://evamaerey.github.io/ggplot\_flipbook/ggplot\_flipbook\_xaringan.html\#1},
  keywords = {ggplot,R,visualisation},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\RYFVHGB9\\ggplot_flipbook_xaringan.html}
}

@misc{GgplotFlipbook,
  title = {The Ggplot Flipbook},
  urldate = {2023-02-21},
  keywords = {ggplot,R,visualisation},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\9AEKF8NH\\ggplot_flipbook_xaringan.html}
}

@inproceedings{ghoshOptimumChoiceNearest2006,
  title = {On Optimum Choice of k in Nearest Neighbor Classification},
  author = {Ghosh, A.},
  year = {2006},
  urldate = {2023-03-07},
  abstract = {A major issue in k-nearest neighbor classification is how to choose the optimum value of the neighborhood parameter k. Popular cross-validation techniques often fail to guide us well in selecting k mainly due to the presence of multiple minimizers of the estimated misclassification rate. This article investigates a Bayesian method in this connection, which solves the problem of multiple optimizers. The utility of the proposed method is illustrated using some benchmark data sets. \textcopyright{} 2005 Elsevier B.V. All rights reserved.},
  keywords = {Classification,KNN}
}

@inproceedings{ghoshOptimumChoiceNearest2006a,
  title = {On Optimum Choice of k in Nearest Neighbor Classification},
  author = {Ghosh, A.},
  year = {2006},
  urldate = {2023-03-07},
  abstract = {A major issue in k-nearest neighbor classification is how to choose the optimum value of the neighborhood parameter k. Popular cross-validation techniques often fail to guide us well in selecting k mainly due to the presence of multiple minimizers of the estimated misclassification rate. This article investigates a Bayesian method in this connection, which solves the problem of multiple optimizers. The utility of the proposed method is illustrated using some benchmark data sets. \textcopyright{} 2005 Elsevier B.V. All rights reserved.},
  keywords = {Classification,KNN}
}

@book{gohelFlextableFunctionsTabular2023,
  title = {Flextable: {{Functions}} for {{Tabular Reporting}}},
  author = {Gohel, David and Skintzos, Panagiotis},
  year = {2023}
}

@book{gohelFlextableFunctionsTabular2023a,
  title = {Flextable: {{Functions}} for {{Tabular Reporting}}},
  author = {Gohel, David and Skintzos, Panagiotis},
  year = {2023}
}

@manual{gohelFlextableFunctionsTabular2023b,
  type = {Manual},
  title = {Flextable: {{Functions}} for Tabular Reporting},
  author = {Gohel, David and Skintzos, Panagiotis},
  year = {2023}
}

@manual{gohelFlextableFunctionsTabular2023c,
  type = {Manual},
  title = {Flextable: {{Functions}} for Tabular Reporting},
  author = {Gohel, David and Skintzos, Panagiotis},
  year = {2023}
}

@book{grolemundDataScience2017,
  title = {R for {{Data Science}}},
  author = {Grolemund, Garrett and Wickham, Hadley},
  year = {2017},
  publisher = {{O'Reilly}}
}

@book{grolemundDataScience2017a,
  title = {R for {{Data Science}}},
  author = {Grolemund, Garrett and Wickham, Hadley},
  year = {2017},
  publisher = {{O'Reilly}}
}

@book{grusDataScienceScratch2019,
  title = {Data {{Science}} from {{Scratch}}: {{First Principles}} with {{Python}}},
  author = {Grus, Joel},
  year = {2019},
  edition = {2nd edition},
  publisher = {{O'Reilly}},
  address = {{Cambridge}},
  isbn = {978-1-4920-4110-8}
}

@book{grusDataScienceScratch2019,
  title = {Data {{Science}} from {{Scratch}}: {{First Principles}} with {{Python}}},
  author = {Grus, Joel},
  year = {2019},
  edition = {2nd edition},
  publisher = {{O'Reilly}},
  address = {{Cambridge}},
  isbn = {978-1-4920-4110-8}
}

@book{guttagIntroductionComputationProgramming2016,
  title = {Introduction to Computation and Programming Using {{Python}}: With Application to Understanding Data},
  author = {Guttag, John},
  year = {2016},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  isbn = {0-262-52962-9}
}

@book{guttagIntroductionComputationProgramming2016,
  title = {Introduction to Computation and Programming Using {{Python}}: {{With}} Application to Understanding Data},
  author = {Guttag, John},
  year = {2016},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  isbn = {0-262-52962-9}
}

@misc{HandsonPythonTutorial,
  title = {Hands-on {{Python Tutorial}} \textemdash{} {{Hands-on Python Tutorial}} for {{Python}} 3.1},
  howpublished = {https://anh.cs.luc.edu/python/hands-on/3.1/handsonHtml/index.html\#}
}

@misc{HandsonPythonTutorial,
  title = {Hands-on {{Python Tutorial}} \textemdash{} {{Hands-on Python Tutorial}} for {{Python}} 3.1}
}

@article{harrisArrayProgrammingNumPy2020,
  title = {Array Programming with {{NumPy}}},
  author = {Harris, Charles R. and Millman, K. Jarrod and Van Der Walt, St{\'e}fan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and Van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and Del R{\'i}o, Jaime Fern{\'a}ndez and Wiebe, Mark and Peterson, Pearu and {G{\'e}rard-Marchant}, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
  year = {2020},
  month = sep,
  journal = {Nature},
  volume = {585},
  number = {7825},
  pages = {357--362},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-020-2649-2},
  urldate = {2023-05-18},
  abstract = {Abstract                            Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves               1               and in the first imaging of a black hole               2               . Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\WN2KVGMA\\Harris et al. - 2020 - Array programming with NumPy.pdf}
}

@article{hashimStudentPerformancePrediction2020,
  title = {Student {{Performance Prediction Model}} Based on {{Supervised Machine Learning Algorithms}}},
  author = {Hashim, Ali Salah and Awadh, Wid Akeel and Hamoud, Alaa Khalaf},
  year = {2020},
  month = nov,
  journal = {IOP Conference Series: Materials Science and Engineering},
  volume = {928},
  number = {3},
  pages = {032019},
  publisher = {{IOP Publishing}},
  issn = {1757-899X},
  doi = {10.1088/1757-899X/928/3/032019},
  urldate = {2023-05-14},
  abstract = {Higher education institutions aim to forecast student success which is an important research subject. Forecasting student success can enable teachers to prevent students from dropping out before final examinations, identify those who need additional help and boost institution ranking and prestige. Machine learning techniques in educational data mining aim to develop a model for discovering meaningful hidden patterns and exploring useful information from educational settings. The key traditional characteristics of students (demographic, academic background and behavioural features) are the main essential factors that can represent the training dataset for supervised machine learning algorithms. In this study, we compared the performances of several supervised machine learning algorithms, such as Decision Tree, Na\"ive Bayes, Logistic Regression, Support Vector Machine, K-Nearest Neighbour, Sequential Minimal Optimisation and Neural Network. We trained a model by using datasets provided by courses in the bachelor study programmes of the College of Computer Science and Information Technology, University of Basra, for academic years 2017\textendash 2018 and 2018\textendash 2019 to predict student performance on final examinations. Results indicated that logistic regression classifier is the most accurate in predicting the exact final grades of students (68.7\% for passed and 88.8\% for failed).},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\BWNIRTC4\\Hashim et al. - 2020 - Student Performance Prediction Model based on Supe.pdf}
}

@article{hashimStudentPerformancePrediction2020a,
  title = {Student {{Performance Prediction Model}} Based on {{Supervised Machine Learning Algorithms}}},
  author = {Hashim, Ali Salah and Awadh, Wid Akeel and Hamoud, Alaa Khalaf},
  year = {2020},
  month = nov,
  journal = {IOP Conference Series. Materials Science and Engineering},
  volume = {928},
  number = {3},
  publisher = {{IOP Publishing}},
  address = {{Bristol, United Kingdom}},
  issn = {17578981},
  doi = {10.1088/1757-899X/928/3/032019},
  urldate = {2023-05-14},
  abstract = {Higher education institutions aim to forecast student success which is an important research subject. Forecasting student success can enable teachers to prevent students from dropping out before final examinations, identify those who need additional help and boost institution ranking and prestige. Machine learning techniques in educational data mining aim to develop a model for discovering meaningful hidden patterns and exploring useful information from educational settings. The key traditional characteristics of students (demographic, academic background and behavioural features) are the main essential factors that can represent the training dataset for supervised machine learning algorithms. In this study, we compared the performances of several supervised machine learning algorithms, such as Decision Tree, Na\"ive Bayes, Logistic Regression, Support Vector Machine, K-Nearest Neighbour, Sequential Minimal Optimisation and Neural Network. We trained a model by using datasets provided by courses in the bachelor study programmes of the College of Computer Science and Information Technology, University of Basra, for academic years 2017\textendash 2018 and 2018\textendash 2019 to predict student performance on final examinations. Results indicated that logistic regression classifier is the most accurate in predicting the exact final grades of students (68.7\% for passed and 88.8\% for failed).},
  copyright = {\textcopyright{} 2020. This work is published under http://creativecommons.org/licenses/by/3.0/ (the ``License''). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  langid = {english},
  keywords = {Algorithms,Colleges \& universities,Data mining,Datasets,Decision trees,Engineering--Engineering Mechanics And Materials,Higher education institutions,Machine learning,Mathematical models,Neural networks,Optimization,Performance prediction,Physics--Mechanics,Prediction models,Students,Support vector machines},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\RURSQCIR\\Hashim et al. - 2020 - Student Performance Prediction Model based on Supe.pdf}
}

@book{hastieElementsStatisticalLearning2009,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  edition = {2nd edition}
}

@book{hastieElementsStatisticalLearning2009a,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  edition = {2nd edition}
}

@article{haytonFactorRetentionDecisions2004,
  title = {Factor {{Retention Decisions}} in {{Exploratory Factor Analysis}}: A {{Tutorial}} on {{Parallel Analysis}}},
  shorttitle = {Factor {{Retention Decisions}} in {{Exploratory Factor Analysis}}},
  author = {Hayton, James C. and Allen, David G. and Scarpello, Vida},
  year = {2004},
  month = apr,
  journal = {Organizational Research Methods},
  volume = {7},
  number = {2},
  pages = {191--205},
  publisher = {{SAGE Publications Inc}},
  issn = {1094-4281},
  doi = {10.1177/1094428104263675},
  urldate = {2023-02-11},
  abstract = {The decision of how many factors to retain is a critical component of exploratory factor analysis. Evidence is presented that parallel analysis is one of the most accurate factor retention methods while also being one of the most underutilized in management and organizational research. Therefore, a step-by-step guide to performing parallel analysis is described, and an example is provided using data from the Minnesota Satisfaction Questionnaire. Recommendations for making factor retention decisions are discussed.},
  keywords = {EFA,Parallel Analysis},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\IVZU3W4X\\Hayton et al. - 2004 - Factor Retention Decisions in Exploratory Factor A.pdf}
}

@article{haytonFactorRetentionDecisions2004,
  title = {Factor {{Retention Decisions}} in {{Exploratory Factor Analysis}}: {{A Tutorial}} on {{Parallel Analysis}}},
  shorttitle = {Factor {{Retention Decisions}} in {{Exploratory Factor Analysis}}},
  author = {Hayton, James C. and Allen, David G. and Scarpello, Vida},
  year = {2004},
  month = apr,
  journal = {Organizational Research Methods},
  volume = {7},
  number = {2},
  pages = {191--205},
  publisher = {{SAGE Publications Inc}},
  issn = {1094-4281},
  doi = {10.1177/1094428104263675},
  urldate = {2023-02-11},
  abstract = {The decision of how many factors to retain is a critical component of exploratory factor analysis. Evidence is presented that parallel analysis is one of the most accurate factor retention methods while also being one of the most underutilized in management and organizational research. Therefore, a step-by-step guide to performing parallel analysis is described, and an example is provided using data from the Minnesota Satisfaction Questionnaire. Recommendations for making factor retention decisions are discussed.},
  keywords = {EFA,Parallel Analysis},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\LBP9J5LH\\Hayton et al. - 2004 - Factor Retention Decisions in Exploratory Factor A.pdf}
}

@manual{hesterGlueInterpretedString2022,
  type = {Manual},
  title = {Glue: {{Interpreted}} String Literals},
  author = {Hester, Jim and Bryan, Jennifer},
  year = {2022}
}

@manual{hesterGlueInterpretedString2022a,
  type = {Manual},
  title = {Glue: {{Interpreted}} String Literals},
  author = {Hester, Jim and Bryan, Jennifer},
  year = {2022}
}

@incollection{hiran2021,
  booktitle = {Machine {{Learning}} for {{Sustainable Development}}},
  author = {Hiran, Kamal Kant and Khazanchi, Deepak and Vyas, Ajay Kumar and Padmanaban, Sanjeevikumar},
  year = {2021},
  month = jul,
  publisher = {{Walter de Gruyter GmbH \& Co KG}},
  abstract = {The book will focus on the applications of machine learning for sustainable development. Machine learning (ML) is an emerging technique whose diffusion and adoption in various sectors (such as energy, agriculture, internet of things, infrastructure) will be of enormous benefit. The state of the art of machine learning models is most useful for forecasting and prediction of various sectors for sustainable development.},
  googlebooks = {Lo89EAAAQBAJ},
  isbn = {978-3-11-070251-4},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition,Computers / Artificial Intelligence / General,Computers / Computer Science,Computers / Data Science / Neural Networks,Computers / Information Technology,Computers / Networking / General,Computers / Programming / Algorithms,Computers / Programming / General}
}

@book{hiranMachineLearningSustainable2021,
  title = {Machine {{Learning}} for {{Sustainable Development}}},
  author = {Hiran, Kamal Kant and Khazanchi, Deepak and Vyas, Ajay Kumar and Padmanaban, Sanjeevikumar},
  year = {2021},
  month = jul,
  publisher = {{Walter de Gruyter GmbH \& Co KG}},
  abstract = {The book will focus on the applications of machine learning for sustainable development. Machine learning (ML) is an emerging technique whose diffusion and adoption in various sectors (such as energy, agriculture, internet of things, infrastructure) will be of enormous benefit. The state of the art of machine learning models is most useful for forecasting and prediction of various sectors for sustainable development.},
  googlebooks = {Lo89EAAAQBAJ},
  isbn = {978-3-11-070251-4},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition,Computers / Artificial Intelligence / General,Computers / Computer Science,Computers / Data Science / Neural Networks,Computers / Information Technology,Computers / Networking / General,Computers / Programming / Algorithms,Computers / Programming / General}
}

@book{huntBeginnersGuidePython2019,
  title = {Beginners {{Guide}} to {{Python}} 3 {{Programming}}},
  author = {Hunt, John},
  year = {2019},
  month = aug,
  edition = {1st ed. 2019},
  publisher = {{Springer Nature Switzerland AG}},
  address = {{Cham}},
  isbn = {978-3-030-20289-7}
}

@book{huntBeginnersGuidePython2019,
  title = {Beginners {{Guide}} to {{Python}} 3 {{Programming}}},
  author = {Hunt, John},
  year = {2019},
  month = aug,
  edition = {1st ed. 2019},
  publisher = {{Springer Nature Switzerland AG}},
  address = {{Cham}},
  isbn = {978-3-030-20289-7}
}

@book{idrisNumPyBeginnerGuide2015,
  title = {{{NumPy}} Beginner's Guide: Build Efficient, High-Speed Programs Using the High-Performance {{NumPy}} Mathematical Library},
  author = {Idris, Ivan},
  year = {2015},
  edition = {3rd ed},
  publisher = {{Packt Publishing}},
  address = {{Birmingham, England}},
  isbn = {1-78528-883-0}
}

@book{idrisNumPyBeginnerGuide2015,
  title = {{{NumPy}} Beginner's Guide: {{Build}} Efficient, High-Speed Programs Using the High-Performance {{NumPy}} Mathematical Library},
  author = {Idris, Ivan},
  year = {2015},
  edition = {3rd ed},
  publisher = {{Packt Publishing}},
  address = {{Birmingham, England}},
  isbn = {1-78528-883-0}
}

@misc{IntroducingIPythonIPython,
  title = {Introducing {{IPython}} \textemdash{} {{IPython}} 3.2.1 Documentation},
  howpublished = {https://ipython.org/ipython-doc/3/interactive/tutorial.html}
}

@misc{IntroducingIPythonIPython,
  title = {Introducing {{IPython}} \textemdash{} {{IPython}} 3.2.1 Documentation}
}

@book{Introduction,
  title = {1. {{Introduction}}},
  urldate = {2023-02-18},
  abstract = {Chapter 1. Introduction  ``Data! Data! Data!'' he cried impatiently. ``I can't make bricks without clay.'' Arthur Conan Doyle The Ascendance of Data  We live in a world that's drowning in...},
  isbn = {978-1-4920-4112-2},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\GYPYIRD5\\ch01.html}
}

@book{Introduction,
  title = {1. {{Introduction}}},
  urldate = {2023-02-18},
  abstract = {Chapter 1. Introduction ``Data! Data! Data!'' he cried impatiently. ``I can't make bricks without clay.'' Arthur Conan Doyle The Ascendance of Data We live in a world that's drowning in...},
  isbn = {978-1-4920-4112-2},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\HRCMY9UN\\ch01.html}
}

@book{jamesIntroductionStatisticalLearning2013,
  title = {An Introduction to Statistical Learning: With Applications in {{R}}},
  shorttitle = {An Introduction to Statistical Learning},
  author = {James, Gareth},
  year = {2013},
  series = {Springer Texts in Statistics},
  publisher = {{Springer}},
  address = {{New York}},
  collaborator = {Witten, Daniela and Hastie, Trevor and Tibshirani, Robert and {EBSCOhost}},
  isbn = {978-1-4614-7138-7},
  langid = {english},
  keywords = {Machine Learning,Mathematical models,Mathematical statistics,R (Computer program language),Statistics},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\UT5GXHHK\\James - 2013 - An introduction to statistical learning with appl.pdf}
}

@book{jamesIntroductionStatisticalLearning2013,
  title = {An Introduction to Statistical Learning: {{With}} Applications in {{R}}},
  shorttitle = {An Introduction to Statistical Learning},
  author = {James, Gareth},
  year = {2013},
  series = {Springer Texts in Statistics},
  publisher = {{Springer}},
  address = {{New York}},
  collaborator = {Witten, Daniela and Hastie, Trevor and Tibshirani, Robert and {EBSCOhost}},
  isbn = {978-1-4614-7138-7},
  langid = {english},
  keywords = {Machine Learning,Mathematical models,Mathematical statistics,R (Computer program language),Statistics},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\3KIXYUCR\\James - 2013 - An introduction to statistical learning with appl.pdf}
}

@incollection{jayaprakashOpportunitiesChallengesTransforming2021,
  title = {Opportunities and Challenges in Transforming Higher Education through Machine Learning},
  booktitle = {Machine {{Learning}} for {{Sustainable Development}}},
  author = {Jayaprakash, Sujith and Kathiresan, V. and Shanmugapriya, N. and Dadhich, Manish},
  year = {2021},
  month = jul,
  pages = {17--30},
  publisher = {{Walter de Gruyter GmbH \& Co KG}},
  googlebooks = {Lo89EAAAQBAJ},
  isbn = {978-3-11-070251-4},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition,Computers / Artificial Intelligence / General,Computers / Computer Science,Computers / Data Science / Neural Networks,Computers / Information Technology,Computers / Networking / General,Computers / Programming / Algorithms,Computers / Programming / General}
}

@book{JoelGrus2015DSfS,
  title = {Data Science from Scratch},
  author = {Grus, Joel},
  year = {2015},
  publisher = {{O'Reilly Media, Inc}},
  abstract = {Data science libraries, frameworks, modules, and toolkits are great for doing data science, but they're also a good way to dive into the discipline without actually understanding data science. In this book, you'll learn how many of the most fundamental data science tools and algorithms work by implementing them from scratch.If you have an aptitude for mathematics and some programming skills, author Joel Grus will help you get comfortable with the math and statistics at the core of data science, and with hacking skills you need to get started as a data scientist. Today's messy glut of data holds answers to questions no one's even thought to ask. This book provides you with the know-how to dig those answers out.Get a crash course in PythonLearn the basics of linear algebra, statistics, and probability\textemdash and understand how and when they're used in data scienceCollect, explore, clean, munge, and manipulate dataDive into the fundamentals of machine learningImplement models such as k-nearest Neighbors, Naive Bayes, linear and logistic regression, decision trees, neural networks, and clusteringExplore recommender systems, natural language processing, network analysis, MapReduce, and databases},
  isbn = {1-4919-0141-1},
  langid = {english}
}

@manual{kassambaraFactoextraExtractVisualize2020,
  type = {Manual},
  title = {Factoextra: {{Extract}} and Visualize the Results of Multivariate Data Analyses},
  author = {Kassambara, Alboukadel and Mundt, Fabian},
  year = {2020}
}

@manual{kassambaraFactoextraExtractVisualize2020a,
  type = {Manual},
  title = {Factoextra: {{Extract}} and Visualize the Results of Multivariate Data Analyses},
  author = {Kassambara, Alboukadel and Mundt, Fabian},
  year = {2020}
}

@manual{kassambaraGgcorrplotVisualizationCorrelation2022,
  type = {Manual},
  title = {Ggcorrplot: {{Visualization}} of a Correlation Matrix Using 'Ggplot2'},
  author = {Kassambara, Alboukadel},
  year = {2022}
}

@manual{kassambaraGgcorrplotVisualizationCorrelation2022a,
  type = {Manual},
  title = {Ggcorrplot: {{Visualization}} of a Correlation Matrix Using 'Ggplot2'},
  author = {Kassambara, Alboukadel},
  year = {2022}
}

@article{kuzilekOpenUniversityLearning2017,
  title = {Open {{University Learning Analytics}} Dataset},
  author = {Kuzilek, Jakub and Hlosta, Martin and Zdrahal, Zdenek},
  year = {2017},
  month = nov,
  journal = {Scientific Data},
  volume = {4},
  number = {1},
  pages = {170171},
  issn = {2052-4463},
  doi = {10.1038/sdata.2017.171},
  urldate = {2023-05-18},
  abstract = {Abstract                            Learning Analytics focuses on the collection and analysis of learners' data to improve their learning experience by providing informed guidance and to optimise learning materials. To support the research in this area we have developed a dataset, containing data from courses presented at the Open University (OU). What makes the dataset unique is the fact that it contains demographic data together with aggregated clickstream data of students' interactions in the Virtual Learning Environment (VLE). This enables the analysis of student behaviour, represented by their actions. The dataset contains the information about 22 courses, 32,593 students, their assessment results, and logs of their interactions with the VLE represented by daily summaries of student clicks (10,655,280 entries). The dataset is freely available at               https://analyse.kmi.open.ac.uk/open\_dataset               under a CC-BY 4.0 license.},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\YI68KAM9\\sdata2017171.pdf}
}

@article{kuzilekOpenUniversityLearning2017a,
  title = {Open {{University Learning Analytics}} Dataset},
  author = {Kuzilek, Jakub and Hlosta, Martin and Zdrahal, Zdenek},
  year = {2017},
  month = nov,
  journal = {Scientific Data},
  volume = {4},
  number = {1},
  pages = {170171},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/sdata.2017.171},
  urldate = {2023-05-18},
  abstract = {Learning Analytics focuses on the collection and analysis of learners' data to improve their learning experience by providing informed guidance and to optimise learning materials. To support the research in this area we have developed a dataset, containing data from courses presented at the Open University (OU). What makes the dataset unique is the fact that it contains demographic data together with aggregated clickstream data of students' interactions in the Virtual Learning Environment (VLE). This enables the analysis of student behaviour, represented by their actions. The dataset contains the information about 22 courses, 32,593 students, their assessment results, and logs of their interactions with the VLE represented by daily summaries of student clicks (10,655,280 entries). The dataset is freely available at https://analyse.kmi.open.ac.uk/open\_datasetunder a CC-BY 4.0 license.},
  copyright = {2017 The Author(s)},
  langid = {english},
  keywords = {Computer science,Education,Scientific data,Statistics},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\Q9D47IER\\Kuzilek et al. - 2017 - Open University Learning Analytics dataset.pdf}
}

@article{liawClassificationRegressionRandomForest2002,
  title = {Classification and Regression by {{randomForest}}},
  author = {Liaw, Andy and Wiener, Matthew},
  year = {2002},
  journal = {R News},
  volume = {2},
  number = {3},
  pages = {18--22}
}

@article{liawClassificationRegressionRandomForest2002a,
  title = {Classification and Regression by {{randomForest}}},
  author = {Liaw, Andy and Wiener, Matthew},
  year = {2002},
  journal = {R News},
  volume = {2},
  number = {3},
  pages = {18--22}
}

@book{lutzProgrammingPython0000,
  title = {Programming {{Python}}},
  author = {Lutz, Mark},
  year = {0000 c},
  edition = {4th ed},
  publisher = {{O'Reilly}},
  address = {{Beijing}},
  isbn = {978-1-4493-0285-6}
}

@book{lutzProgrammingPython0000,
  title = {Programming {{Python}}},
  author = {Lutz, Mark},
  year = {0000 c},
  edition = {4th ed},
  publisher = {{O'Reilly}},
  address = {{Beijing}},
  isbn = {978-1-4493-0285-6}
}

@misc{MachineLearningHigher,
  title = {Machine Learning in Higher Education | {{McKinsey}}},
  urldate = {2023-03-26},
  keywords = {HE,ML},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\4JQLTC63\\using-machine-learning-to-improve-student-success-in-higher-education.html}
}

@misc{MachineLearningHighera,
  title = {Machine Learning in Higher Education | {{McKinsey}}},
  urldate = {2023-03-26},
  howpublished = {https://www.mckinsey.com/industries/education/our-insights/using-machine-learning-to-improve-student-success-in-higher-education},
  keywords = {HE,ML},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\KARRE25U\\using-machine-learning-to-improve-student-success-in-higher-education.html}
}

@manual{maechlerClusterClusterAnalysis2022,
  type = {Manual},
  title = {Cluster: {{Cluster}} Analysis Basics and Extensions},
  author = {Maechler, Martin and Rousseeuw, Peter and Struyf, Anja and Hubert, Mia and Hornik, Kurt},
  year = {2022}
}

@manual{maechlerClusterClusterAnalysis2022a,
  type = {Manual},
  title = {Cluster: {{Cluster}} Analysis Basics and Extensions},
  author = {Maechler, Martin and Rousseeuw, Peter and Struyf, Anja and Hubert, Mia and Hornik, Kurt},
  year = {2022}
}

@article{martinianiStructuralAnalysisHighdimensional2016,
  title = {Structural Analysis of High-Dimensional Basins of Attraction},
  author = {Martiniani, Stefano and Schrenk, K. Julian and Stevenson, Jacob D. and Wales, David J. and Frenkel, Daan},
  year = {2016},
  month = sep,
  journal = {Physical Review E},
  volume = {94},
  number = {3},
  eprint = {1603.09627},
  primaryclass = {cond-mat, physics:physics},
  pages = {031301},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.94.031301},
  urldate = {2023-03-07},
  abstract = {We propose an efficient Monte Carlo method for the computation of the volumes of high-dimensional bodies with arbitrary shape. We start with a region of known volume within the interior of the manifold and then use the multistate Bennett acceptance-ratio method to compute the dimensionless free-energy difference between a series of equilibrium simulations performed within this object. The method produces results that are in excellent agreement with thermodynamic integration, as well as a direct estimate of the associated statistical uncertainties. The histogram method also allows us to directly obtain an estimate of the interior radial probability density profile, thus yielding useful insight into the structural properties of such a high-dimensional body. We illustrate the method by analyzing the effect of structural disorder on the basins of attraction of mechanically stable packings of soft repulsive spheres.},
  archiveprefix = {arxiv},
  keywords = {Cluster,Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Soft Condensed Matter,Condensed Matter - Statistical Mechanics,Dimensionality,ML,Physics - Computational Physics},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\RHTSV59G\\Martiniani et al. - 2016 - Structural analysis of high-dimensional basins of .pdf;C\:\\Users\\zoona\\Zotero\\storage\\TL293HHM\\1603.html}
}

@article{martinianiStructuralAnalysisHighdimensional2016,
  title = {Structural Analysis of High-Dimensional Basins of Attraction},
  author = {Martiniani, Stefano and Schrenk, K. Julian and Stevenson, Jacob D. and Wales, David J. and Frenkel, Daan},
  year = {2016},
  month = sep,
  journal = {Physical Review E},
  volume = {94},
  number = {3},
  eprint = {1603.09627},
  primaryclass = {cond-mat, physics:physics},
  pages = {031301},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.94.031301},
  urldate = {2023-03-07},
  abstract = {We propose an efficient Monte Carlo method for the computation of the volumes of high-dimensional bodies with arbitrary shape. We start with a region of known volume within the interior of the manifold and then use the multistate Bennett acceptance-ratio method to compute the dimensionless free-energy difference between a series of equilibrium simulations performed within this object. The method produces results that are in excellent agreement with thermodynamic integration, as well as a direct estimate of the associated statistical uncertainties. The histogram method also allows us to directly obtain an estimate of the interior radial probability density profile, thus yielding useful insight into the structural properties of such a high-dimensional body. We illustrate the method by analyzing the effect of structural disorder on the basins of attraction of mechanically stable packings of soft repulsive spheres.},
  archiveprefix = {arxiv},
  keywords = {Cluster,Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Soft Condensed Matter,Condensed Matter - Statistical Mechanics,Dimensionality,ML,Physics - Computational Physics},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\IHF6B3LP\\Martiniani et al. - 2016 - Structural analysis of high-dimensional basins of .pdf;C\:\\Users\\zoona\\Zotero\\storage\\JI6EPPA3\\1603.html}
}

@misc{MBCBookWebpage,
  title = {The {{MBC}} Book Webpage},
  urldate = {2023-02-11},
  howpublished = {https://math.unice.fr/\textasciitilde cbouveyr/MBCbook/},
  keywords = {R},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\ZJEEXQKS\\MBCbook.html}
}

@misc{MBCBookWebpage,
  title = {The {{MBC}} Book Webpage},
  urldate = {2023-02-11},
  keywords = {R},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\6HN9TZQP\\MBCbook.html}
}

@book{mckinneyPythonDataAnalysis2017,
  title = {Python for Data Analysis: Data Wrangling with {{Pandas}}, {{NumPy}}, and {{IPython}}},
  author = {McKinney, Wes},
  year = {2017},
  edition = {Second edition},
  publisher = {{O'Reilly}},
  address = {{Beijing}},
  isbn = {978-1-4919-5763-9}
}

@book{mckinneyPythonDataAnalysis2017,
  title = {Python for Data Analysis: {{Data}} Wrangling with {{Pandas}}, {{NumPy}}, and {{IPython}}},
  author = {McKinney, Wes},
  year = {2017},
  edition = {Second edition},
  publisher = {{O'Reilly}},
  address = {{Beijing}},
  isbn = {978-1-4919-5763-9}
}

@book{mckinneyPythonDataAnalysis2017a,
  title = {Python for Data Analysis: Data Wrangling with {{Pandas}}, {{NumPy}}, and {{IPython}}},
  author = {McKinney, Wes},
  year = {2017},
  edition = {Second edition},
  publisher = {{O'Reilly}},
  address = {{Beijing}},
  isbn = {978-1-4919-5763-9}
}

@book{mckinneyPythonDataAnalysis2017a,
  title = {Python for Data Analysis: {{Data}} Wrangling with {{Pandas}}, {{NumPy}}, and {{IPython}}},
  author = {McKinney, Wes},
  year = {2017},
  edition = {Second edition},
  publisher = {{O'Reilly}},
  address = {{Beijing}},
  isbn = {978-1-4919-5763-9}
}

@manual{meyerE1071MiscFunctions2023,
  type = {Manual},
  title = {E1071: {{Misc}} Functions of the Department of Statistics, Probability Theory Group (Formerly: {{E1071}}), {{TU}} Wien},
  author = {Meyer, David and Dimitriadou, Evgenia and Hornik, Kurt and Weingessel, Andreas and Leisch, Friedrich},
  year = {2023}
}

@manual{meyerE1071MiscFunctions2023a,
  type = {Manual},
  title = {E1071: {{Misc}} Functions of the Department of Statistics, Probability Theory Group (Formerly: {{E1071}}), {{TU}} Wien},
  author = {Meyer, David and Dimitriadou, Evgenia and Hornik, Kurt and Weingessel, Andreas and Leisch, Friedrich},
  year = {2023}
}

@article{moubayedStudentEngagementLevel2020,
  title = {Student {{Engagement Level}} in an E-{{Learning Environment}}: {{Clustering Using K-means}}},
  shorttitle = {Student {{Engagement Level}} in an E-{{Learning Environment}}},
  author = {Moubayed, Abdallah and Injadat, Mohammadnoor and Shami, Abdallah and Lutfiyya, Hanan},
  year = {2020},
  month = apr,
  journal = {American Journal of Distance Education},
  volume = {34},
  number = {2},
  pages = {137--156},
  publisher = {{Routledge}},
  issn = {0892-3647},
  doi = {10.1080/08923647.2020.1696140},
  urldate = {2023-05-14},
  abstract = {E-learning platforms and processes face several challenges, among which is the idea of personalizing the e-learning experience and to keep students motivated and engaged. This work is part of a larger study that aims to tackle these two challenges using a variety of machine learning techniques. To that end, this paper proposes the use of k-means algorithm to cluster students based on 12 engagement metrics divided into two categories: interaction-related and effort-related. Quantitative analysis is performed to identify the students that are not engaged who may need help. Three different clustering models are considered: two-level, three-level, and five-level. The considered dataset is the students' event log of a second-year undergraduate Science course from a North American university that was given in a blended format. The event log is transformed using MATLAB to generate a new dataset representing the considered metrics. Experimental results' analysis shows that among the considered interaction-related and effort-related metrics, the number of logins and the average duration to submit assignments are the most representative of the students' engagement level. Furthermore, using the silhouette coefficient as a performance metric, it is shown that the two-level model offers the best performance in terms of cluster separation. However, the three-level model has a similar performance while better identifying students with low engagement levels.}
}

@book{murphyProbabilisticMachineLearning2022,
  title = {Probabilistic {{Machine Learning}}: {{An}} Introduction},
  author = {Murphy, Kevin P.},
  year = {2022},
  publisher = {{MIT Press}},
  keywords = {Machine Learning,Probabilistic ML},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\MXH78UE7\\Murphy - 2022 - Probabilistic Machine Learning An introduction.pdf}
}

@book{murphyProbabilisticMachineLearning2022a,
  title = {Probabilistic {{Machine Learning}}: {{An}} Introduction},
  author = {Murphy, Kevin P.},
  year = {2022},
  publisher = {{MIT Press}},
  keywords = {Machine Learning,Probabilistic ML},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\EEWL6GCA\\Murphy - 2022 - Probabilistic Machine Learning An introduction.pdf}
}

@manual{neuwirthRColorBrewerColorBrewerPalettes2022,
  type = {Manual},
  title = {{{RColorBrewer}}: {{ColorBrewer}} Palettes},
  author = {Neuwirth, Erich},
  year = {2022}
}

@manual{neuwirthRColorBrewerColorBrewerPalettes2022a,
  type = {Manual},
  title = {{{RColorBrewer}}: {{ColorBrewer}} Palettes},
  author = {Neuwirth, Erich},
  year = {2022}
}

@article{newlandLearningAnalyticsUK2017,
  title = {Learning {{Analytics}} in {{UK HE}} 2017},
  author = {Newland, Dr Barbara},
  year = {2017},
  abstract = {Methodology ......................................................................................................................... 6 Results .................................................................................................................................. 6 Progression of Learning AnalytIcs implementation ........................................................... 7 Stage of Learning AnalytIcs developments.......................................................................8 Rate of implementation over the past 2 years .................................................................. 9 Change in level of understanding of senior mangagement.............................................10 Current focus of developments ....................................................................................... 11 Evidence of return on investment or impact....................................................................12 Management of Learning Analytics development ........................................................... 13 Biggest driver .................................................................................................................. 14 Biggest barrier.................................................................................................................15 More staff ........................................................................................................................ 16 Level of involvement as Head of eLearning .................................................................... 17 Change in this level of involvement ................................................................................ 18 Other significant Learning Analytics developments ........................................................ 19 Conclusion .......................................................................................................................... 19 References ......................................................................................................................... 20},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\DY9SC3NZ\\188257811.pdf}
}

@article{oqaidiStudentsDropoutPrediction2022,
  title = {Towards a {{Students}}' {{Dropout Prediction Model}} in {{Higher Education Institutions Using Machine Learning Algorithms}}},
  author = {Oqaidi, Khalid and Aouhassi, Sarah and Mansouri, Khalifa},
  year = {2022},
  month = sep,
  journal = {International Journal of Emerging Technologies in Learning (iJET)},
  volume = {17},
  number = {18},
  pages = {103--117},
  issn = {1863-0383},
  doi = {10.3991/ijet.v17i18.25567},
  urldate = {2023-05-14},
  abstract = {Using machine learning to predict students' dropout in higher education institutions and programs has proven to be effective in many use cases. In an approach based on machine learning algorithms to detect students at risk of dropout, there are three main factors: the choice of features likely to influence a partial or total stop of the student, the choice of the algorithm to implement a prediction model, and the choice of the evaluation metrics to monitor and assess the credibility of the results. This paper aims to provide a diagnosis of machine learning techniques used to detect students' dropout in higher education programs, a critical analysis of the limitations of the models proposed in the literature, as well as the major contribution of this arti-cle is to present recommendations that may resolve the lack of global model that can be generalized in all the higher education institutions at least in the same country or in the same university.},
  copyright = {Copyright (c) 2022 Khalid Oqaidi, Sarah Aouhassi, Khalifa Mansouri},
  langid = {english},
  keywords = {classi-fication},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\6M6EZEG7\\Oqaidi et al. - 2022 - Towards a Students’ Dropout Prediction Model in Hi.pdf}
}

@inproceedings{orjiUsingMachineLearning2020,
  title = {Using {{Machine Learning}} to {{Explore}} the {{Relation Between Student Engagement}} and {{Student Performance}}},
  booktitle = {2020 24th {{International Conference Information Visualisation}} ({{IV}})},
  author = {Orji, Fidelia and Vassileva, Julita},
  year = {2020},
  month = sep,
  pages = {480--485},
  issn = {2375-0138},
  doi = {10.1109/IV51561.2020.00083},
  abstract = {Engagement in learning activities is an important factor that affects student performance in education. According to research, student engagement involves the degree of passion, interest and attention that they exhibit in their educational environment. In the traditional learning system, educators encourage students to engage in their learning activities through various teaching strategies such as making them pay attention, take notes, ask questions and participate actively in the learning processes. Sometimes, educators call on a specific student to answer a question as a means of encouraging the student to participate in learning processes. Nowadays, engagement strategies for learning are changing, especially with the use of technology-enhanced learning systems (TELS) in education. As a result, improving the engagement level of students in online learning environments remains an open research question that needs to be explored. This research is part of a preliminary study on discovering ways of increasing student engagement in an online learning system through data-driven interventions. Student engagement in this research is determined using objective data (activity logs of a specific undergraduate course in a TELS). Activity log is unbiased data and a reflection of students' actual learning behaviours (uncontrolled). In this study, we mined the log of students' learning activities from a TELS used for an undergraduate course to explore differences between students' learning behaviours as they relate to their engagement level and academic performance (measured in terms of final grade points in a course). We employed supervised (Random Forest) and unsupervised (Clustering) machine learning approaches in exploring the relations. The approaches identified an interesting pattern on student engagement and show that engagement and assessment scores are good predictors of student academic performance. Assessment scores are measured with results of quizzes and assignments performed by the students in the TELS, while academic performance is measured with the final grade of the student in the course. The implications of our findings are discussed.},
  keywords = {academic performance,Adaptation models,clustering,Education,educational data mining,learning pattern,machine learning,Monitoring,online learning,Predictive models,random forest,Random forests,Reflection,Reliability,student engagement,student performance,supervised and unsupervised machine learning,technology-enhanced learning systems},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\NBCSGMWL\\9373084.html}
}

@article{osborneWhatRotatingExploratory,
  title = {What Is {{Rotating}} in {{Exploratory Factor Analysis}}?},
  author = {Osborne, Jason W.},
  publisher = {{University of Massachusetts Amherst}},
  doi = {10.7275/HB2G-M060},
  urldate = {2023-02-11},
  abstract = {Exploratory factor analysis (EFA) is one of the most commonly-reported quantitative methodology in the social sciences, yet much of the detail regarding what happens during an EFA remains unclear. The goal of this brief technical note is to explore what "rotation" is, what exactly is rotating, and why we use rotation when performing EFAs. Some commentary about the relative utility and desirability of different rotation methods concludes the narrative.},
  langid = {english},
  keywords = {EFA,Rotation},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\CV3QPYRZ\\Osborne - What is Rotating in Exploratory Factor Analysis.pdf}
}

@article{osborneWhatRotatingExploratorya,
  title = {What Is {{Rotating}} in {{Exploratory Factor Analysis}}?},
  author = {Osborne, Jason W.},
  publisher = {{University of Massachusetts Amherst}},
  doi = {10.7275/HB2G-M060},
  urldate = {2023-02-11},
  abstract = {Exploratory factor analysis (EFA) is one of the most commonly-reported quantitative methodology in the social sciences, yet much of the detail regarding what happens during an EFA remains unclear. The goal of this brief technical note is to explore what "rotation" is, what exactly is rotating, and why we use rotation when performing EFAs. Some commentary about the relative utility and desirability of different rotation methods concludes the narrative.},
  langid = {english},
  keywords = {EFA,Rotation},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\EVG7DXPE\\Osborne - What is Rotating in Exploratory Factor Analysis.pdf}
}

@article{pallathadkaClassificationPredictionStudent2023,
  title = {Classification and Prediction of Student Performance Data Using Various Machine Learning Algorithms},
  author = {Pallathadka, Harikumar and Wenda, Alex and {Ramirez-As{\'i}s}, Edwin and {As{\'i}s-L{\'o}pez}, Maximiliano and {Flores-Albornoz}, Judith and Phasinam, Khongdet},
  year = {2023},
  month = jan,
  journal = {Materials Today: Proceedings},
  series = {{{SI}}:5 {{NANO}} 2021},
  volume = {80},
  pages = {3782--3785},
  issn = {2214-7853},
  doi = {10.1016/j.matpr.2021.07.382},
  urldate = {2023-05-14},
  abstract = {In today's competitive world, it is critical for an institute to forecast student performance, classify individuals based on their talents, and attempt to enhance their performance in future tests. Students should be advised well in advance to concentrate their efforts in a specific area in order to improve their academic achievement. This type of analysis assists an institute in lowering its failure rates. Based on their prior performance in comparable courses, this study predicts students' performance in a course. Data mining is a collection of techniques used to uncover hidden patterns in massive amounts of existing data. These patterns may be valuable for analysis and prediction. Education data mining refers to the collection of data mining applications in the field of education. These applications are concerned with the analysis of data from students and teachers. The analysis might be used for categorization or prediction.Machine learning such as Nave Bayes, ID3, C4.5, and SVM are investigated. UCI machinery student performance data set is used in experimental study. Algorithms are analysed on certain parameters like- accuracy, error rate.},
  langid = {english},
  keywords = {Classification,Educational Data Mining,Machine Learning,Prediction,Student Performance},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\SKUBYCVE\\S221478532105241X.html}
}

@book{pearlCausalityModelsReasoning2000,
  title = {Causality: Models, Reasoning, and Inference},
  author = {Pearl, Judea},
  year = {2000},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  isbn = {978-0-511-80316-1}
}

@book{pearlCausalityModelsReasoning2000,
  title = {Causality: {{Models}}, Reasoning, and Inference},
  author = {Pearl, Judea},
  year = {2000},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  isbn = {978-0-511-80316-1}
}

@book{pedersenGgplot2ElegantGraphics,
  title = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}}},
  author = {Pedersen, Danielle Navarro, Hadley Wickham, {and} Thomas Lin},
  urldate = {2023-02-11},
  abstract = {A book created with bookdown.},
  langid = {english},
  keywords = {ggplot,R,Visualisation},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\NTS6BZM2\\ggplot2-book.org.html}
}

@book{pedersenGgplot2ElegantGraphicsa,
  title = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}}},
  author = {Pedersen, Danielle Navarro, {and} Thomas Lin, Hadley Wickham},
  urldate = {2023-02-11},
  abstract = {A book created with bookdown.},
  langid = {english},
  keywords = {ggplot,R,Visualisation},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\3T6R9A9I\\ggplot2-book.org.html}
}

@manual{pedersenPatchworkComposerPlots2022,
  type = {Manual},
  title = {Patchwork: {{The}} Composer of Plots},
  author = {Pedersen, Thomas Lin},
  year = {2022}
}

@manual{pedersenPatchworkComposerPlots2022a,
  type = {Manual},
  title = {Patchwork: {{The}} Composer of Plots},
  author = {Pedersen, Thomas Lin},
  year = {2022}
}

@book{pedersenWelcomeGgplot2,
  title = {Welcome | Ggplot2},
  author = {Pedersen, Danielle Navarro, {and} Thomas Lin, Hadley Wickham},
  urldate = {2023-03-20},
  abstract = {A book created with bookdown.},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\46SGWZJG\\Pedersen - Welcome  ggplot2.html}
}

@book{pedersenWelcomeGgplot2,
  title = {Welcome | Ggplot2},
  author = {Pedersen, Danielle Navarro, Hadley Wickham, {and} Thomas Lin},
  urldate = {2023-03-20},
  abstract = {A book created with bookdown.},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\QNSIMPK3\\Pedersen - Welcome  ggplot2.html}
}

@article{pojonUsingMachineLearning2017,
  title = {Using {{Machine Learning}} to {{Predict Student Performance}}},
  author = {Pojon, Murat},
  year = {2017},
  urldate = {2023-05-14},
  abstract = {This thesis examines the application of machine learning algorithms to predict whether a student will be successful or not. The specific focus of the thesis is the comparison of machine learning methods and feature engineering techniques in terms of how much they improve the prediction performance.     Three different machine learning methods were used in this thesis. They are linear regression, decision trees, and na\"ive Bayes classification. Feature engineering, the process of modification and selection of the features of a data set, was used to improve predictions made by these learning algorithms.     Two different data sets containing records of student information were used. The machine learning methods were applied to both the raw version and the feature engineered version of the data sets, to predict the student's success.     The thesis comes to the same conclusion as the earlier studies: The results show that it is possible to predict student performance successfully by using machine learning. The best algorithm was na\"ive Bayes classification for the first data set, with 98 percent accuracy, and decision trees for the second data set, with 78 percent accuracy. Feature engineering was found to be more important factor in prediction performance than method selection in the data used in this study.},
  copyright = {This publication is copyrighted. You may download, display and print it for Your own personal use. Commercial use is prohibited.},
  langid = {english},
  annotation = {Accepted: 2017-06-26T10:22:44Z},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\IUMSWQ42\\Pojon - 2017 - Using Machine Learning to Predict Student Performa.pdf}
}

@book{Preface,
  title = {Preface},
  urldate = {2023-02-18},
  abstract = {Preface The Machine Learning Tsunami  In 2006, Geoffrey Hinton et al. published a paper\nolinebreak\nolinebreak 1 showing how to train a deep neural network capable of recognizing handwritten digits with...},
  isbn = {978-1-09-812596-7},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\IMKTMNEF\\Preface.html}
}

@book{Preface,
  title = {Preface},
  urldate = {2023-02-18},
  abstract = {Preface The Machine Learning Tsunami In 2006, Geoffrey Hinton et al. published a paper\nolinebreak\nolinebreak 1 showing how to train a deep neural network capable of recognizing handwritten digits with...},
  isbn = {978-1-09-812596-7},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\FMM8VGJU\\Preface.html}
}

@book{PrefaceSecondEdition,
  title = {Preface to the {{Second Edition}}},
  urldate = {2023-02-18},
  abstract = {Preface to the Second Edition  I am exceptionally proud of the first edition of Data Science from Scratch. It turned out very much the book I wanted it to be. But several years of developments...},
  isbn = {978-1-4920-4112-2},
  langid = {english},
  keywords = {Data Science,Machine Learning,ML},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\F26SCPLF\\preface01.html}
}

@book{PrefaceSecondEdition,
  title = {Preface to the {{Second Edition}}},
  urldate = {2023-02-18},
  abstract = {Preface to the Second Edition I am exceptionally proud of the first edition of Data Science from Scratch. It turned out very much the book I wanted it to be. But several years of developments...},
  isbn = {978-1-4920-4112-2},
  langid = {english},
  keywords = {Data Science,Machine Learning,ML},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\WN6ANZPN\\preface01.html}
}

@misc{Python3TutorialPython,
  title = {Python3 {{Tutorial}}: {{Python Online Course}}},
  howpublished = {https://www.python-course.eu/python3\_course.php}
}

@misc{Python3TutorialPython,
  title = {Python3 {{Tutorial}}: {{Python Online Course}}}
}

@misc{pythonPythonMySQLDatabase,
  title = {Python and {{MySQL Database}}: {{A Practical Introduction}} \textendash{} {{Real Python}}},
  shorttitle = {Python and {{MySQL Database}}},
  author = {Python, Real},
  urldate = {2023-02-11},
  abstract = {In this tutorial, you'll learn how to connect your Python application with a MySQL database. You'll design a movie rating system and perform some common queries on it. You'll also see best practices and tips to prevent SQL injection attacks.},
  howpublished = {https://realpython.com/python-mysql/},
  langid = {english},
  keywords = {Database,MySQL,Python},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\QVJ6PH9K\\python-mysql.html}
}

@misc{pythonPythonMySQLDatabase,
  title = {Python and {{MySQL Database}}: {{A Practical Introduction}} \textendash{} {{Real Python}}},
  shorttitle = {Python and {{MySQL Database}}},
  author = {Python, Real},
  urldate = {2023-02-11},
  abstract = {In this tutorial, you'll learn how to connect your Python application with a MySQL database. You'll design a movie rating system and perform some common queries on it. You'll also see best practices and tips to prevent SQL injection attacks.},
  langid = {english},
  keywords = {Database,MySQL,Python},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\LDXGPUSE\\python-mysql.html}
}

@misc{PythonTutorialPython,
  title = {The {{Python Tutorial}} \textemdash{} {{Python}} 3.6.4 Documentation},
  howpublished = {https://docs.python.org/3/tutorial/index.html}
}

@misc{PythonTutorialPython,
  title = {The {{Python Tutorial}} \textemdash{} {{Python}} 3.6.4 Documentation}
}

@book{rajagopalanPythonDataAnalyst2020,
  title = {A {{Python}} Data Analyst's Toolkit: Learn {{Python}} and {{Python-based}} Libraries with Applications in Data Analysis and Statistics},
  author = {Rajagopalan, Gayathri},
  year = {2020},
  publisher = {{Apress}},
  address = {{New York}},
  isbn = {978-1-4842-6399-0}
}

@book{rajagopalanPythonDataAnalyst2020,
  title = {A {{Python}} Data Analyst's Toolkit: {{Learn Python}} and {{Python-based}} Libraries with Applications in Data Analysis and Statistics},
  author = {Rajagopalan, Gayathri},
  year = {2020},
  publisher = {{Apress}},
  address = {{New York}},
  isbn = {978-1-4842-6399-0}
}

@manual{rcoreteamLanguageEnvironmentStatistical2022,
  type = {Manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2022},
  address = {{Vienna, Austria}},
  institution = {{R Foundation for Statistical Computing}}
}

@manual{rcoreteamLanguageEnvironmentStatistical2022a,
  type = {Manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2022},
  address = {{Vienna, Austria}},
  institution = {{R Foundation for Statistical Computing}}
}

@book{revellePsychProceduresPsychological2022,
  title = {Psych: {{Procedures}} for {{Psychological}}, {{Psychometric}}, and {{Personality Research}}},
  author = {Revelle, William},
  year = {2022}
}

@book{revellePsychProceduresPsychological2022,
  title = {Psych: {{Procedures}} for {{Psychological}}, {{Psychometric}}, and {{Personality Research}}},
  author = {Revelle, William},
  year = {2022}
}

@book{revellePsychProceduresPsychological2022a,
  title = {Psych: {{Procedures}} for {{Psychological}}, {{Psychometric}}, and {{Personality Research}}},
  author = {Revelle, William},
  year = {2022}
}

@book{revellePsychProceduresPsychological2022b,
  title = {Psych: {{Procedures}} for {{Psychological}}, {{Psychometric}}, and {{Personality Research}}},
  author = {Revelle, William},
  year = {2022}
}

@book{riedererMarkdownCookbook,
  title = {R {{Markdown Cookbook}}},
  author = {Riederer, Christophe Dervieux, Yihui Xie, Emily},
  urldate = {2023-02-11},
  abstract = {This book showcases short, practical examples of lesser-known tips and tricks to helps users get the most out of these tools. After reading this book, you will understand how R Markdown documents are transformed from plain text and how you may customize nearly every step of this processing. For example, you will learn how to dynamically create content from R code, reference code in other documents or chunks, control the formatting with customer templates, fine-tune how your code is processed, and incorporate multiple languages into your analysis.},
  keywords = {Markdown,R,RMD},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\F3IDLH9Y\\rmarkdown-cookbook.html}
}

@book{riedererMarkdownCookbooka,
  title = {R {{Markdown Cookbook}}},
  author = {Riederer, Christophe Dervieux, Emily, Yihui Xie},
  urldate = {2023-02-11},
  abstract = {This book showcases short, practical examples of lesser-known tips and tricks to helps users get the most out of these tools. After reading this book, you will understand how R Markdown documents are transformed from plain text and how you may customize nearly every step of this processing. For example, you will learn how to dynamically create content from R code, reference code in other documents or chunks, control the formatting with customer templates, fine-tune how your code is processed, and incorporate multiple languages into your analysis.},
  keywords = {Markdown,R,RMD},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\GR98LP8F\\rmarkdown-cookbook.html}
}

@manual{schloerkeGGallyExtensionGgplot22021,
  type = {Manual},
  title = {{{GGally}}: {{Extension}} to 'Ggplot2'},
  author = {Schloerke, Barret and Cook, Di and Larmarange, Joseph and Briatte, Francois and Marbach, Moritz and Thoen, Edwin and Elberg, Amos and Crowley, Jason},
  year = {2021}
}

@manual{schloerkeGGallyExtensionGgplot22021a,
  type = {Manual},
  title = {{{GGally}}: {{Extension}} to 'Ggplot2'},
  author = {Schloerke, Barret and Cook, Di and Larmarange, Joseph and Briatte, Francois and Marbach, Moritz and Thoen, Edwin and Elberg, Amos and Crowley, Jason},
  year = {2021}
}

@article{scikit-learn,
  title = {Scikit-Learn: {{Machine}} Learning in {{Python}}},
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2825--2830}
}

@misc{ScikitlearnMachineLearning,
  title = {Scikit-Learn: {{Machine}} Learning in {{Python}} \textemdash{} {{Scikit-learn}} 0.19.1 Documentation}
}

@article{scruccaMclustClusteringClassification2016,
  title = {{{mclust}} 5: Clustering, Classification and Density Estimation Using {{Gaussian}} Finite Mixture Models},
  author = {Scrucca, Luca and Fop, Michael and Murphy, T. Brendan and Raftery, Adrian E.},
  year = {2016},
  journal = {The R Journal},
  volume = {8},
  number = {1},
  pages = {289--317}
}

@article{scruccaMclustClusteringClassification2016a,
  title = {{{mclust}} 5: Clustering, Classification and Density Estimation Using {{Gaussian}} Finite Mixture Models},
  author = {Scrucca, Luca and Fop, Michael and Murphy, T. Brendan and Raftery, Adrian E.},
  year = {2016},
  journal = {The R Journal},
  volume = {8},
  number = {1},
  pages = {289--317}
}

@inproceedings{sekerogluStudentPerformancePrediction2019,
  title = {Student {{Performance Prediction}} and {{Classification Using Machine Learning Algorithms}}},
  booktitle = {Proceedings of the 2019 8th {{International Conference}} on {{Educational}} and {{Information Technology}}},
  author = {Sekeroglu, Boran and Dimililer, Kamil and Tuncal, Kubra},
  year = {2019},
  month = mar,
  series = {{{ICEIT}} 2019},
  pages = {7--11},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3318396.3318419},
  urldate = {2023-05-14},
  abstract = {For a productive and a good life, education is a necessity and it improves individuals' life with value and excellence. Also, education is considered a vital need for motivating self-assurance as well as providing the things are needed to partake in today's World. Throughout the years, education faced a number of challenges. Different methods of teaching and learning are suggested to increase the learning quality. In today's world, computers and portable devices are employed in every phase of daily life and many materials are available online anytime, anywhere. Technologies like Artificial Intelligence had a surprising evolution in many fields especially in educational teaching and learning processes. Higher education institutions have started to adopt the use of technology into their traditional teaching mechanisms for enhancing learning and teaching. In this paper, two datasets have been considered for the prediction and classification of student performance respectively using five machine learning algorithms. Eighteen experiments have been performed and preliminary results suggest that performances of students might be predictable and classification of these performances can be increased by applying pre-processing to the raw data before implementing machine learning algorithms.},
  isbn = {978-1-4503-6267-2},
  keywords = {classification,Educational Evaluation,machine learning,prediction}
}

@article{shahidComparisonDistanceMeasures2009,
  title = {Comparison of Distance Measures in Spatial Analytical Modeling for Health Service Planning},
  author = {Shahid, Rizwan and Bertazzon, Stefania and Knudtson, Merril L and Ghali, William A},
  year = {2009},
  month = nov,
  journal = {BMC Health Services Research},
  volume = {9},
  pages = {200},
  issn = {1472-6963},
  doi = {10.1186/1472-6963-9-200},
  urldate = {2023-03-07},
  abstract = {Background Several methodological approaches have been used to estimate distance in health service research. In this study, focusing on cardiac catheterization services, Euclidean, Manhattan, and the less widely known Minkowski distance metrics are used to estimate distances from patient residence to hospital. Distance metrics typically produce less accurate estimates than actual measurements, but each metric provides a single model of travel over a given network. Therefore, distance metrics, unlike actual measurements, can be directly used in spatial analytical modeling. Euclidean distance is most often used, but unlikely the most appropriate metric. Minkowski distance is a more promising method. Distances estimated with each metric are contrasted with road distance and travel time measurements, and an optimized Minkowski distance is implemented in spatial analytical modeling. Methods Road distance and travel time are calculated from the postal code of residence of each patient undergoing cardiac catheterization to the pertinent hospital. The Minkowski metric is optimized, to approximate travel time and road distance, respectively. Distance estimates and distance measurements are then compared using descriptive statistics and visual mapping methods. The optimized Minkowski metric is implemented, via the spatial weight matrix, in a spatial regression model identifying socio-economic factors significantly associated with cardiac catheterization. Results The Minkowski coefficient that best approximates road distance is 1.54; 1.31 best approximates travel time. The latter is also a good predictor of road distance, thus providing the best single model of travel from patient's residence to hospital. The Euclidean metric and the optimal Minkowski metric are alternatively implemented in the regression model, and the results compared. The Minkowski method produces more reliable results than the traditional Euclidean metric. Conclusion Road distance and travel time measurements are the most accurate estimates, but cannot be directly implemented in spatial analytical modeling. Euclidean distance tends to underestimate road distance and travel time; Manhattan distance tends to overestimate both. The optimized Minkowski distance partially overcomes their shortcomings; it provides a single model of travel over the network. The method is flexible, suitable for analytical modeling, and more accurate than the traditional metrics; its use ultimately increases the reliability of spatial analytical models.},
  pmcid = {PMC2781002},
  pmid = {19895692},
  keywords = {Distance Measure,Minkowski,ML},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\2RUHHUQV\\Shahid et al. - 2009 - Comparison of distance measures in spatial analyti.pdf}
}

@article{shahidComparisonDistanceMeasures2009,
  title = {Comparison of Distance Measures in Spatial Analytical Modeling for Health Service Planning},
  author = {Shahid, Rizwan and Bertazzon, Stefania and Knudtson, Merril L and Ghali, William A},
  year = {2009},
  month = nov,
  journal = {BMC Health Services Research},
  volume = {9},
  pages = {200},
  issn = {1472-6963},
  doi = {10.1186/1472-6963-9-200},
  urldate = {2023-03-07},
  abstract = {Background Several methodological approaches have been used to estimate distance in health service research. In this study, focusing on cardiac catheterization services, Euclidean, Manhattan, and the less widely known Minkowski distance metrics are used to estimate distances from patient residence to hospital. Distance metrics typically produce less accurate estimates than actual measurements, but each metric provides a single model of travel over a given network. Therefore, distance metrics, unlike actual measurements, can be directly used in spatial analytical modeling. Euclidean distance is most often used, but unlikely the most appropriate metric. Minkowski distance is a more promising method. Distances estimated with each metric are contrasted with road distance and travel time measurements, and an optimized Minkowski distance is implemented in spatial analytical modeling. Methods Road distance and travel time are calculated from the postal code of residence of each patient undergoing cardiac catheterization to the pertinent hospital. The Minkowski metric is optimized, to approximate travel time and road distance, respectively. Distance estimates and distance measurements are then compared using descriptive statistics and visual mapping methods. The optimized Minkowski metric is implemented, via the spatial weight matrix, in a spatial regression model identifying socio-economic factors significantly associated with cardiac catheterization. Results The Minkowski coefficient that best approximates road distance is 1.54; 1.31 best approximates travel time. The latter is also a good predictor of road distance, thus providing the best single model of travel from patient's residence to hospital. The Euclidean metric and the optimal Minkowski metric are alternatively implemented in the regression model, and the results compared. The Minkowski method produces more reliable results than the traditional Euclidean metric. Conclusion Road distance and travel time measurements are the most accurate estimates, but cannot be directly implemented in spatial analytical modeling. Euclidean distance tends to underestimate road distance and travel time; Manhattan distance tends to overestimate both. The optimized Minkowski distance partially overcomes their shortcomings; it provides a single model of travel over the network. The method is flexible, suitable for analytical modeling, and more accurate than the traditional metrics; its use ultimately increases the reliability of spatial analytical models.},
  pmcid = {PMC2781002},
  pmid = {19895692},
  keywords = {Distance Measure,Minkowski,ML},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\HIWFZC72\\Shahid et al. - 2009 - Comparison of distance measures in spatial analyti.pdf}
}

@book{shawLearnMorePython2018,
  title = {Learn More {{Python}} 3 the Hard Way: The next Step for New {{Python}} Programmers},
  author = {Shaw, Zed},
  year = {2018},
  publisher = {{Addison-Wesley}},
  address = {{Boston}},
  isbn = {978-0-13-412299-1}
}

@book{shawLearnMorePython2018,
  title = {Learn More {{Python}} 3 the Hard Way: {{The}} next Step for New {{Python}} Programmers},
  author = {Shaw, Zed},
  year = {2018},
  publisher = {{Addison-Wesley}},
  address = {{Boston}},
  isbn = {978-0-13-412299-1}
}

@article{shilinhAlgorithmFormingOffer,
  title = {Algorithm for {{Forming}} the {{Offer}} of {{Educational Services}} by {{Higher Education Institutions}} to {{Improve}} the {{Technology}} of {{Processing Educational Content}} by {{Potential Entrants}}},
  author = {Shilinh, Anna and Zhezhnych, Pavlo and Shakhovska, Natalya},
  abstract = {The aim of this article is the algorithm for forming the offer of educational services by higher education institutions. This will improve the technology of processing educational information by potential entrants. The article examines the formation of the offer of educational services by higher education institutions that based on the concept of related connection of specialties of higher education institutions. The possibility of additional informing potential consumers of educational services about the availability of specialties that are relate to the chosen specialty was established. The article formed a differentiation of related groups for the chosen specialty in accordance with the possibility of other specialties to replace it during the entry campaign based on the interests and motivational intentions of potential entrants. The article proposes an algorithm for forming the offer of educational services of a higher education institution for potential entrants on the basis of available certificates of external independent evaluation. It established in the article that the expansion of the offer of educational services provides additional information to entrants about the availability of specialties that have a related connection of group I, II, III to the chosen specialty. The results of the study are used and can be used for effective planning of educational services by higher education institutions and forecasting the contingent of students during the entry campaign.},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\4PMY85XE\\Shilinh et al. - Algorithm for Forming the Offer of Educational Ser.pdf}
}

@article{shilinhDevelopmentProcedureForming,
  title = {Development of a {{Procedure}} for {{Forming Recommendations}} to {{Updating}} the {{University}}'s {{Variable Courses Based}} on {{Their Indicator}} of {{Selection Trends}}},
  author = {Shilinh, Anna and Zhezhnych, Pavlo},
  abstract = {The aim of this article is to development of a procedure for forming recommendations to updating the university's variable courses. This makes it possible to predict students' choice of variable courses and study load for the structural units of the university. The article proposes a procedure for forming recommendations to updating variable courses. It is determined that it consists of an algorithm for determining the indicator of trends in the variable courses choice in accordance with the interest over time of popular search queries on the Internet and an algorithm for generating recommendations to updating variable courses. The article found that the indicator of trends in the choice of a variable course is the sum of the relevant indicators of interest with the time of search queries and/or related queries on the Internet. The paper proposes standard recommendations to updating variable courses.Its based on the value of the indicator of trends in these courses choice. Three groups of standard recommendations to updating variable courses in accordance with the value of the trends indicator in these courses choice. The main typical recommendations are the compliance of the variable course ({$\mathsl{R}\mathsl{e}\mathsl{c}\mathsl{o}\mathsl{m}\mathsl{e}\mathsl{n}\mathsl{d}\mathsl{a}\mathsl{t}\mathsl{i}\mathsl{o}\mathsl{n}\mathsl{I}$}), partial update of the variable course ({$\mathsl{R}\mathsl{e}\mathsl{c}\mathsl{o}\mathsl{m}\mathsl{e}\mathsl{n}\mathsl{d}\mathsl{a}\mathsl{t}\mathsl{i}\mathsl{o}\mathsl{n}\mathsl{I}\mathsl{I}$}), complete update of the variable course ({$\mathsl{R}\mathsl{e}\mathsl{c}\mathsl{o}\mathsl{m}\mathsl{e}\mathsl{n}\mathsl{d}\mathsl{a}\mathsl{t}\mathsl{i}\mathsl{o}\mathsl{n}\mathsl{I}\mathsl{I}\mathsl{I}$}). The article contains an analysis of interest rates over time and a quantitative analysis of the variable courses choice by the Lviv Polytechnic National University students according to the quantitative choice of 2019-2021. The study results are used and can be used for effective planning of educational services by university.},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\FMHQGQ6I\\Shilinh and Zhezhnych - Development of a Procedure for Forming Recommendat.pdf}
}

@misc{shlensTutorialPrincipalComponent2014,
  title = {A {{Tutorial}} on {{Principal Component Analysis}}},
  author = {Shlens, Jonathon},
  year = {2014},
  month = apr,
  number = {arXiv:1404.1100},
  eprint = {1404.1100},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-02-11},
  abstract = {Principal component analysis (PCA) is a mainstay of modern data analysis - a black box that is widely used but (sometimes) poorly understood. The goal of this paper is to dispel the magic behind this black box. This manuscript focuses on building a solid intuition for how and why principal component analysis works. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind PCA. This tutorial does not shy away from explaining the ideas informally, nor does it shy away from the mathematics. The hope is that by addressing both aspects, readers of all levels will be able to gain a better understanding of PCA as well as the when, the how and the why of applying this technique.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,PCA,Statistics - Machine Learning},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\YKZWEK7T\\Shlens - 2014 - A Tutorial on Principal Component Analysis.pdf;C\:\\Users\\zoona\\Zotero\\storage\\Z72K2W68\\1404.html}
}

@misc{shlensTutorialPrincipalComponent2014a,
  title = {A {{Tutorial}} on {{Principal Component Analysis}}},
  author = {Shlens, Jonathon},
  year = {2014},
  month = apr,
  number = {arXiv:1404.1100},
  eprint = {1404.1100},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-02-11},
  abstract = {Principal component analysis (PCA) is a mainstay of modern data analysis - a black box that is widely used but (sometimes) poorly understood. The goal of this paper is to dispel the magic behind this black box. This manuscript focuses on building a solid intuition for how and why principal component analysis works. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind PCA. This tutorial does not shy away from explaining the ideas informally, nor does it shy away from the mathematics. The hope is that by addressing both aspects, readers of all levels will be able to gain a better understanding of PCA as well as the when, the how and the why of applying this technique.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,PCA,Statistics - Machine Learning},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\C2N27AXA\\Shlens - 2014 - A Tutorial on Principal Component Analysis.pdf;C\:\\Users\\zoona\\Zotero\\storage\\FBKCNXYQ\\1404.html}
}

@article{sSupervisedMachineLearning2020,
  title = {Supervised {{Machine Learning Modelling}} \& {{Analysis For Graduate Admission Prediction}}},
  author = {S, Sujay},
  year = {2020},
  volume = {7},
  abstract = {Predictive modelling has found its place in this century for providing an in-depth view and in helping humans in their day to day activity. In this paper, I have analyzed and predicted the possibility of a person getting an admit for graduate courses in the United States based on a supervised machine learning algorithm using Python and its various libraries on a Kaggle dataset. After implementing immense research on the dataset, explored the relationship between each factor which contribute in one or the other way to get an admit. Finally, using linear regression, allowed the program to predict the data from the user.},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\KRBXIAST\\2020 - ¬¬¬Supervised Machine Learning Modelling & Analysi.pdf}
}

@article{sterneMultipleImputationMissing2009,
  title = {Multiple Imputation for Missing Data in Epidemiological and Clinical Research: {{Potential}} and Pitfalls},
  shorttitle = {Multiple Imputation for Missing Data in Epidemiological and Clinical Research},
  author = {Sterne, Jonathan A. C. and White, Ian R. and Carlin, John B. and Spratt, Michael and Royston, Patrick and Kenward, Michael G. and Wood, Angela M. and Carpenter, James R.},
  year = {2009},
  month = jun,
  journal = {BMJ (Clinical research ed.)},
  volume = {338},
  pages = {b2393},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.b2393},
  urldate = {2023-04-18},
  abstract = {{$<$}p{$>$}Most studies have some missing data. \textbf{Jonathan Sterne and colleagues} describe the appropriate use and reporting of the multiple imputation approach to dealing with them {$<$}/p{$>$}},
  chapter = {Research Methods \&amp; Reporting},
  copyright = {\textcopyright{} . This is an open-access article distributed under the terms of the Creative Commons Attribution Non-commercial License, which permits use, distribution, and reproduction in any medium, provided the original work is properly cited, the use is non commercial and is otherwise in compliance with the license. See: http://creativecommons.org/licenses/by-nc/2.0/ and http://creativecommons.org/licenses/by-nc/2.0/legalcode.},
  langid = {english},
  pmid = {19564179},
  keywords = {epidemiological,imputation,Missing data},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\99JWEDL3\\Sterne et al. - 2009 - Multiple imputation for missing data in epidemiolo.pdf;C\:\\Users\\zoona\\Zotero\\storage\\W36ADGFN\\bmj.html}
}

@article{sterneMultipleImputationMissing2009a,
  title = {Multiple Imputation for Missing Data in Epidemiological and Clinical Research: Potential and Pitfalls},
  shorttitle = {Multiple Imputation for Missing Data in Epidemiological and Clinical Research},
  author = {Sterne, Jonathan A. C. and White, Ian R. and Carlin, John B. and Spratt, Michael and Royston, Patrick and Kenward, Michael G. and Wood, Angela M. and Carpenter, James R.},
  year = {2009},
  month = jun,
  journal = {BMJ},
  volume = {338},
  pages = {b2393},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.b2393},
  urldate = {2023-04-18},
  abstract = {{$<$}p{$>$}Most studies have some missing data. \textbf{Jonathan Sterne and colleagues} describe the appropriate use and reporting of the multiple imputation approach to dealing with them {$<$}/p{$>$}},
  chapter = {Research Methods \&amp; Reporting},
  copyright = {\textcopyright{}  . This is an open-access article distributed under the terms of the Creative Commons Attribution Non-commercial License, which permits use, distribution, and reproduction in any medium, provided the original work is properly cited, the use is non commercial and is otherwise in compliance with the license. See: http://creativecommons.org/licenses/by-nc/2.0/  and  http://creativecommons.org/licenses/by-nc/2.0/legalcode.},
  langid = {english},
  pmid = {19564179},
  keywords = {epidemiological,imputation,Missing data},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\UC3AMSUS\\Sterne et al. - 2009 - Multiple imputation for missing data in epidemiolo.pdf;C\:\\Users\\zoona\\Zotero\\storage\\M8N9LCWE\\bmj.html}
}

@book{sweigartAutomateBoringStuff2019,
  title = {Automate {{The Boring Stuff With Python}}, 2nd {{Edition}}},
  author = {Sweigart, Al},
  year = {2019},
  month = nov,
  publisher = {{No Starch Press,US}},
  address = {{San Francisco}},
  isbn = {978-1-59327-992-9}
}

@book{sweigartAutomateBoringStuff2019,
  title = {Automate {{The Boring Stuff With Python}}, 2nd {{Edition}}},
  author = {Sweigart, Al},
  year = {2019},
  month = nov,
  publisher = {{No Starch Press,US}},
  address = {{San Francisco}},
  isbn = {978-1-59327-992-9}
}

@misc{SystemDesignFundamentals,
  title = {System Design Fundamentals: {{What}} Is the {{CAP}} Theorem?},
  shorttitle = {System Design Fundamentals},
  journal = {Educative: Interactive Courses for Software Developers},
  urldate = {2023-02-11},
  abstract = {Today, we'll dive deeper into the CAP theorem, explaining its meaning, its components, and more.},
  howpublished = {https://www.educative.io/blog/what-is-cap-theorem\#whatiscaptheorem},
  langid = {english},
  keywords = {CAP theorem},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\VPM8GJ2W\\what-is-cap-theorem.html}
}

@misc{SystemDesignFundamentals,
  title = {System Design Fundamentals: {{What}} Is the {{CAP}} Theorem?},
  shorttitle = {System Design Fundamentals},
  urldate = {2023-02-11},
  abstract = {Today, we'll dive deeper into the CAP theorem, explaining its meaning, its components, and more.},
  langid = {english},
  keywords = {CAP theorem},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\WM57MDI6\\what-is-cap-theorem.html}
}

@book{tabachnickUsingMultivariateStatistics2014,
  title = {Using Multivariate Statistics},
  author = {Tabachnick, Barbara G. and Fidell, Linda S.},
  year = {2014},
  edition = {Sixth edition},
  publisher = {{Pearson}},
  address = {{Harlow, Essex}},
  isbn = {978-1-292-02131-7}
}

@book{tabachnickUsingMultivariateStatistics2014,
  title = {Using Multivariate Statistics},
  author = {Tabachnick, Barbara G. and Fidell, Linda S.},
  year = {2014},
  edition = {Sixth edition},
  publisher = {{Pearson}},
  address = {{Harlow, Essex}},
  isbn = {978-1-292-02131-7}
}

@misc{teamPandasdevPandasPandas2023,
  title = {Pandas-Dev/Pandas: {{Pandas}}},
  shorttitle = {Pandas-Dev/Pandas},
  author = {Team, The Pandas Development},
  year = {2023},
  month = apr,
  doi = {10.5281/ZENODO.3509134},
  urldate = {2023-05-18},
  abstract = {This is a patch release in the 2.0.x series and includes some regression and bug fixes. We recommend that all users upgrade to this version. See the full whatsnew for a list of all the changes. The release will be available on the defaults and conda-forge channels: &lt;code&gt;conda install pandas &lt;/code&gt; Or via PyPI: &lt;code&gt;python3 -m pip install --upgrade pandas &lt;/code&gt; Please report any issues with the release on the pandas issue tracker. Thanks to all the contributors who made this release possible.},
  copyright = {BSD 3-Clause "New" or "Revised" License, Open Access},
  howpublished = {Zenodo}
}

@manual{therneauRpartRecursivePartitioning2022,
  type = {Manual},
  title = {Rpart: {{Recursive}} Partitioning and Regression Trees},
  author = {Therneau, Terry and Atkinson, Beth},
  year = {2022}
}

@manual{therneauRpartRecursivePartitioning2022a,
  type = {Manual},
  title = {Rpart: {{Recursive}} Partitioning and Regression Trees},
  author = {Therneau, Terry and Atkinson, Beth},
  year = {2022}
}

@book{thioulouseMultivariateAnalysisEcological2018,
  title = {Multivariate Analysis of Ecological Data with {{ade4}}},
  author = {Thioulouse, Jean and Dray, St{\'e}phane and Dufour, Anne{\textendash}B{\'e}atrice and Siberchicot, Aur{\'e}lie and Jombart, Thibaut and Pavoine, Sandrine},
  year = {2018},
  publisher = {{Springer}},
  doi = {10.1007/978-1-4939-8850-1}
}

@article{tierneyExpandingTidyData2023,
  title = {Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations},
  author = {Tierney, Nicholas and Cook, Dianne},
  year = {2023},
  journal = {Journal of Statistical Software},
  volume = {105},
  number = {7},
  pages = {1--31},
  doi = {10.18637/jss.v105.i07}
}

@article{tierneyExpandingTidyData2023a,
  title = {Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations},
  author = {Tierney, Nicholas and Cook, Dianne},
  year = {2023},
  journal = {Journal of Statistical Software},
  volume = {105},
  number = {7},
  pages = {1--31},
  doi = {10.18637/jss.v105.i07}
}

@misc{TryJupyterWebbased,
  title = {Try {{Jupyter}}! {{Web-based Notebook}}},
  howpublished = {https://try.jupyter.org/}
}

@misc{TryJupyterWebbased,
  title = {Try {{Jupyter}}! {{Web-based Notebook}}}
}

@book{vanderplasPythonDataScience2016,
  title = {Python Data Science Handbook: Essential Tools for Working with Data},
  author = {Vanderplas, Jacob T.},
  year = {2016},
  edition = {First edition},
  publisher = {{O'Reilly Media, Inc}},
  address = {{Sebastopol, CA}},
  isbn = {1-4919-1214-6}
}

@book{vanderplasPythonDataScience2016,
  title = {Python Data Science Handbook: {{Essential}} Tools for Working with Data},
  author = {Vanderplas, Jacob T.},
  year = {2016},
  edition = {First edition},
  publisher = {{O'Reilly Media, Inc}},
  address = {{Sebastopol, CA}},
  isbn = {1-4919-1214-6}
}

@book{venablesModernAppliedStatistics2002,
  title = {Modern Applied Statistics with {{S}}},
  author = {Venables, W. N. and Ripley, B. D.},
  year = {2002},
  edition = {Fourth},
  publisher = {{Springer}},
  address = {{New York}}
}

@book{venablesModernAppliedStatistics2002a,
  title = {Modern Applied Statistics with {{S}}},
  author = {Venables, W. N. and Ripley, B. D.},
  year = {2002},
  edition = {Fourth},
  publisher = {{Springer}},
  address = {{New York}}
}

@book{weiCorrplotVisualizationCorrelation2021,
  title = {Corrplot: {{Visualization}} of a {{Correlation Matrix}}},
  author = {Wei, Taiyun and Simko, Viliam},
  year = {2021}
}

@book{weiCorrplotVisualizationCorrelation2021,
  title = {Corrplot: {{Visualization}} of a {{Correlation Matrix}}},
  author = {Wei, Taiyun and Simko, Viliam},
  year = {2021}
}

@book{weiCorrplotVisualizationCorrelation2021a,
  title = {Corrplot: {{Visualization}} of a {{Correlation Matrix}}},
  author = {Wei, Taiyun and Simko, Viliam},
  year = {2021}
}

@book{weiCorrplotVisualizationCorrelation2021b,
  title = {Corrplot: {{Visualization}} of a {{Correlation Matrix}}},
  author = {Wei, Taiyun and Simko, Viliam},
  year = {2021}
}

@book{weiPackageCorrplotVisualization2021,
  title = {R Package 'Corrplot': {{Visualization}} of a {{Correlation Matrix}}},
  author = {Wei, Taiyun and Simko, Viliam},
  year = {2021}
}

@book{weiPackageCorrplotVisualization2021,
  title = {R Package 'Corrplot': {{Visualization}} of a {{Correlation Matrix}}},
  author = {Wei, Taiyun and Simko, Viliam},
  year = {2021}
}

@book{weiPackageCorrplotVisualization2021a,
  title = {R Package 'Corrplot': {{Visualization}} of a {{Correlation Matrix}}},
  author = {Wei, Taiyun and Simko, Viliam},
  year = {2021}
}

@book{weiPackageCorrplotVisualization2021b,
  title = {R Package 'Corrplot': {{Visualization}} of a {{Correlation Matrix}}},
  author = {Wei, Taiyun and Simko, Viliam},
  year = {2021}
}

@misc{WhatAnacondaAnaconda,
  title = {What Is {{Anaconda}}? | {{Anaconda}}},
  howpublished = {https://www.anaconda.com/what-is-anaconda/}
}

@misc{WhatAnacondaAnaconda,
  title = {What Is {{Anaconda}}? | {{Anaconda}}}
}

@manual{wickhamDplyrGrammarData2023,
  type = {Manual},
  title = {Dplyr: {{A}} Grammar of Data Manipulation},
  author = {Wickham, Hadley and Fran{\c c}ois, Romain and Henry, Lionel and M{\"u}ller, Kirill and Vaughan, Davis},
  year = {2023}
}

@manual{wickhamDplyrGrammarData2023a,
  type = {Manual},
  title = {Dplyr: {{A}} Grammar of Data Manipulation},
  author = {Wickham, Hadley and Fran{\c c}ois, Romain and Henry, Lionel and M{\"u}ller, Kirill and Vaughan, Davis},
  year = {2023}
}

@manual{wickhamForcatsToolsWorking2023,
  type = {Manual},
  title = {Forcats: {{Tools}} for Working with Categorical Variables (Factors)},
  author = {Wickham, Hadley},
  year = {2023}
}

@manual{wickhamForcatsToolsWorking2023a,
  type = {Manual},
  title = {Forcats: {{Tools}} for Working with Categorical Variables (Factors)},
  author = {Wickham, Hadley},
  year = {2023}
}

@book{wickhamGgplot2CreateElegant2022,
  title = {Ggplot2: {{Create Elegant Data Visualisations Using}} the {{Grammar}} of {{Graphics}}},
  author = {Wickham, Hadley and Chang, Winston and Henry, Lionel and Pedersen, Thomas Lin and Takahashi, Kohske and Wilke, Claus and Woo, Kara and Yutani, Hiroaki and Dunnington, Dewey},
  year = {2022}
}

@book{wickhamGgplot2CreateElegant2022,
  title = {Ggplot2: {{Create Elegant Data Visualisations Using}} the {{Grammar}} of {{Graphics}}},
  author = {Wickham, Hadley and Chang, Winston and Henry, Lionel and Pedersen, Thomas Lin and Takahashi, Kohske and Wilke, Claus and Woo, Kara and Yutani, Hiroaki and Dunnington, Dewey},
  year = {2022}
}

@book{wickhamGgplot2CreateElegant2022a,
  title = {Ggplot2: {{Create Elegant Data Visualisations Using}} the {{Grammar}} of {{Graphics}}},
  author = {Wickham, Hadley and Chang, Winston and Henry, Lionel and Pedersen, Thomas Lin and Takahashi, Kohske and Wilke, Claus and Woo, Kara and Yutani, Hiroaki and Dunnington, Dewey},
  year = {2022}
}

@book{wickhamGgplot2CreateElegant2022b,
  title = {Ggplot2: {{Create Elegant Data Visualisations Using}} the {{Grammar}} of {{Graphics}}},
  author = {Wickham, Hadley and Chang, Winston and Henry, Lionel and Pedersen, Thomas Lin and Takahashi, Kohske and Wilke, Claus and Woo, Kara and Yutani, Hiroaki and Dunnington, Dewey},
  year = {2022}
}

@book{wickhamGgplot2ElegantGraphics,
  title = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}}},
  author = {Wickham, Hadley and Navarro, Danielle and Pedersen, Thomas Lin},
  edition = {work-in-progress 3rd edition}
}

@book{wickhamGgplot2ElegantGraphics2016,
  title = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}}},
  author = {Wickham, Hadley},
  year = {2016},
  publisher = {{Springer-Verlag New York}},
  isbn = {978-3-319-24277-4}
}

@book{wickhamGgplot2ElegantGraphics2016,
  title = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}}},
  author = {Wickham, Hadley},
  year = {2016},
  publisher = {{Springer-Verlag New York}},
  isbn = {978-3-319-24277-4}
}

@book{wickhamGgplot2ElegantGraphics2016a,
  title = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}}},
  author = {Wickham, Hadley},
  year = {2016},
  publisher = {{Springer-Verlag New York}},
  isbn = {978-3-319-24277-4}
}

@book{wickhamGgplot2ElegantGraphics2016b,
  title = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}}},
  author = {Wickham, Hadley},
  year = {2016},
  publisher = {{Springer-Verlag New York}},
  isbn = {978-3-319-24277-4}
}

@book{wickhamGgplot2ElegantGraphics2016c,
  title = {Ggplot2: {{Elegant}} Graphics for Data Analysis},
  author = {Wickham, Hadley},
  year = {2016},
  publisher = {{Springer-Verlag New York}},
  isbn = {978-3-319-24277-4}
}

@book{wickhamGgplot2ElegantGraphics2016d,
  title = {Ggplot2: {{Elegant}} Graphics for Data Analysis},
  author = {Wickham, Hadley},
  year = {2016},
  publisher = {{Springer-Verlag New York}},
  isbn = {978-3-319-24277-4}
}

@book{wickhamGgplot2ElegantGraphicsa,
  title = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}}},
  author = {Wickham, Hadley and Navarro, Danielle and Pedersen, Thomas Lin},
  edition = {work-in-progress 3rd edition}
}

@manual{wickhamTidyrTidyMessy2023,
  type = {Manual},
  title = {Tidyr: {{Tidy}} Messy Data},
  author = {Wickham, Hadley and Vaughan, Davis and Girlich, Maximilian},
  year = {2023}
}

@manual{wickhamTidyrTidyMessy2023a,
  type = {Manual},
  title = {Tidyr: {{Tidy}} Messy Data},
  author = {Wickham, Hadley and Vaughan, Davis and Girlich, Maximilian},
  year = {2023}
}

@manual{wilkeCowplotStreamlinedPlot2020,
  type = {Manual},
  title = {Cowplot: {{Streamlined}} Plot Theme and Plot Annotations for 'Ggplot2'},
  author = {Wilke, Claus O.},
  year = {2020}
}

@manual{wilkeCowplotStreamlinedPlot2020a,
  type = {Manual},
  title = {Cowplot: {{Streamlined}} Plot Theme and Plot Annotations for 'Ggplot2'},
  author = {Wilke, Claus O.},
  year = {2020}
}

@book{wilkeFundamentalsDataVisualization,
  title = {Fundamentals of {{Data Visualization}}},
  author = {Wilke, Claus O.},
  urldate = {2023-02-11},
  abstract = {A guide to making visualizations that accurately reflect the data, tell a story, and look professional.},
  keywords = {Data Visualisation,R},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\QE8ASRIR\\dataviz.html}
}

@book{wilkeFundamentalsDataVisualizationa,
  title = {Fundamentals of {{Data Visualization}}},
  author = {Wilke, Claus O.},
  urldate = {2023-02-11},
  abstract = {A guide to making visualizations that accurately reflect the data, tell a story, and look professional.},
  keywords = {Data Visualisation,R},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\TVARL3EF\\dataviz.html}
}

@misc{wilkeSDS375,
  title = {{{SDS}} 375},
  author = {Wilke, Claus O.},
  journal = {SDS 375},
  urldate = {2023-02-23},
  abstract = {Data Visualization in R},
  howpublished = {https://wilkelab.org/SDS375/},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\PJ466LYX\\SDS375.html}
}

@manual{williamrevellePsychProceduresPsychological2023,
  type = {Manual},
  title = {Psych: {{Procedures}} for Psychological, Psychometric, and Personality Research},
  author = {{William Revelle}},
  year = {2023},
  address = {{Evanston, Illinois}},
  institution = {{Northwestern University}}
}

@manual{williamrevellePsychProceduresPsychological2023a,
  type = {Manual},
  title = {Psych: {{Procedures}} for Psychological, Psychometric, and Personality Research},
  author = {{William Revelle}},
  year = {2023},
  address = {{Evanston, Illinois}},
  institution = {{Northwestern University}}
}

@book{xieDynamicDocumentsKnitr2015,
  title = {Dynamic Documents with {{R}} and Knitr},
  author = {Xie, Yihui},
  year = {2015},
  edition = {Second},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}}
}

@incollection{xieKnitrComprehensiveTool2014,
  title = {Knitr: {{A}} Comprehensive Tool for Reproducible Research in {{R}}},
  booktitle = {Implementing Reproducible Computational Research},
  author = {Xie, Yihui},
  editor = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D.},
  year = {2014},
  publisher = {{Chapman and Hall/CRC}}
}

@manual{xieKnitrGeneralpurposePackage2023,
  type = {Manual},
  title = {Knitr: {{A}} General-Purpose Package for Dynamic Report Generation in {{R}}},
  author = {Xie, Yihui},
  year = {2023}
}

@misc{yadavMiningEducationData2012,
  title = {Mining {{Education Data}} to {{Predict Student}}'s {{Retention}}: {{A}} Comparative {{Study}}},
  shorttitle = {Mining {{Education Data}} to {{Predict Student}}'s {{Retention}}},
  author = {Yadav, Surjeet Kumar and Bharadwaj, Brijesh and Pal, Saurabh},
  year = {2012},
  month = mar,
  number = {arXiv:1203.2987},
  eprint = {1203.2987},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1203.2987},
  urldate = {2023-05-14},
  abstract = {The main objective of higher education is to provide quality education to students. One way to achieve highest level of quality in higher education system is by discovering knowledge for prediction regarding enrolment of students in a course. This paper presents a data mining project to generate predictive models for student retention management. Given new records of incoming students, these predictive models can produce short accurate prediction lists identifying students who tend to need the support from the student retention program most. This paper examines the quality of the predictive models generated by the machine learning algorithms. The results show that some of the machines learning algorithms are able to establish effective predictive models from the existing student retention data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Databases,Computer Science - Machine Learning},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\N3VBMTVJ\\Yadav et al. - 2012 - Mining Education Data to Predict Student's Retenti.pdf;C\:\\Users\\zoona\\Zotero\\storage\\HWBNADTL\\1203.html}
}

@article{yagciEducationalDataMining2022,
  title = {Educational Data Mining: Prediction of Students' Academic Performance Using Machine Learning Algorithms},
  shorttitle = {Educational Data Mining},
  author = {Ya{\u g}c{\i}, Mustafa},
  year = {2022},
  month = mar,
  journal = {Smart Learning Environments},
  volume = {9},
  number = {1},
  pages = {11},
  issn = {2196-7091},
  doi = {10.1186/s40561-022-00192-z},
  urldate = {2023-05-14},
  abstract = {Educational data mining has become an effective tool for exploring the hidden relationships in educational data and predicting students' academic achievements. This study proposes a new model based on machine learning algorithms to predict the final exam grades of undergraduate students, taking their midterm exam grades as the source data. The performances of the random forests, nearest neighbour, support vector machines, logistic regression, Na\"ive Bayes, and k-nearest neighbour algorithms, which are among the machine learning algorithms, were calculated and compared to predict the final exam grades of the students. The dataset consisted of the academic achievement grades of 1854 students who took the Turkish Language-I course in a state University in Turkey during the fall semester of 2019\textendash 2020. The results show that the proposed model achieved a classification accuracy of 70\textendash 75\%. The predictions were made using only three types of parameters; midterm exam grades, Department data and Faculty data. Such data-driven studies are very important in terms of establishing a learning analysis framework in higher education and contributing to the decision-making processes. Finally, this study presents a contribution to the early prediction of students at high risk of failure and determines the most effective machine learning methods.},
  keywords = {Early warning systems,Educational data mining,Learning analytics,Machine learning,Predicting achievement},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\7KAL95WZ\\Yağcı - 2022 - Educational data mining prediction of students' a.pdf;C\:\\Users\\zoona\\Zotero\\storage\\5PGFDGZB\\s40561-022-00192-z.html}
}

@article{yongBeginnerGuideFactor2013,
  title = {A {{Beginner}}'s {{Guide}} to {{Factor Analysis}}: {{Focusing}} on {{Exploratory Factor Analysis}}},
  shorttitle = {A {{Beginner}}'s {{Guide}} to {{Factor Analysis}}},
  author = {Yong, An Gie and Pearce, Sean},
  year = {2013},
  month = oct,
  journal = {Tutorials in Quantitative Methods for Psychology},
  volume = {9},
  number = {2},
  pages = {79--94},
  issn = {1913-4126},
  doi = {10.20982/tqmp.09.2.p079},
  urldate = {2023-02-11},
  langid = {english},
  keywords = {EFA,Factor Analysis},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\UJDJIEPK\\Yong and Pearce - 2013 - A Beginner’s Guide to Factor Analysis Focusing on.pdf}
}

@article{yongBeginnerGuideFactor2013,
  title = {A {{Beginner}}'s {{Guide}} to {{Factor Analysis}}: {{Focusing}} on {{Exploratory Factor Analysis}}},
  shorttitle = {A {{Beginner}}'s {{Guide}} to {{Factor Analysis}}},
  author = {Yong, An Gie and Pearce, Sean},
  year = {2013},
  month = oct,
  journal = {Tutorials in Quantitative Methods for Psychology},
  volume = {9},
  number = {2},
  pages = {79--94},
  issn = {1913-4126},
  doi = {10.20982/tqmp.09.2.p079},
  urldate = {2023-02-11},
  langid = {english},
  keywords = {EFA,Factor Analysis},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\Q53T35EA\\Yong and Pearce - 2013 - A Beginner’s Guide to Factor Analysis Focusing on.pdf}
}

@manual{zhuKableExtraConstructComplex2021,
  type = {Manual},
  title = {{{kableExtra}}: {{Construct}} Complex Table with 'kable' and Pipe Syntax},
  author = {Zhu, Hao},
  year = {2021}
}

@manual{zhuKableExtraConstructComplex2021a,
  type = {Manual},
  title = {{{kableExtra}}: {{Construct}} Complex Table with 'kable' and Pipe Syntax},
  author = {Zhu, Hao},
  year = {2021}
}

@misc{zotero-116,
  urldate = {2023-02-17},
  howpublished = {https://probml.github.io/pml-book/book1.html},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\TQCFQEC8\\book1.html}
}

@misc{zotero-116,
  urldate = {2023-02-17},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\74XYFP2I\\book1.html}
}

@misc{zotero-243,
  type = {Misc}
}

@misc{zotero-243,
  type = {Misc}
}

@incollection{zotero-498,
  type = {Incollection}
}

@article{zubPerformanceEvaluationMLbased,
  title = {Performance {{Evaluation}} of {{ML-based Classifiers}} for {{HEI Graduate Entrants}}},
  author = {Zub, Khrystyna and Zhezhnych, Pavlo},
  abstract = {The development of intelligent decision support systems for admission to higher education institutions (HEI) is an essential task for both the institution, particularly for the selection of the best entrants, and for the entrant - to assess their chances of admission to the chosen HEI. The efficiency of such systems is largely based on the accuracy of the intelligent components underlying the system. This article investigates the effectiveness of machine learning (ML) based classifiers in solving the task of predicting the entry of entrants in the HEI. The simulation was performed using Orange software and a real data set. The task relates to binary classification in the case of an unbalanced data set. The simulation was performed by selecting the optimal operating parameters of each studied classifier and running it 100 times on a randomly generated data sample. This approach ensured the reliability of the results. Comparison of the accuracy of different classifiers was performed based on total accuracy, Fmeasure, Precision, and Recall measures. It has been experimentally established that Support Vector Machine (SVM) based classifiers demonstrated the highest accuracy in all four performance indicators among the considered methods. Receiver operating characteristic (ROC) curves in both classes also confirmed the highest accuracy of its work. This makes it possible to apply it in practice.},
  langid = {english},
  file = {C\:\\Users\\zoona\\Zotero\\storage\\Z5RR6WUJ\\Zub and Zhezhnych - Performance Evaluation of ML-based Classifiers for.pdf}
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This note looks at fitting various models to the datasets.  \n",
    "\n",
    "I am going fit the following classifiers on each of the dataseets (with PCA and without PCA):\n",
    "\n",
    "* Logistic Regression\n",
    "* Random Forest\n",
    "* Support Vector Machine (classifier)\n",
    "* K-Nearest Neighbours\n",
    "* Naive Bayes\n",
    "* Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_train_transformed = pd.read_csv('../../data/X_train_transformed.csv')\n",
    "X_test_transformed = pd.read_csv('../../data/X_test_transformed.csv')\n",
    "X_val_transformed = pd.read_csv('../../data/X_val_transformed.csv')\n",
    "X_train_pca = pd.read_csv('../../data/X_train_pca.csv')\n",
    "X_test_pca = pd.read_csv('../../data/X_test_pca.csv')\n",
    "X_val_pca = pd.read_csv('../../data/X_val_pca.csv')\n",
    "\n",
    "\n",
    "y_train = pd.read_csv('../../data/y_train.csv')\n",
    "y_test = pd.read_csv('../../data/y_test.csv')\n",
    "y_val = pd.read_csv('../../data/y_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18861\n",
      "6288\n",
      "18861\n",
      "6288\n",
      "18861\n",
      "6288\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_transformed))\n",
    "print(len(X_test_transformed))\n",
    "print(len(X_train_pca))\n",
    "print(len(X_test_pca))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having reconsidered the situation, I am now going to split the training datasets to get a validation set and leave the test sets for final model evaluation. I would probably want to split into three datasets from the start next time - I went back and updated the previous code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [23577, 15088]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zoona\\OneDrive - UWE Bristol\\Modules\\ML_CW\\Machine_Learning_Analytics\\code\\X0_commentary\\04_3_model_fit.ipynb Cell 7\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zoona/OneDrive%20-%20UWE%20Bristol/Modules/ML_CW/Machine_Learning_Analytics/code/X0_commentary/04_3_model_fit.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m X_train_transformed, X_val_transformed, y_train, y_val \u001b[39m=\u001b[39m train_test_split(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zoona/OneDrive%20-%20UWE%20Bristol/Modules/ML_CW/Machine_Learning_Analytics/code/X0_commentary/04_3_model_fit.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     X_train_transformed, y_train, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m567\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zoona/OneDrive%20-%20UWE%20Bristol/Modules/ML_CW/Machine_Learning_Analytics/code/X0_commentary/04_3_model_fit.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Split the PCA-transformed data into training and validation sets\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/zoona/OneDrive%20-%20UWE%20Bristol/Modules/ML_CW/Machine_Learning_Analytics/code/X0_commentary/04_3_model_fit.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m X_train_pca, X_val_pca, y_train_pca, y_val_pca \u001b[39m=\u001b[39m train_test_split(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zoona/OneDrive%20-%20UWE%20Bristol/Modules/ML_CW/Machine_Learning_Analytics/code/X0_commentary/04_3_model_fit.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     X_train_pca, y_train, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m567\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\zoona\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2559\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2556\u001b[0m \u001b[39mif\u001b[39;00m n_arrays \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2557\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAt least one array required as input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2559\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39;49marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m   2562\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m\n\u001b[0;32m   2564\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\zoona\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:443\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \n\u001b[0;32m    426\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    442\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[1;32m--> 443\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[0;32m    444\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\zoona\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [23577, 15088]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# models to iterate over\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC(),\n",
    "    KNeighborsClassifier(),\n",
    "    GaussianNB()\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Split the transformed data into training and validation sets\n",
    "X_train_transformed, X_val_transformed, y_train, y_val = train_test_split(\n",
    "    X_train_transformed, y_train, test_size=0.2, random_state=567)\n",
    "\n",
    "# Split the PCA-transformed data into training and validation sets\n",
    "X_train_pca, X_val_pca, y_train_pca, y_val_pca = train_test_split(\n",
    "    X_train_pca, y_train, test_size=0.2, random_state=567)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# iterate over the datasets\n",
    "for dataset_name, X_train, X_val in [('X_train_transformed', X_train_transformed, X_val_transformed),\n",
    "                                    ('X_train_pca', X_train_pca, X_val_pca)]:\n",
    "    print(f\"Results for {dataset_name}:\")\n",
    "    \n",
    "    # iterate over the models\n",
    "    for model in models:\n",
    "        model_name = type(model).__name__\n",
    "        print(f\"Model: {model_name}\")\n",
    "        \n",
    "        # Fit the model on the training data\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions on the validation set\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Calculate evaluation metrics on the validation set\n",
    "        accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "        precision_val = precision_score(y_val, y_pred_val, average='weighted')\n",
    "        recall_val = recall_score(y_val, y_pred_val, average='weighted')\n",
    "        f1_val = f1_score(y_val, y_pred_val, average='weighted')\n",
    "        \n",
    "        print(f\"Validation Accuracy: {accuracy_val}\")\n",
    "        print(f\"Validation Precision: {precision_val}\")\n",
    "        print(f\"Validation Recall: {recall_val}\")\n",
    "        print(f\"Validation F1 Score: {f1_val}\")\n",
    "        \n",
    "        # Perform any necessary preprocessing or transformations on X_test_transformed\n",
    "        \n",
    "        # Make predictions on the test set\n",
    "        y_pred_test = model.predict(X_test_transformed)\n",
    "        \n",
    "        # Calculate evaluation metrics on the test set\n",
    "        accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "        precision_test = precision_score(y_test, y_pred_test, average='weighted')\n",
    "        recall_test = recall_score(y_test, y_pred_test, average='weighted')\n",
    "        f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
    "        \n",
    "        print(f\"Test Accuracy: {accuracy_test}\")\n",
    "        print(f\"Test Precision: {precision_test}\")\n",
    "        print(f\"Test Recall: {recall_test}\")\n",
    "        print(f\"Test F1 Score: {f1_test}\")\n",
    "        \n",
    "        # Calculate confusion matrix for the test set\n",
    "        \n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# list of models to iterate over\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC(),\n",
    "    KNeighborsClassifier(),\n",
    "    GaussianNB()\n",
    "]\n",
    "\n",
    "# datasets names\n",
    "datasets = [\n",
    "    ('X_train_transformed', X_train_transformed),\n",
    "    ('X_train_pca', X_train_pca)\n",
    "]\n",
    "\n",
    "# iterate over the datasets\n",
    "for dataset_name, X_train in datasets:\n",
    "    print(f\"Results for {dataset_name}:\")\n",
    "    \n",
    "    # iterate over the models\n",
    "    for model in models:\n",
    "        model_name = type(model).__name__\n",
    "        print(f\"Model: {model_name}\")\n",
    "        \n",
    "        # fit model on training data\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "             \n",
    "        # predictions on the test data\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # evaluate the model's performance\n",
    "        # You can use any evaluation metric you prefer (accuracy, recall, precision, F1 score)\n",
    "        accuracy = model.score(X_test, y_test)\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        \n",
    "        # Calculate other evaluation metrics if needed\n",
    "        # e.g., recall, precision, F1 score using scikit-learn's classification metrics module\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        # You can use scikit-learn's confusion_matrix function to calculate the confusion matrix\n",
    "        \n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

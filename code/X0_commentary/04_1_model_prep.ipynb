{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modelling  \n",
    "\n",
    "This section is preparing for data modelling and stepping through the process of building a model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "\n",
    "I have had several thoughts about how the data could and should be split:\n",
    "\n",
    "* train/validate/test split - where the model is trained on `train`, validated and tuned on `validate` and tested/confirmed on `test`\n",
    "* train/test split - where the model is trained, validated and tuned on `train` using cross validation, before final model is tested/confirmed on `test`\n",
    "* train/test split on `year` in the dataset - where the model is trained on data from 2013 and tested on 2014 data.  This would mimic the real world scenario, where a HE institution would use data from previous years to build a model which is applied to the current year students.  The model would be retrained on an annual basis with new data.  \n",
    "  * However, the dataset had some `module_presentations` which would not feature in the training data\n",
    "  * The distribution of the data between years is quite different - I focused on `subject` as a proxy or student type and behaviour as well as curriculum differences between modules - and it varied between years.  \n",
    "* reworking the final_result target variables into two categories - `intervene` and `no_intervene` - and using a binary classification model to predict whether a student would need intervention or not.  This could be a more useful model for the HE institution, as it would be able to identify students who need intervention and target resources at them.  However, this would require a different approach to the modelling, as the target variable would be binary rather than multi-class.\n",
    "\n",
    "So, I decided to split the whole dataset into `train` and `test` sets, using the `train_test_split` function from `sklearn.model_selection`.  I used a `test_size` of 0.25, which is 25% of the data.  I also set the `random_state` to 567, so that the split would be reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed data from csv file\n",
    "data = pd.read_csv('../../data/final_model_150_20230529.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23122 entries, 0 to 23121\n",
      "Data columns (total 26 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   code_module              23122 non-null  object \n",
      " 1   code_presentation        23122 non-null  object \n",
      " 2   id_student               23122 non-null  int64  \n",
      " 3   gender                   23122 non-null  object \n",
      " 4   region                   23122 non-null  object \n",
      " 5   highest_education        23122 non-null  object \n",
      " 6   imd_band                 23122 non-null  object \n",
      " 7   age_band                 23122 non-null  object \n",
      " 8   num_of_prev_attempts     23122 non-null  int64  \n",
      " 9   studied_credits          23122 non-null  int64  \n",
      " 10  disability               23122 non-null  object \n",
      " 11  course_length            23122 non-null  int64  \n",
      " 12  date_registration        23122 non-null  float64\n",
      " 13  date_unregistration      23122 non-null  float64\n",
      " 14  prop_submissions         23122 non-null  float64\n",
      " 15  avg_score                23122 non-null  float64\n",
      " 16  submission_distance      23122 non-null  float64\n",
      " 17  stu_activity_count       23122 non-null  float64\n",
      " 18  stu_activity_type_count  23122 non-null  float64\n",
      " 19  stu_total_clicks         23122 non-null  float64\n",
      " 20  stu_days_active          23122 non-null  float64\n",
      " 21  mod_pres_vle_type_count  23122 non-null  float64\n",
      " 22  year                     23122 non-null  int64  \n",
      " 23  month                    23122 non-null  object \n",
      " 24  subject                  23122 non-null  object \n",
      " 25  final_result             23122 non-null  object \n",
      "dtypes: float64(10), int64(5), object(11)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns as discussed in previous notebooks.  \n",
    "\n",
    "I still think it would be interesting to build different models with some of these dropped features, although they are different research questions which need to be properly considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = data.copy()\n",
    "# columns to drop\n",
    "columns_to_drop = ['code_module','code_presentation', 'id_student', 'gender', 'region', 'highest_education', 'imd_band',\n",
    "                   'age_band', 'disability', 'course_length', 'mod_pres_vle_type_count', 'year', 'month','date_registration', ]\n",
    "\n",
    "# drop columns\n",
    "model = model.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save into standard `X` and `y` variables.\n",
    "\n",
    "Use stratification to ensure that the proportions of `final_result` are the same in both the `train` and `test` sets.  This is important as the `final_result` is the target variable and we want to ensure that the model is trained on a representative sample of the data.  I used the `stratify` parameter in the `train_test_split` function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop target from X, save target to y\n",
    "#X = model.drop('final_result', axis=1)  \n",
    "#y = model['final_result']  \n",
    "\n",
    "# split data into train and test sets with stratification\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=567)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# drop target from X, save target to y\n",
    "X = model.drop('final_result', axis=1)  \n",
    "y = model['final_result']  \n",
    "\n",
    "# split data into train-test and validation sets with stratification\n",
    "X_train_test, X_val, y_train_test, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=567)\n",
    "\n",
    "# split train-test set into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_test, y_train_test, test_size=0.25, stratify=y_train_test, random_state=567)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I have checked to see that the proportions are the same in both the `train` and `test` sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportions of target variable in original data\n",
    "#original_proportions = model['final_result'].value_counts(normalize=True)\n",
    "\n",
    "# proportions of target variable in train and test sets\n",
    "#train_proportions = y_train.value_counts(normalize=True)\n",
    "#test_proportions = y_test.value_counts(normalize=True)\n",
    "\n",
    "# results\n",
    "#print(\"Original Proportions:\")\n",
    "#print(original_proportions)\n",
    "\n",
    "#print(\"\\nTrain Set Proportions:\")\n",
    "#print(train_proportions)\n",
    "\n",
    "#print(\"\\nTest Set Proportions:\")\n",
    "#print(test_proportions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Proportions:\n",
      "Pass           0.511591\n",
      "Fail           0.298158\n",
      "Distinction    0.122178\n",
      "Withdrawn      0.068074\n",
      "Name: final_result, dtype: float64\n",
      "\n",
      "Train Set Proportions:\n",
      "Pass           0.511606\n",
      "Fail           0.298155\n",
      "Distinction    0.122189\n",
      "Withdrawn      0.068051\n",
      "Name: final_result, dtype: float64\n",
      "\n",
      "Validation Set Proportions:\n",
      "Pass           0.511568\n",
      "Fail           0.298162\n",
      "Distinction    0.122162\n",
      "Withdrawn      0.068108\n",
      "Name: final_result, dtype: float64\n",
      "\n",
      "Test Set Proportions:\n",
      "Pass           0.511568\n",
      "Fail           0.298162\n",
      "Distinction    0.122162\n",
      "Withdrawn      0.068108\n",
      "Name: final_result, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# proportions of target variable in original data\n",
    "original_proportions = y.value_counts(normalize=True)\n",
    "\n",
    "# proportions of target variable in train, validation, and test sets\n",
    "train_proportions = y_train.value_counts(normalize=True)\n",
    "val_proportions = y_val.value_counts(normalize=True)\n",
    "test_proportions = y_test.value_counts(normalize=True)\n",
    "\n",
    "# results\n",
    "print(\"Original Proportions:\")\n",
    "print(original_proportions)\n",
    "\n",
    "print(\"\\nTrain Set Proportions:\")\n",
    "print(train_proportions)\n",
    "\n",
    "print(\"\\nValidation Set Proportions:\")\n",
    "print(val_proportions)\n",
    "\n",
    "print(\"\\nTest Set Proportions:\")\n",
    "print(test_proportions)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another standard check is to ensure that there are no missing values in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in X_train: 0\n",
      "Missing values in X_test: 0\n",
      "Missing values in y_train: 0\n",
      "Missing values in y_test: 0\n",
      "Missing values in X_val: 0\n",
      "Missing values in y_val: 0\n"
     ]
    }
   ],
   "source": [
    "# missing values in X_train, X_test, y_train, y_test\n",
    "missing_values_X_train = X_train.isnull().sum()\n",
    "missing_values_X_test = X_test.isnull().sum()\n",
    "missing_values_y_train = y_train.isnull().sum()\n",
    "missing_values_y_test = y_test.isnull().sum()\n",
    "missing_values_X_val = X_val.isnull().sum()\n",
    "missing_values_y_val = y_val.isnull().sum()\n",
    "\n",
    "\n",
    "# rows with missing values\n",
    "rows_with_missing_X_train = X_train[X_train.isnull().any(axis=1)]\n",
    "rows_with_missing_X_test = X_test[X_test.isnull().any(axis=1)]\n",
    "rows_with_missing_y_train = y_train[y_train.isnull()]\n",
    "rows_with_missing_y_test = y_test[y_test.isnull()]\n",
    "rows_with_missing_X_val = X_val[X_val.isnull().any(axis=1)]\n",
    "rows_with_missing_y_val = y_val[y_val.isnull()]\n",
    "\n",
    "\n",
    "\n",
    "# results\n",
    "print(\"Missing values in X_train:\", len(rows_with_missing_X_train))\n",
    "print(\"Missing values in X_test:\", len(rows_with_missing_X_test))\n",
    "print(\"Missing values in y_train:\", len(rows_with_missing_y_train))\n",
    "print(\"Missing values in y_test:\", len(rows_with_missing_y_test))\n",
    "print(\"Missing values in X_val:\", len(rows_with_missing_X_val))\n",
    "print(\"Missing values in y_val:\", len(rows_with_missing_y_val))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Columns:\n",
      "['num_of_prev_attempts', 'studied_credits', 'prop_submissions', 'avg_score', 'submission_distance', 'stu_activity_count', 'stu_activity_type_count', 'stu_total_clicks', 'stu_days_active']\n",
      "\n",
      "\n",
      "Non-Numeric Columns:\n",
      "['date_unregistration', 'subject']\n"
     ]
    }
   ],
   "source": [
    "# date_unregistration' column to string\n",
    "X_train['date_unregistration'] = X_train['date_unregistration'].astype(str)\n",
    "numeric_columns = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "non_numeric_columns = X_train.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "\n",
    "print(\"Numeric Columns:\")\n",
    "print(numeric_columns)\n",
    "print(\"\\n\")\n",
    "print(\"Non-Numeric Columns:\")\n",
    "print(non_numeric_columns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot / Ordinal Encoding Categorical Variables\n",
    "\n",
    "Categorical values need to be converted into numerical values for the model.  There are two main approaches:\n",
    "\n",
    "Originally, I needed to consider both - but with the current dataset only one-hot encoding is required.  [model_01_plan](../V1/model_01_plan%20%2B%20split%20%2B%20scale.ipynb) has initial exploration of ordinal encoding.\n",
    "\n",
    "* One-hot encoding - converts categorical variables into binary vectors.  That is - it creates new binary columns for each category.  For example, `subject` will be encoded as two features - subject_socsci and subject_stem which is either a 0 or 1 for each row.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_nominal_encoded: (13872, 2)\n",
      "Shape of X_test_nominal_encoded: (4625, 2)\n",
      "Shape of X_val_nominal_encoded: (4625, 2)\n"
     ]
    }
   ],
   "source": [
    "nominal_cols = ['subject']\n",
    "\n",
    "# One-Hot Encoding\n",
    "X_train_nominal_encoded = pd.get_dummies(X_train[nominal_cols])\n",
    "X_test_nominal_encoded = pd.get_dummies(X_test[nominal_cols])\n",
    "X_val_nominal_encoded = pd.get_dummies(X_val[nominal_cols])\n",
    "\n",
    "# reset indices\n",
    "X_train_nominal_encoded.reset_index(drop=True, inplace=True)\n",
    "X_test_nominal_encoded.reset_index(drop=True, inplace=True)\n",
    "X_val_nominal_encoded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Shape of X_train_nominal_encoded:\", X_train_nominal_encoded.shape)\n",
    "print(\"Shape of X_test_nominal_encoded:\", X_test_nominal_encoded.shape)\n",
    "print(\"Shape of X_val_nominal_encoded:\", X_val_nominal_encoded.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Scaling Numerical Variables\n",
    "\n",
    "Because the variables are in different units and scales - i.e. average score (0-100) v number_of_clicks (000s), the dataset needs to be scaled/normalised.  \n",
    "\n",
    "The `train` dataset is scaled and the same transformation (i.e. the same parameters) are applied to the `test` set.  This way there is no 'data leakage' - we have not accessed `test` in any way.\n",
    "\n",
    "Scaling only applies to 'numeric' variables - that is variables which can be, for example, means-centred (which is what I apply below).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_transformed: (13872, 11)\n",
      "Shape of X_test_transformed: (4625, 11)\n",
      "Shape of X_val_transformed: (4625, 11)\n"
     ]
    }
   ],
   "source": [
    "# standard Scaling\n",
    "X_train_numeric = X_train[numeric_columns]\n",
    "X_test_numeric = X_test[numeric_columns]\n",
    "X_val_numeric = X_val[numeric_columns]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_numeric)\n",
    "X_test_scaled = scaler.transform(X_test_numeric)\n",
    "X_val_scaled = scaler.transform(X_val_numeric)\n",
    "\n",
    "# reset indices \n",
    "X_train_scaled_reset = pd.DataFrame(X_train_scaled, columns=numeric_columns).reset_index(drop=True)\n",
    "X_test_scaled_reset = pd.DataFrame(X_test_scaled, columns=numeric_columns).reset_index(drop=True)\n",
    "X_val_scaled_reset = pd.DataFrame(X_val_scaled, columns=numeric_columns).reset_index(drop=True)\n",
    "\n",
    "# concatenate merged nominal dataframes with scaled dataframes\n",
    "X_train_transformed = pd.concat([X_train_nominal_encoded, X_train_scaled_reset], axis=1)\n",
    "X_test_transformed = pd.concat([X_test_nominal_encoded, X_test_scaled_reset], axis=1)\n",
    "X_val_transformed = pd.concat([X_val_nominal_encoded, X_val_scaled_reset], axis=1)\n",
    "\n",
    "# merging all dataframes\n",
    "print(\"Shape of X_train_transformed:\", X_train_transformed.shape)\n",
    "print(\"Shape of X_test_transformed:\", X_test_transformed.shape)\n",
    "print(\"Shape of X_val_transformed:\", X_val_transformed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_transformed: (13872, 12)\n",
      "Shape of X_test_transformed: (4625, 12)\n",
      "Shape of X_val_transformed: (4625, 12)\n"
     ]
    }
   ],
   "source": [
    "X_train_udate = X_train['date_unregistration']\n",
    "X_test_udate = X_test['date_unregistration']\n",
    "X_val_udate = X_val['date_unregistration']  \n",
    "\n",
    "# reset indices\n",
    "X_train_udate.reset_index(drop=True, inplace=True)\n",
    "X_test_udate.reset_index(drop=True, inplace=True)\n",
    "X_val_udate.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# concatenate \n",
    "X_train_transformed = pd.concat([X_train_transformed, X_train_udate], axis=1)\n",
    "X_test_transformed = pd.concat([X_test_transformed, X_test_udate], axis=1)\n",
    "X_val_transformed = pd.concat([X_val_transformed, X_val_udate], axis=1)\n",
    "\n",
    "# merging all dataframes\n",
    "print(\"Shape of X_train_transformed:\", X_train_transformed.shape)\n",
    "print(\"Shape of X_test_transformed:\", X_test_transformed.shape)\n",
    "print(\"Shape of X_val_transformed:\", X_val_transformed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_transformed.to_csv('../../data/X_train50_transformed.csv', index=False)\n",
    "X_test_transformed.to_csv('../../data/X_test_transformed_150.csv', index=False)\n",
    "#y_train.to_csv('../../data/y_train50.csv', index=False)\n",
    "y_test.to_csv('../../data/y_test_150.csv', index=False)\n",
    "#X_val_transformed.to_csv('../../data/X_val50_transformed.csv', index=False)\n",
    "#y_val.to_csv('../../data/y_val50.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_SocSci</th>\n",
       "      <th>subject_Stem</th>\n",
       "      <th>num_of_prev_attempts</th>\n",
       "      <th>studied_credits</th>\n",
       "      <th>prop_submissions</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>submission_distance</th>\n",
       "      <th>stu_activity_count</th>\n",
       "      <th>stu_activity_type_count</th>\n",
       "      <th>stu_total_clicks</th>\n",
       "      <th>stu_days_active</th>\n",
       "      <th>date_unregistration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.338707</td>\n",
       "      <td>-0.406437</td>\n",
       "      <td>-2.313860</td>\n",
       "      <td>-2.191219</td>\n",
       "      <td>4.765116</td>\n",
       "      <td>-1.055976</td>\n",
       "      <td>-2.021746</td>\n",
       "      <td>-0.836101</td>\n",
       "      <td>-1.332637</td>\n",
       "      <td>268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.338707</td>\n",
       "      <td>1.208794</td>\n",
       "      <td>0.539926</td>\n",
       "      <td>0.650207</td>\n",
       "      <td>-0.448201</td>\n",
       "      <td>-0.620232</td>\n",
       "      <td>-0.409533</td>\n",
       "      <td>-0.612901</td>\n",
       "      <td>-0.560720</td>\n",
       "      <td>262.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.338707</td>\n",
       "      <td>-0.406437</td>\n",
       "      <td>0.539926</td>\n",
       "      <td>-0.730623</td>\n",
       "      <td>-0.448201</td>\n",
       "      <td>-0.692856</td>\n",
       "      <td>-1.215639</td>\n",
       "      <td>-0.619188</td>\n",
       "      <td>-0.891541</td>\n",
       "      <td>262.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.338707</td>\n",
       "      <td>-0.406437</td>\n",
       "      <td>0.539926</td>\n",
       "      <td>1.180909</td>\n",
       "      <td>-0.448201</td>\n",
       "      <td>1.507647</td>\n",
       "      <td>0.396574</td>\n",
       "      <td>1.454841</td>\n",
       "      <td>1.644757</td>\n",
       "      <td>268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.338707</td>\n",
       "      <td>-0.406437</td>\n",
       "      <td>0.539926</td>\n",
       "      <td>0.637002</td>\n",
       "      <td>-0.448201</td>\n",
       "      <td>0.625267</td>\n",
       "      <td>1.605734</td>\n",
       "      <td>0.749875</td>\n",
       "      <td>0.597156</td>\n",
       "      <td>268.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_SocSci  subject_Stem  num_of_prev_attempts  studied_credits  \\\n",
       "0               1             0             -0.338707        -0.406437   \n",
       "1               0             1             -0.338707         1.208794   \n",
       "2               1             0             -0.338707        -0.406437   \n",
       "3               0             1             -0.338707        -0.406437   \n",
       "4               0             1             -0.338707        -0.406437   \n",
       "\n",
       "   prop_submissions  avg_score  submission_distance  stu_activity_count  \\\n",
       "0         -2.313860  -2.191219             4.765116           -1.055976   \n",
       "1          0.539926   0.650207            -0.448201           -0.620232   \n",
       "2          0.539926  -0.730623            -0.448201           -0.692856   \n",
       "3          0.539926   1.180909            -0.448201            1.507647   \n",
       "4          0.539926   0.637002            -0.448201            0.625267   \n",
       "\n",
       "   stu_activity_type_count  stu_total_clicks  stu_days_active  \\\n",
       "0                -2.021746         -0.836101        -1.332637   \n",
       "1                -0.409533         -0.612901        -0.560720   \n",
       "2                -1.215639         -0.619188        -0.891541   \n",
       "3                 0.396574          1.454841         1.644757   \n",
       "4                 1.605734          0.749875         0.597156   \n",
       "\n",
       "   date_unregistration  \n",
       "0                268.0  \n",
       "1                262.0  \n",
       "2                262.0  \n",
       "3                268.0  \n",
       "4                268.0  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4625 entries, 0 to 4624\n",
      "Data columns (total 12 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   subject_SocSci           4625 non-null   uint8  \n",
      " 1   subject_Stem             4625 non-null   uint8  \n",
      " 2   num_of_prev_attempts     4625 non-null   float64\n",
      " 3   studied_credits          4625 non-null   float64\n",
      " 4   prop_submissions         4625 non-null   float64\n",
      " 5   avg_score                4625 non-null   float64\n",
      " 6   submission_distance      4625 non-null   float64\n",
      " 7   stu_activity_count       4625 non-null   float64\n",
      " 8   stu_activity_type_count  4625 non-null   float64\n",
      " 9   stu_total_clicks         4625 non-null   float64\n",
      " 10  stu_days_active          4625 non-null   float64\n",
      " 11  date_unregistration      4625 non-null   float64\n",
      "dtypes: float64(10), uint8(2)\n",
      "memory usage: 370.5 KB\n"
     ]
    }
   ],
   "source": [
    "X_test_transformed.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draft function for handling unseen data - first transformation\n",
    "\n",
    "do not use - not updated for new dataset\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # columns to be used for each type of variable\n",
    "    numeric_columns = ['num_of_prev_attempts', 'studied_credits', 'prop_submissions', 'avg_score', 'submission_distance', 'stu_activity_count', 'stu_activity_type_count', 'stu_total_clicks', 'stu_days_active']\n",
    "    nominal_columns = ['subject']\n",
    "\n",
    "    # check required columns exist\n",
    "    missing_numeric_cols = [col for col in numeric_columns if col not in data.columns]\n",
    "    missing_nominal_cols = [col for col in nominal_columns if col not in data.columns]\n",
    "\n",
    "    assert not missing_numeric_cols, f\"Missing numeric columns: {', '.join(missing_numeric_cols)}\"\n",
    "    assert not missing_nominal_cols, f\"Missing nominal columns: {', '.join(missing_nominal_cols)}\"\n",
    "\n",
    "    # drop unneeded columns\n",
    "    unneeded_cols = [col for col in data.columns if col not in numeric_columns + nominal_columns]\n",
    "    data = data.drop(unneeded_cols, axis=1)\n",
    "\n",
    "    # preprocessing for each type of variable\n",
    "    numeric_transformer = StandardScaler()\n",
    "    nominal_transformer = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "    # ColumnTransformer for appropriate transformations\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('numeric', numeric_transformer, numeric_columns),\n",
    "            ('nominal', nominal_transformer, nominal_columns)\n",
    "        ])\n",
    "\n",
    "    # fit and transform the data\n",
    "    transformed_data = preprocessor.fit_transform(data)\n",
    "\n",
    "    return transformed_data\n",
    "\n",
    "\n",
    "# Example usage\n",
    "data = pd.read_csv('data.csv')  # Load your dataset here\n",
    "preprocessed_data = preprocess_data(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

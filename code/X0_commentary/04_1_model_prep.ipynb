{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modelling  \n",
    "\n",
    "This section is preparing for data modelling and stepping through the process of building a model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "\n",
    "I have had several thoughts about how the data could and should be split:\n",
    "\n",
    "* train/validate/test split - where the model is trained on `train`, validated and tuned on `validate` and tested/confirmed on `test`\n",
    "* train/test split - where the model is trained, validated and tuned on `train` using cross validation, before final model is tested/confirmed on `test`\n",
    "* train/test split on `year` in the dataset - where the model is trained on data from 2013 and tested on 2014 data.  This would mimic the real world scenario, where a HE institution would use data from previous years to build a model which is applied to the current year students.  The model would be retrained on an annual basis with new data.  \n",
    "  * However, the dataset had some `module_presentations` which would not feature in the training data\n",
    "  * The distribution of the data between years is quite different - I focused on `subject` as a proxy or student type and behaviour as well as curriculum differences between modules - and it varied between years.  \n",
    "* reworking the final_result target variables into two categories - `intervene` and `no_intervene` - and using a binary classification model to predict whether a student would need intervention or not.  This could be a more useful model for the HE institution, as it would be able to identify students who need intervention and target resources at them.  However, this would require a different approach to the modelling, as the target variable would be binary rather than multi-class.\n",
    "\n",
    "So, I decided to split the whole dataset into `train` and `test` sets, using the `train_test_split` function from `sklearn.model_selection`.  I used a `test_size` of 0.25, which is 25% of the data.  I also set the `random_state` to 567, so that the split would be reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed data from csv file\n",
    "data = pd.read_csv('../../data/final_model_ALL_20230526.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31437 entries, 0 to 31436\n",
      "Data columns (total 28 columns):\n",
      " #   Column                                      Non-Null Count  Dtype  \n",
      "---  ------                                      --------------  -----  \n",
      " 0   code_module                                 31437 non-null  object \n",
      " 1   code_presentation                           31437 non-null  object \n",
      " 2   id_student                                  31437 non-null  int64  \n",
      " 3   gender                                      31437 non-null  object \n",
      " 4   region                                      31437 non-null  object \n",
      " 5   highest_education                           31437 non-null  object \n",
      " 6   imd_band                                    31437 non-null  object \n",
      " 7   age_band                                    31437 non-null  object \n",
      " 8   num_of_prev_attempts                        31437 non-null  int64  \n",
      " 9   studied_credits                             31437 non-null  int64  \n",
      " 10  disability                                  31437 non-null  object \n",
      " 11  course_length                               31437 non-null  int64  \n",
      " 12  date_registration                           31437 non-null  float64\n",
      " 13  date_unregistration                         31437 non-null  float64\n",
      " 14  unregistration_before_registration          31437 non-null  bool   \n",
      " 15  unregistration_before_registration_14_days  31437 non-null  bool   \n",
      " 16  prop_submissions                            31437 non-null  float64\n",
      " 17  avg_score                                   31437 non-null  float64\n",
      " 18  submission_distance                         31437 non-null  float64\n",
      " 19  stu_activity_count                          31437 non-null  float64\n",
      " 20  stu_activity_type_count                     31437 non-null  float64\n",
      " 21  stu_total_clicks                            31437 non-null  float64\n",
      " 22  stu_days_active                             31437 non-null  float64\n",
      " 23  mod_pres_vle_type_count                     31437 non-null  float64\n",
      " 24  year                                        31437 non-null  int64  \n",
      " 25  month                                       31437 non-null  object \n",
      " 26  subject                                     31437 non-null  object \n",
      " 27  final_result                                31437 non-null  object \n",
      "dtypes: bool(2), float64(10), int64(5), object(11)\n",
      "memory usage: 6.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns as discussed in previous notebooks.  \n",
    "\n",
    "I still think it would be interesting to build different models with some of these dropped features, although they are different research questions which need to be properly considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = data.copy()\n",
    "# columns to drop\n",
    "columns_to_drop = ['code_module','code_presentation', 'id_student', 'gender', 'region', 'highest_education', 'imd_band',\n",
    "                   'age_band', 'disability', 'course_length', 'unregistration_before_registration',\n",
    "                   'unregistration_before_registration_14_days', 'mod_pres_vle_type_count', 'year', 'month','date_registration', 'date_unregistration',]\n",
    "\n",
    "# drop columns\n",
    "model = model.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save into standard `X` and `y` variables.\n",
    "\n",
    "Use stratification to ensure that the proportions of `final_result` are the same in both the `train` and `test` sets.  This is important as the `final_result` is the target variable and we want to ensure that the model is trained on a representative sample of the data.  I used the `stratify` parameter in the `train_test_split` function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop target from X, save target to y\n",
    "X = model.drop('final_result', axis=1)  \n",
    "y = model['final_result']  \n",
    "\n",
    "# split data into train and test sets with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=567)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I have checked to see that the proportions are the same in both the `train` and `test` sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Proportions:\n",
      "Pass           0.376276\n",
      "Withdrawn      0.314311\n",
      "Fail           0.219550\n",
      "Distinction    0.089862\n",
      "Name: final_result, dtype: float64\n",
      "\n",
      "Train Set Proportions:\n",
      "Pass           0.376257\n",
      "Withdrawn      0.314332\n",
      "Fail           0.219536\n",
      "Distinction    0.089876\n",
      "Name: final_result, dtype: float64\n",
      "\n",
      "Test Set Proportions:\n",
      "Pass           0.376336\n",
      "Withdrawn      0.314249\n",
      "Fail           0.219593\n",
      "Distinction    0.089822\n",
      "Name: final_result, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# proportions of target variable in original data\n",
    "original_proportions = model['final_result'].value_counts(normalize=True)\n",
    "\n",
    "# proportions of target variable in train and test sets\n",
    "train_proportions = y_train.value_counts(normalize=True)\n",
    "test_proportions = y_test.value_counts(normalize=True)\n",
    "\n",
    "# results\n",
    "print(\"Original Proportions:\")\n",
    "print(original_proportions)\n",
    "\n",
    "print(\"\\nTrain Set Proportions:\")\n",
    "print(train_proportions)\n",
    "\n",
    "print(\"\\nTest Set Proportions:\")\n",
    "print(test_proportions)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another standard check is to ensure that there are no missing values in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in X_train: 0\n",
      "Missing values in X_test: 0\n",
      "Missing values in y_train: 0\n",
      "Missing values in y_test: 0\n"
     ]
    }
   ],
   "source": [
    "# missing values in X_train, X_test, y_train, y_test\n",
    "missing_values_X_train = X_train.isnull().sum()\n",
    "missing_values_X_test = X_test.isnull().sum()\n",
    "missing_values_y_train = y_train.isnull().sum()\n",
    "missing_values_y_test = y_test.isnull().sum()\n",
    "\n",
    "\n",
    "# rows with missing values\n",
    "rows_with_missing_X_train = X_train[X_train.isnull().any(axis=1)]\n",
    "rows_with_missing_X_test = X_test[X_test.isnull().any(axis=1)]\n",
    "rows_with_missing_y_train = y_train[y_train.isnull()]\n",
    "rows_with_missing_y_test = y_test[y_test.isnull()]\n",
    "\n",
    "\n",
    "\n",
    "# results\n",
    "print(\"Missing values in X_train:\", len(rows_with_missing_X_train))\n",
    "print(\"Missing values in X_test:\", len(rows_with_missing_X_test))\n",
    "print(\"Missing values in y_train:\", len(rows_with_missing_y_train))\n",
    "print(\"Missing values in y_test:\", len(rows_with_missing_y_test))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Columns:\n",
      "['num_of_prev_attempts', 'studied_credits', 'prop_submissions', 'avg_score', 'submission_distance', 'stu_activity_count', 'stu_activity_type_count', 'stu_total_clicks', 'stu_days_active']\n",
      "\n",
      "\n",
      "Non-Numeric Columns:\n",
      "['subject']\n"
     ]
    }
   ],
   "source": [
    "numeric_columns = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "non_numeric_columns = X_train.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(\"Numeric Columns:\")\n",
    "print(numeric_columns)\n",
    "print(\"\\n\")\n",
    "print(\"Non-Numeric Columns:\")\n",
    "print(non_numeric_columns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot / Ordinal Encoding Categorical Variables\n",
    "\n",
    "Categorical values need to be converted into numerical values for the model.  There are two main approaches:\n",
    "\n",
    "Originally, I needed to consider both - but with the current dataset only one-hot encoding is required.  [model_01_plan](../V1/model_01_plan%20%2B%20split%20%2B%20scale.ipynb) has initial exploration of ordinal encoding.\n",
    "\n",
    "* One-hot encoding - converts categorical variables into binary vectors.  That is - it creates new binary columns for each category.  For example, `subject` will be encoded as two features - subject_socsci and subject_stem which is either a 0 or 1 for each row.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_nominal_encoded: (23577, 2)\n",
      "Shape of X_test_nominal_encoded: (7860, 2)\n"
     ]
    }
   ],
   "source": [
    "nominal_cols = ['subject']\n",
    "\n",
    "# One-Hot Encoding\n",
    "X_train_nominal_encoded = pd.get_dummies(X_train[nominal_cols])\n",
    "X_test_nominal_encoded = pd.get_dummies(X_test[nominal_cols])\n",
    "\n",
    "# reset indices\n",
    "X_train_nominal_encoded.reset_index(drop=True, inplace=True)\n",
    "X_test_nominal_encoded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Shape of X_train_nominal_encoded:\", X_train_nominal_encoded.shape)\n",
    "print(\"Shape of X_test_nominal_encoded:\", X_test_nominal_encoded.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Scaling Numerical Variables\n",
    "\n",
    "Because the variables are in different units and scales - i.e. average score (0-100) v number_of_clicks (000s), the dataset needs to be scaled/normalised.  \n",
    "\n",
    "The `train` dataset is scaled and the same transformation (i.e. the same parameters) are applied to the `test` set.  This way there is no 'data leakage' - we have not accessed `test` in any way.\n",
    "\n",
    "Scaling only applies to 'numeric' variables - that is variables which can be, for example, means-centred (which is what I apply below).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_transformed: (23577, 11)\n",
      "Shape of X_test_transformed: (7860, 11)\n"
     ]
    }
   ],
   "source": [
    "# standard Scaling\n",
    "X_train_numeric = X_train[numeric_columns]\n",
    "X_test_numeric = X_test[numeric_columns]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_numeric)\n",
    "X_test_scaled = scaler.transform(X_test_numeric)\n",
    "\n",
    "# reset indices \n",
    "X_train_scaled_reset = pd.DataFrame(X_train_scaled, columns=numeric_columns).reset_index(drop=True)\n",
    "X_test_scaled_reset = pd.DataFrame(X_test_scaled, columns=numeric_columns).reset_index(drop=True)\n",
    "\n",
    "# concatenate merged nominal dataframes with scaled dataframes\n",
    "X_train_transformed = pd.concat([X_train_nominal_encoded, X_train_scaled_reset], axis=1)\n",
    "X_test_transformed = pd.concat([X_test_nominal_encoded, X_test_scaled_reset], axis=1)\n",
    "\n",
    "# merging all dataframes\n",
    "print(\"Shape of X_train_transformed:\", X_train_transformed.shape)\n",
    "print(\"Shape of X_test_transformed:\", X_test_transformed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed.to_csv('../../data/X_train_transformed.csv', index=False)\n",
    "X_test_transformed.to_csv('../../data/X_test_transformed.csv', index=False)\n",
    "y_train.to_csv('../../data/y_train.csv', index=False)\n",
    "y_test.to_csv('../../data/y_test.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draft function for handling unseen data - first transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # columns to be used for each type of variable\n",
    "    numeric_columns = ['num_of_prev_attempts', 'studied_credits', 'prop_submissions', 'avg_score', 'submission_distance', 'stu_activity_count', 'stu_activity_type_count', 'stu_total_clicks', 'stu_days_active']\n",
    "    nominal_columns = ['subject']\n",
    "\n",
    "    # check required columns exist\n",
    "    missing_numeric_cols = [col for col in numeric_columns if col not in data.columns]\n",
    "    missing_nominal_cols = [col for col in nominal_columns if col not in data.columns]\n",
    "\n",
    "    assert not missing_numeric_cols, f\"Missing numeric columns: {', '.join(missing_numeric_cols)}\"\n",
    "    assert not missing_nominal_cols, f\"Missing nominal columns: {', '.join(missing_nominal_cols)}\"\n",
    "\n",
    "    # drop unneeded columns\n",
    "    unneeded_cols = [col for col in data.columns if col not in numeric_columns + nominal_columns]\n",
    "    data = data.drop(unneeded_cols, axis=1)\n",
    "\n",
    "    # preprocessing for each type of variable\n",
    "    numeric_transformer = StandardScaler()\n",
    "    nominal_transformer = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "    # ColumnTransformer for appropriate transformations\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('numeric', numeric_transformer, numeric_columns),\n",
    "            ('nominal', nominal_transformer, nominal_columns)\n",
    "        ])\n",
    "\n",
    "    # fit and transform the data\n",
    "    transformed_data = preprocessor.fit_transform(data)\n",
    "\n",
    "    return transformed_data\n",
    "\n",
    "\n",
    "# Example usage\n",
    "data = pd.read_csv('data.csv')  # Load your dataset here\n",
    "preprocessed_data = preprocess_data(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
